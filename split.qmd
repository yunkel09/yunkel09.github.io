
# Split {#sec-split}

## Snooping bias

Para evitar el *snooping bias* es necesario poder dividir el conjunto total
en tres partes: entrenamiento, validación y prueba.  Esto se debe a que si
se realiza el EDA con todos los datos, se corre el riesgo de que esto influya
en la selección de los modelos.  Las vistas preliminares de la sección
anterior no generan problemas, sin embargo es importante asegurarse de que
cualquier decisión que tomemos se base únicamente en lo visto en el conjunto
de entrenamiento [@geron_hands-machine_2017]


```{r}
#| echo: false
source("_common.R")
source("_functions.R")
source("_load.R")
```

```{r}
#| echo: false
ctl_01 <- read_fst("data/ctl_01.fst") |> as_tibble()
```

## Data spending

En el contexto del análisis de datos utilizando métodos de resampling agrupados,
como `group_initial_validation_split` en el paquete `rsample`, se asume que
todas las observaciones dentro de un grupo son interdependientes y comparten
características similares. Por lo tanto, se requiere que cada unidad
experimental y de agrupamiento, en este caso, cada usuario, tenga un único valor
para la variable de estratificación, en este caso, el diagnóstico. La presencia
de múltiples valores de diagnóstico para un solo usuario violaría la suposición
de homogeneidad dentro del grupo, lo cual podría llevar a estimaciones sesgadas
y resultados poco fiables en el análisis posterior. Además, la interpretación de
los resultados y la aplicabilidad del modelo podrían verse complicadas. Por lo
tanto, se establece que la homogeneidad dentro de cada grupo es esencial para
mantener la integridad del análisis y la validez de los resultados.

Con la función `group_initial_validation_split()` se creará una división triple
de los datos en un conjunto de entrenamiento, validación y prueba. Se realizará
estratificación sobre nuestra variable objetivo `diag`. Dejaremos las
proporciones por defecto debido a que tenemos `r nrow(ctl_01)` observaciones.
El valor predeterminado de `prop = c(0.6, 0.2)` lo que significa que el 60% de
los datos se asignarán al conjunto de entrenamiento, un 20% a validación y el
20% restante al conjunto de prueba.

```{r}
ctl_split <- ctl_01 |> 
 group_initial_validation_split(group = msisdn_dd, strata = diag)
```

```{r}
ctl_split
```

```{r}
#| code-fold: true
list(train = training(ctl_split), 
     validation = validation(ctl_split), 
     test = testing(ctl_split)) |> 
 map(\(x) nrow(x)) |> 
 as_tibble() |> 
 pivot_longer(cols = everything(), 
              names_to = "division", values_to = "observaciones") |> 
 mutate(prop = observaciones / sum(observaciones)) |> 
 gt() |> 
 tab_header(
    title = md("**Three-Way Split**"),
    subtitle = md("Distribución basada en la cantidad disponible de datos")
  ) |> 
 gt_theme_538() |> 
 fmt_number(columns = observaciones, decimals = 0) |> 
 fmt_percent(
    columns = prop,
    decimals = 1
  )
```

En la tabla anterior se observa como quedó la distribución.

Podemos acceder a estos conjuntos de la siguiente manera:

```{r}
training(ctl_split) |> 
 head()
```

```{r}
#| echo: false
tablero_ctl <- board_folder(path = "tablero_ctl")
```

```{r}
#| echo: false
#| message: false
#| warning: false
pin_write(
 board       = tablero_ctl,
 x           = ctl_split,
 name        = "ctl_split", 
 type        = "rds", 
 title       = "Three-way split", 
 description = "División aleatoria de tres vías")
```

```{r}
#| eval: false
#| echo: false
tablero_ctl |> pin_meta("ctl_split")
pin_list(tablero_ctl)
```











