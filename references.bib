
@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, R (Computer program language), Problems, exercises, etc, Statistics},
}

@book{kuhn_feature_2020,
	address = {Boca Raton London New York},
	series = {Chapman \& {Hall}/{CRC} data science series},
	title = {Feature engineering and selection: a practical approach for predictive models},
	isbn = {978-1-03-209085-6 978-1-138-07922-9},
	shorttitle = {Feature engineering and selection},
	language = {eng},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2020},
}

@book{duboue_art_2020,
	address = {Cambridge ; New York, NY},
	edition = {First edition},
	title = {The art of feature engineering: essentials for machine learning},
	isbn = {978-1-108-67168-2},
	shorttitle = {The art of feature engineering},
	abstract = {"When working with a data set, a machine learning engineer might train a model but find that the results are not as good as they need. To get better results, they can try to improve the model or collect more data, but there is another avenue: feature engineering. The feature engineering process can help improve results by modifying the data's features to better capture the nature of the problem. This process is partly an art and partly a palette of tricks and recipes. This practical guide to feature engineering is an essential addition to any data scientist's or machine learning engineer's toolbox, providing new ideas on how to improve the performance of a machine learning solution. Beginning with the basic concepts and techniques of feature engineering, the text builds up to a unique cross-domain approach that spans data on graphs, texts, time series, and images, with fully worked out case studies. Key topics include binning, out-of-fold estimation, feature selection, dimensionality reduction, and encoding variable-length data. The full source code for the case studies is available on a companion website as Python Jupyter notebooks"--},
	publisher = {Cambridge University Press},
	author = {Duboue, Pablo},
	year = {2020},
	keywords = {Python (Computer program language), Machine learning},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Machine learning, Computer algorithms},
}

@book{hosmer_applied_2000,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Applied logistic regression},
	isbn = {978-0-471-35632-5},
	publisher = {Wiley},
	author = {Hosmer, David W. and Lemeshow, Stanley},
	year = {2000},
	keywords = {Regression analysis},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Statistics, Data mining, Machine learning, Bioinformatics, Computational intelligence, Forecasting, Inference, Methodology},
}

@book{michelucci_applied_2018,
	address = {New York, NY},
	title = {Applied deep learning: a case-based approach to understanding deep neural networks},
	isbn = {978-1-4842-3789-2},
	shorttitle = {Applied deep learning},
	publisher = {Springer Science+Business Media, LLC},
	author = {Michelucci, Umberto},
	year = {2018},
}

@book{levin_statistics_1998,
	address = {Upper Saddle River, N.J},
	edition = {7th ed},
	title = {Statistics for management},
	isbn = {978-0-13-476292-0},
	publisher = {Prentice Hall},
	author = {Levin, Richard I. and Rubin, David S.},
	year = {1998},
	keywords = {Statistical methods, Commercial statistics, Management, Social sciences},
}

@book{kuhn_applied_2013,
	address = {New York},
	title = {Applied predictive modeling},
	isbn = {978-1-4614-6848-6},
	publisher = {Springer},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2013},
	note = {OCLC: ocn827083441},
	keywords = {Mathematical models, Mathematical statistics, Prediction theory},
}

@book{geron_hands-machine_2017,
	address = {Beijing ; Boston},
	edition = {First edition},
	title = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4919-6229-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}},
	abstract = {"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks--Scikit-Learn and TensorFlow--author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started" --},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	year = {2017},
	note = {OCLC: ocn953432302},
	keywords = {Machine learning, Artificial intelligence, Automatische Klassifikation, COMPUTERS / Computer Vision \& Pattern Recognition, COMPUTERS / Data Processing, COMPUTERS / Intelligence (AI) \& Semantics, COMPUTERS / Natural Language Processing, COMPUTERS / Neural Networks, Künstliche Intelligenz, Maschinelles Lernen, Nonfiction, Python 3.0},
}

@book{kuhn_tidy_2022,
	address = {Sebastopol, CA},
	title = {Tidy modeling with {R}: a framework for modeling in the tidyverse},
	isbn = {978-1-4920-9648-1},
	shorttitle = {Tidy modeling with {R}},
	abstract = {Get going with tidymodels, a collection of R packges for modeling and machine learning. Whether you're just starting out or have years of experience with modeling, this practical introduction shows data analysts, business analysts, and data scientists how the tidymodels framework offers a consistent, flexible approach for your work. RStudio engineers Max Kuhn and Julia Silge demonstrate ways to create models by focusing on an R dialect called the tidyverse. Software that adops tidyverse principles shares both a high-level design philosophy and low-level grammar and data structures, so learning one piece of the ecosystem makes it easier to learn the next. You'll understand why the tidymodels framework has been built to be used by a broad range of people},
	publisher = {O'Reilly Media},
	author = {Kuhn, Max and Silge, Julia},
	year = {2022},
	note = {OCLC: on1338675673},
	keywords = {Machine learning, Mathematical models, Quantitative research, R (Computer program language)},
}
