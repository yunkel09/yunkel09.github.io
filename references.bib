
@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, R (Computer program language), Problems, exercises, etc, Statistics},
}

@book{kuhn_feature_2020,
	address = {Boca Raton London New York},
	series = {Chapman \& {Hall}/{CRC} data science series},
	title = {Feature engineering and selection: a practical approach for predictive models},
	isbn = {978-1-03-209085-6 978-1-138-07922-9},
	shorttitle = {Feature engineering and selection},
	language = {eng},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2020},
}

@book{duboue_art_2020,
	address = {Cambridge ; New York, NY},
	edition = {First edition},
	title = {The art of feature engineering: essentials for machine learning},
	isbn = {978-1-108-67168-2},
	shorttitle = {The art of feature engineering},
	abstract = {"When working with a data set, a machine learning engineer might train a model but find that the results are not as good as they need. To get better results, they can try to improve the model or collect more data, but there is another avenue: feature engineering. The feature engineering process can help improve results by modifying the data's features to better capture the nature of the problem. This process is partly an art and partly a palette of tricks and recipes. This practical guide to feature engineering is an essential addition to any data scientist's or machine learning engineer's toolbox, providing new ideas on how to improve the performance of a machine learning solution. Beginning with the basic concepts and techniques of feature engineering, the text builds up to a unique cross-domain approach that spans data on graphs, texts, time series, and images, with fully worked out case studies. Key topics include binning, out-of-fold estimation, feature selection, dimensionality reduction, and encoding variable-length data. The full source code for the case studies is available on a companion website as Python Jupyter notebooks"--},
	publisher = {Cambridge University Press},
	author = {Duboue, Pablo},
	year = {2020},
	keywords = {Python (Computer program language), Machine learning},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Machine learning, Computer algorithms},
}

@book{hosmer_applied_2000,
	address = {New York},
	edition = {2nd ed},
	series = {Wiley series in probability and statistics},
	title = {Applied logistic regression},
	isbn = {978-0-471-35632-5},
	publisher = {Wiley},
	author = {Hosmer, David W. and Lemeshow, Stanley},
	year = {2000},
	keywords = {Regression analysis},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {2nd ed},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
	year = {2009},
	keywords = {Statistics, Data mining, Machine learning, Bioinformatics, Computational intelligence, Forecasting, Inference, Methodology},
}

@book{michelucci_applied_2018,
	address = {New York, NY},
	title = {Applied deep learning: a case-based approach to understanding deep neural networks},
	isbn = {978-1-4842-3789-2},
	shorttitle = {Applied deep learning},
	publisher = {Springer Science+Business Media, LLC},
	author = {Michelucci, Umberto},
	year = {2018},
}

@book{levin_statistics_1998,
	address = {Upper Saddle River, N.J},
	edition = {7th ed},
	title = {Statistics for management},
	isbn = {978-0-13-476292-0},
	publisher = {Prentice Hall},
	author = {Levin, Richard I. and Rubin, David S.},
	year = {1998},
	keywords = {Statistical methods, Commercial statistics, Management, Social sciences},
}

@book{kuhn_applied_2013,
	address = {New York},
	title = {Applied predictive modeling},
	isbn = {978-1-4614-6848-6},
	publisher = {Springer},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2013},
	note = {OCLC: ocn827083441},
	keywords = {Mathematical models, Mathematical statistics, Prediction theory},
}

@book{geron_hands-machine_2017,
	address = {Beijing ; Boston},
	edition = {First edition},
	title = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4919-6229-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn} and {TensorFlow}},
	abstract = {"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks--Scikit-Learn and TensorFlow--author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started" --},
	publisher = {O'Reilly Media},
	author = {Géron, Aurélien},
	year = {2017},
	note = {OCLC: ocn953432302},
	keywords = {Machine learning, Artificial intelligence, Automatische Klassifikation, COMPUTERS / Computer Vision \& Pattern Recognition, COMPUTERS / Data Processing, COMPUTERS / Intelligence (AI) \& Semantics, COMPUTERS / Natural Language Processing, COMPUTERS / Neural Networks, Künstliche Intelligenz, Maschinelles Lernen, Nonfiction, Python 3.0},
}

@book{kuhn_tidy_2022,
	address = {Sebastopol, CA},
	title = {Tidy modeling with {R}: a framework for modeling in the tidyverse},
	isbn = {978-1-4920-9648-1},
	shorttitle = {Tidy modeling with {R}},
	abstract = {Get going with tidymodels, a collection of R packges for modeling and machine learning. Whether you're just starting out or have years of experience with modeling, this practical introduction shows data analysts, business analysts, and data scientists how the tidymodels framework offers a consistent, flexible approach for your work. RStudio engineers Max Kuhn and Julia Silge demonstrate ways to create models by focusing on an R dialect called the tidyverse. Software that adops tidyverse principles shares both a high-level design philosophy and low-level grammar and data structures, so learning one piece of the ecosystem makes it easier to learn the next. You'll understand why the tidymodels framework has been built to be used by a broad range of people},
	publisher = {O'Reilly Media},
	author = {Kuhn, Max and Silge, Julia},
	year = {2022},
	note = {OCLC: on1338675673},
	keywords = {Mathematical models, R (Computer program language), Machine learning, Quantitative research},
}

@article{diebner_prediction_2021,
	title = {Prediction: {The} future of {CX}},
	volume = {1},
	copyright = {McKinsey \& Company},
	url = {https://mck.co/40e4gja},
	language = {English},
	journal = {McKinsey \& Company},
	author = {Diebner, Rachel and Mlafara, David},
	month = feb,
	year = {2021},
	pages = {8},
}

@article{kampa_winning_2023,
	title = {Winning in telecom {CX}},
	url = {https://mck.co/3MkkVvw},
	abstract = {Digital natives have redefined customer expectations. With a customer led, customer experience transformation, incumbent telcos can rise to the challenge.},
	language = {English},
	journal = {McKinsey \& Company},
	author = {Kampa, Dominika and Kucherenko, Stanislav and Maechler, Nicolas},
	month = apr,
	year = {2023},
	pages = {7},
}

@article{wassouf_predictive_2020,
	title = {Predictive analytics using big data for increased customer loyalty: {Syriatel} {Telecom} {Company} case study},
	volume = {7},
	issn = {2196-1115},
	shorttitle = {Predictive analytics using big data for increased customer loyalty},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00290-0},
	doi = {10.1186/s40537-020-00290-0},
	abstract = {Given the growing importance of customer behavior in the business market nowadays, telecom operators focus not only on customer profitability to increase market share but also on highly loyal customers as well as customers who are churn. The emergence of big data concepts introduced a new wave of Customer Relationship Management (CRM) strategies. Big data analysis helps to describe customer’s behavior, understand their habits, develop appropriate marketing plans for organizations to identify sales transactions and build a long-term loyalty relationship. This paper provides a methodology for telecom companies to target different-value customers by appropriate offers and services. This methodology was implemented and tested using a dataset that contains about 127 million records for training and testing supplied by Syriatel corporation. Firstly, customers were segmented based on the new approach (Time-frequencymonetary) TFM (TFM where: Time (T): total of calls duration and Internet sessions in a certain period of time. Frequency (F): use services frequently within a certain period. Monetary (M): The money spent during a certain period.) and the level of loyalty was defined for each segment or group. Secondly, The loyalty level descriptors were taken as categories, choosing the best behavioral features for customers, their demographic information such as age, gender, and the services they share. Thirdly, Several classification algorithms were applied based on the descriptors and the chosen features to build different predictive models that were used to classify new users by loyalty. Finally, those models were evaluated based on several criteria and derive the rules of loyalty prediction. After that by analyzing these rules, the loyalty reasons at each level were discovered to target them the most appropriate offers and services.},
	language = {en},
	number = {1},
	urldate = {2023-10-27},
	journal = {Journal of Big Data},
	author = {Wassouf, Wissam Nazeer and Alkhatib, Ramez and Salloum, Kamal and Balloul, Shadi},
	month = dec,
	year = {2020},
	pages = {29},
}

@article{markoulidakis_machine_2020,
	title = {A {Machine} {Learning} {Based} {Classification} {Method} for {Customer} {Experience} {Survey} {Analysis}},
	volume = {8},
	issn = {2227-7080},
	url = {https://www.mdpi.com/2227-7080/8/4/76},
	doi = {10.3390/technologies8040076},
	abstract = {Customer Experience (CX) is monitored through market research surveys, based on metrics like the Net Promoter Score (NPS) and the customer satisfaction for certain experience attributes (e.g., call center, website, billing, service quality, tariff plan). The objective of companies is to maximize NPS through the improvement of the most important CX attributes. However, statistical analysis suggests that there is a lack of clear and accurate association between NPS and the CX attributes’ scores. In this paper, we address the aforementioned deficiency using a novel classification approach, which was developed based on logistic regression and tested with several state-of-the-art machine learning (ML) algorithms. The proposed method was applied on an extended data set from the telecommunication sector and the results were quite promising, showing a significant improvement in most statistical metrics.},
	language = {en},
	number = {4},
	urldate = {2023-10-27},
	journal = {Technologies},
	author = {Markoulidakis, Ioannis and Rallis, Ioannis and Georgoulas, Ioannis and Kopsiaftis, George and Doulamis, Anastasios and Doulamis, Nikolaos},
	month = dec,
	year = {2020},
	pages = {76},
}

1
@article{muhammad_how_2016,
	title = {How {Important} {Is} {Customer} {Satisfaction}? {Quantitative} {Evidence} from {Mobile} {Telecommunication} {Market}},
	volume = {11},
	issn = {1833-8119, 1833-3850},
	shorttitle = {How {Important} {Is} {Customer} {Satisfaction}?},
	url = {http://www.ccsenet.org/journal/index.php/ijbm/article/view/58665},
	doi = {10.5539/ijbm.v11n6p57},
	abstract = {To investigates the importance of customer satisfaction in Pakistani mobile telecommunication market. This study explores whether customer satisfaction affects the relationship between customer loyalty and service quality, and also between customer loyalty and perceived value. The study found the coefficient of determination (R2) for the overall model to be considerable. The role of customer satisfaction was significant in assessing the contribution of exogenous constructs to the R2 value of endogenous constructs (f2{\textgreater} 0.35). All exogenous constructs in the model had good predictive relevance for endogenous constructs, as Q2 value was above the threshold (0.156 for customer satisfaction, and 0.467 for customer loyalty). The q2 effect size of customer satisfaction on customer loyalty is large (q2= 0.448). VAF accounted for more than 80\% of both indirect effects, indicating the importance of customer satisfaction on the relationship between service quality and customer loyalty, and between perceived value and customer loyalty.},
	language = {en},
	number = {6},
	urldate = {2023-09-29},
	journal = {International Journal of Business and Management},
	author = {Muhammad, Irfan and Farid Shamsudin, Mohammad and Hadi, Noor Ul},
	month = may,
	year = {2016},
	pages = {57},
	file = {Muhammad et al. - 2016 - How Important Is Customer Satisfaction Quantitati.pdf:C\:\\Users\\franc\\Zotero\\storage\\T2QYZ5ZT\\Muhammad et al. - 2016 - How Important Is Customer Satisfaction Quantitati.pdf:application/pdf},
}

2
@article{raassens_nps_2017,
	title = {{NPS} and {Online} {WOM}: {Investigating} the {Relationship} {Between} {Customers}’ {Promoter} {Scores} and {eWOM} {Behavior}},
	volume = {20},
	issn = {1094-6705, 1552-7379},
	shorttitle = {{NPS} and {Online} {WOM}},
	url = {http://journals.sagepub.com/doi/10.1177/1094670517696965},
	doi = {10.1177/1094670517696965},
	abstract = {The Net Promoter Score (NPS) is, according to Reichheld, the single most reliable indicator of company growth, and many companies use this recommendation-based technique for measuring customer loyalty. Despite its widespread adoption by many companies across multiple industries, the debate about NPS goes on. A major concern is that managers treat NPS as being equivalent across customers, which is often very misleading. By using a unique data set that combines customers’ promoter scores and online word-of-mouth (eWOM) behavior, this research studies how individual customers’ promoter scores are related to eWOM, including its relationship with the three categories of customers that are identified by the NPS paradigm (i.e., promoters, passives, and detractors). Based on a sample of 189 customers, their promoter scores and corresponding eWOM, the results show that there is a positive relationship between customers’ promoter scores and the valence of online messages. Further, while detractors and promoters are homogeneous with respect to the valence of the eWOM messages they spread, passives show message valenceheterogeneity. Thus, although passives, the largest group of customers, have no weight in calculating the NPS, our results reveal that companies should flag passives for further attention and action.},
	language = {en},
	number = {3},
	urldate = {2023-09-30},
	journal = {Journal of Service Research},
	author = {Raassens, Néomie and Haans, Hans},
	month = aug,
	year = {2017},
	pages = {322--334},
	file = {Raassens and Haans - 2017 - NPS and Online WOM Investigating the Relationship.pdf:C\:\\Users\\franc\\Zotero\\storage\\H5F5XLI8\\Raassens and Haans - 2017 - NPS and Online WOM Investigating the Relationship.pdf:application/pdf},
}
3

@article{mustafa_customer_2021,
	title = {Customer churn prediction for telecommunication industry: {A} {Malaysian} {Case} {Study}},
	volume = {10},
	issn = {2046-1402},
	shorttitle = {Customer churn prediction for telecommunication industry},
	url = {https://f1000research.com/articles/10-1274/v1},
	doi = {10.12688/f1000research.73597.1},
	abstract = {Background: Customer churn is a term that refers to the rate at which customers leave the business. Churn could be due to various factors, including switching to a competitor, cancelling their subscription because of poor customer service, or discontinuing all contact with a brand due to insufficient touchpoints. Long-term relationships with customers are more effective than trying to attract new customers. A rise of 5\% in customer satisfaction is followed by a 95\% increase in sales. By analysing past behaviour, companies can anticipate future revenue. This article will look at which variables in the Net Promoter Score (NPS) dataset influence customer churn in Malaysia's telecommunications industry. The aim of This study was to identify the factors behind customer churn and propose a churn prediction framework currently lacking in the telecommunications industry.
Methods: This study applied data mining techniques to the NPS dataset from a Malaysian telecommunications company in September 2019 and September 2020, analysing 7776 records with 30 fields to determine which variables were significant for the churn prediction model. We developed a propensity for customer churn using the Logistic Regression, Linear Discriminant Analysis, K-Nearest Neighbours Classifier, Classification and Regression Trees (CART), Gaussian Naïve Bayes, and Support Vector Machine using 33 variables.
Results: Customer churn is elevated for customers with a low NPS. However, an immediate helpdesk can act as a neutral party to ensure that the customer needs are met and to determine an employee's ability to obtain customer satisfaction.
Conclusions: It can be concluded that CART has the most accurate churn prediction (98\%). However, the research is prohibited from accessing personal customer information under Malaysia's data protection policy. Results are expected for other businesses to measure potential customer churn using NPS scores to gather customer feedback.},
	language = {en},
	urldate = {2023-09-30},
	journal = {F1000Research},
	author = {Mustafa, Nurulhuda and Sook Ling, Lew and Abdul Razak, Siti Fatimah},
	month = dec,
	year = {2021},
	pages = {1274},
	file = {Mustafa et al. - 2021 - Customer churn prediction for telecommunication in.pdf:C\:\\Users\\franc\\Zotero\\storage\\UV57V6WW\\Mustafa et al. - 2021 - Customer churn prediction for telecommunication in.pdf:application/pdf},
}
4

@article{ullah_churn_2019,
	title = {A {Churn} {Prediction} {Model} {Using} {Random} {Forest}: {Analysis} of {Machine} {Learning} {Techniques} for {Churn} {Prediction} and {Factor} {Identification} in {Telecom} {Sector}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {A {Churn} {Prediction} {Model} {Using} {Random} {Forest}},
	url = {https://ieeexplore.ieee.org/document/8706988/},
	doi = {10.1109/ACCESS.2019.2914999},
	abstract = {In the telecom sector, a huge volume of data is being generated on a daily basis due to a vast client base. Decision makers and business analysts emphasized that attaining new customers is costlier than retaining the existing ones. Business analysts and customer relationship management (CRM) analyzers need to know the reasons for churn customers, as well as, behavior patterns from the existing churn customers’ data. This paper proposes a churn prediction model that uses classiﬁcation, as well as, clustering techniques to identify the churn customers and provides the factors behind the churning of customers in the telecom sector. Feature selection is performed by using information gain and correlation attribute ranking ﬁlter. The proposed model ﬁrst classiﬁes churn customers data using classiﬁcation algorithms, in which the Random Forest (RF) algorithm performed well with 88.63\% correctly classiﬁed instances. Creating effective retention policies is an essential task of the CRM to prevent churners. After classiﬁcation, the proposed model segments the churning customer’s data by categorizing the churn customers in groups using cosine similarity to provide group-based retention offers. This paper also identiﬁed churn factors that are essential in determining the root causes of churn. By knowing the signiﬁcant churn factors from customers’ data, CRM can improve productivity, recommend relevant promotions to the group of likely churn customers based on similar behavior patterns, and excessively improve marketing campaigns of the company. The proposed churn prediction model is evaluated using metrics, such as accuracy, precision, recall, f-measure, and receiving operating characteristics (ROC) area. The results reveal that our proposed churn prediction model produced better churn classiﬁcation using the RF algorithm and customer proﬁling using k-means clustering. Furthermore, it also provides factors behind the churning of churn customers through the rules generated by using the attribute-selected classiﬁer algorithm.},
	language = {en},
	urldate = {2023-09-30},
	journal = {IEEE Access},
	author = {Ullah, Irfan and Raza, Basit and Malik, Ahmad Kamran and Imran, Muhammad and Islam, Saif Ul and Kim, Sung Won},
	year = {2019},
	pages = {60134--60149},
	file = {Ullah et al. - 2019 - A Churn Prediction Model Using Random Forest Anal.pdf:C\:\\Users\\franc\\Zotero\\storage\\TP474I39\\Ullah et al. - 2019 - A Churn Prediction Model Using Random Forest Anal.pdf:application/pdf},
}

5
@article{zhou_measuring_2019,
	title = {Measuring e-service quality and its importance to customer satisfaction and loyalty: an empirical study in a telecom setting},
	volume = {19},
	issn = {1389-5753, 1572-9362},
	shorttitle = {Measuring e-service quality and its importance to customer satisfaction and loyalty},
	url = {http://link.springer.com/10.1007/s10660-018-9301-3},
	doi = {10.1007/s10660-018-9301-3},
	abstract = {The important influence of e-service quality (e-SQ) on customer satisfaction and loyalty has been demonstrated in many contexts, but has not been examined in telecom settings yet. The current study aimed to construct a measurement scale for e-SQ in telecom settings, as well as to investigate the relationship between e-SQ, customer satisfaction, and customer loyalty. In this study, we analyzed selfreports from 9249 respondents (74.55\% were male) between the ages of 19 and 45. A scale consisting of five user experinece dimensions (functional completeness, performance, interface and interaction quality, content and information, support or service) was developed to measure e-SQ in the telecom industry. The scale was proven reliable and valid. The analysis confirmed a positive relationship between e-SQ, customer satisfaction and loyalty. In addition, e-SQ was found to be a core predictor of customer satisfaction and customer loyalty. Moreover, customer satisfaction emerged as the strongest predictor of customer loyalty.},
	language = {en},
	number = {3},
	urldate = {2023-09-30},
	journal = {Electronic Commerce Research},
	author = {Zhou, Ronggang and Wang, Xiaorui and Shi, Yuhan and Zhang, Renqian and Zhang, Leyuan and Guo, Haiyan},
	month = sep,
	year = {2019},
	pages = {477--499},
	file = {Zhou et al. - 2019 - Measuring e-service quality and its importance to .pdf:C\:\\Users\\franc\\Zotero\\storage\\L8WGBJQM\\Zhou et al. - 2019 - Measuring e-service quality and its importance to .pdf:application/pdf},
}
6

@article{izogo_customer_2017,
	title = {Customer loyalty in telecom service sector: the role of service quality and customer commitment},
	volume = {29},
	issn = {1754-2731},
	shorttitle = {Customer loyalty in telecom service sector},
	url = {https://www.emerald.com/insight/content/doi/10.1108/TQM-10-2014-0089/full/html},
	doi = {10.1108/TQM-10-2014-0089},
	abstract = {Purpose – The purpose of this paper is to examine how firms can influence customer loyalty through customer commitment by leveraging two constructs of service quality: service assurance and service reliability. Design/methodology/approach – The analyses are based on 138 responses retrieved from experienced users of mobile phone services in one of the big cities in the South-eastern part of Nigeria through a survey questionnaire. The validity and reliability of the measurement model as well as the proposed hypotheses were examined through the partial least squares structural equation modelling procedure.},
	language = {en},
	number = {1},
	urldate = {2023-09-30},
	journal = {The TQM Journal},
	author = {Izogo, Ernest Emeka},
	month = jan,
	year = {2017},
	pages = {19--36},
	file = {Izogo - 2017 - Customer loyalty in telecom service sector the ro.pdf:C\:\\Users\\franc\\Zotero\\storage\\QSIQZQ8D\\Izogo - 2017 - Customer loyalty in telecom service sector the ro.pdf:application/pdf},
}
7

@article{ahmed_reliability_2017,
	title = {Reliability {Modeling} and {Analysis} of {Communication} {Networks}},
	volume = {78},
	issn = {10848045},
	url = {http://arxiv.org/abs/1612.08910},
	doi = {10.1016/j.jnca.2016.11.008},
	abstract = {In recent times, the functioning of various aspects of modern society—ranging from the various infrastructural utilities such as electrical power, water to socio-economical aspects such as telecommunications, business, commerce, education—has become critically reliant on communication networks, and particularly on the Internet. With the migration of critical facilities to the Internet, it has become vitally important to ensure the reliability and availability of networks. In this paper, we study various modeling and analysis techniques that can aid in the study of reliability of communication networks. In this regard, we provide background on the modeling techniques (such as reliability block diagrams, fault trees, Markov chains, etc.) and analysis techniques (such as mathematical analytical methods, simulation methods, and formal methods). Apart from providing the necessary background, we also critically evaluate the pros and cons of diﬀerent approaches, and provide a detailed survey of their applications in communication networks. To the best of our knowledge, this is the ﬁrst in-depth review of the application of reliability modeling and analysis techniques in communication networks.},
	language = {en},
	urldate = {2023-09-30},
	journal = {Journal of Network and Computer Applications},
	author = {Ahmed, Waqar and Hasan, Osman and Pervez, Usman and Qadir, Junaid},
	month = jan,
	year = {2017},
	note = {arXiv:1612.08910 [cs]},
	keywords = {Computer Science - Networking and Internet Architecture},
	pages = {191--215},
	annote = {Comment: 82},
	file = {Ahmed et al. - 2017 - Reliability Modeling and Analysis of Communication.pdf:C\:\\Users\\franc\\Zotero\\storage\\7UCS9KYU\\Ahmed et al. - 2017 - Reliability Modeling and Analysis of Communication.pdf:application/pdf},
}
8
@article{ahmad_customer_2019,
	title = {Customer churn prediction in telecom using machine learning in big data platform},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0191-6},
	doi = {10.1186/s40537-019-0191-6},
	abstract = {Customer churn is a major problem and one of the most important concerns for large companies. Due to the direct effect on the revenues of the companies, especially in the telecom field, companies are seeking to develop means to predict potential customer to churn. Therefore, finding factors that increase customer churn is important to take necessary actions to reduce this churn. The main contribution of our work is to develop a churn prediction model which assists telecom operators to predict customers who are most likely subject to churn. The model developed in this work uses machine learning techniques on big data platform and builds a new way of features’ engineering and selection. In order to measure the performance of the model, the Area Under Curve (AUC) standard measure is adopted, and the AUC value obtained is 93.3\%. Another main contribution is to use customer social network in the prediction model by extracting Social Network Analysis (SNA) features. The use of SNA enhanced the performance of the model from 84 to 93.3\% against AUC standard. The model was prepared and tested through Spark environment by working on a large dataset created by transforming big raw data provided by SyriaTel telecom company. The dataset contained all customers’ information over 9 months, and was used to train, test, and evaluate the system at SyriaTel. The model experimented four algorithms: Decision Tree, Random Forest, Gradient Boosted Machine Tree “GBM” and Extreme Gradient Boosting “XGBOOST”. However, the best results were obtained by applying XGBOOST algorithm. This algorithm was used for classification in this churn predictive model.},
	language = {en},
	number = {1},
	urldate = {2023-09-30},
	journal = {Journal of Big Data},
	author = {Ahmad, Abdelrahim Kasem and Jafar, Assef and Aljoumaa, Kadan},
	month = dec,
	year = {2019},
	pages = {28},
	file = {Ahmad et al. - 2019 - Customer churn prediction in telecom using machine.pdf:C\:\\Users\\franc\\Zotero\\storage\\BMEUQTXG\\Ahmad et al. - 2019 - Customer churn prediction in telecom using machine.pdf:application/pdf},
}

9
@article{tong_research_2017,
	title = {The research of customer loyalty improvement in telecom industry based on {NPS} data mining},
	volume = {14},
	issn = {1673-5447},
	url = {http://ieeexplore.ieee.org/document/8233665/},
	doi = {10.1109/CC.2017.8233665},
	abstract = {In recent years, the telecommunications have used the concept of NPS (Net Promoter Score) for customer relationship management, but there is neither definite theory research nor instructive instance research. However, this paper summarizes an approach with instance case analysis to improve customer loyalty via NPS data mining, which has extensive and practical significance for tele-companies. First, this paper finds some driven forces of customer loyalty, which are relative to customer consumption such as the call duration, the usage of data, ARPU, etc., by using some innovative reasoning-analysis based on IG (Information Gain) and xg-boost decision-making tree model, so the tele-companies can predict the role of individual customer and form daily monitoring on big data, which will save a lot of NPS survey cost. Second, this paper summarizes how customer group feature impacts the relationship between NPS and financial performance. Taking ARPU value as the performance goals, we divide the sample customers into 6 groups and summarize their characteristics based on k-means clustering, and give targeted suggestion of each group.},
	language = {en},
	number = {11},
	urldate = {2023-09-29},
	journal = {China Communications},
	author = {Tong, Lili and Wang, Yiting and Wen, Fan and Li, Xiaowen},
	month = nov,
	year = {2017},
	pages = {260--268},
	file = {Tong et al. - 2017 - The research of customer loyalty improvement in te.pdf:C\:\\Users\\franc\\Zotero\\storage\\26BS8LH5\\Tong et al. - 2017 - The research of customer loyalty improvement in te.pdf:application/pdf},
}

10
@article{jain_telecom_2021,
	title = {Telecom churn prediction and used techniques, datasets and performance measures: a review},
	volume = {76},
	issn = {1018-4864, 1572-9451},
	shorttitle = {Telecom churn prediction and used techniques, datasets and performance measures},
	url = {https://link.springer.com/10.1007/s11235-020-00727-0},
	doi = {10.1007/s11235-020-00727-0},
	abstract = {Customer churn prediction in telecommunication industry is a very essential factor to be achieved and it makes direct impact to customer retention and its revenues. Developing a good and effective churn prediction model is very important however it is a time-consuming process. This study presents a very good review of customer churn, its effects, identiﬁcation of its causes, business needs, methods, and all the techniques used for churn prediction. On the other hand, this study provides the best understanding of the telecom dataset, datasets used by past researches and features used in different researches. Also, this study shows the best techniques used for the churn prediction and describes all performance measures used in the churn prediction models. In this study a wide range of researches are added from the year 2005 to 2020. It includes variety of methods proposed by past researches and technologies used in these researches. At the end, a state of art comparison is added that gives a very good and meaningful comparison of past researches. The study indicates that machine learning techniques are mostly used and feature extraction is a very important task for developing an effective churn prediction model. Deep learning algorithm CNN itself has the capability of feature extraction and establish itself as a powerful technique for churn model, in particular for large datasets. For performance ‘Accuracy’ is a good measure however measuring performance only with ‘Accuracy’ is not sufﬁcient because on small datasets accuracy is more predictable and will be the same. With Accuracy, researchers also need to look at other performance measures such as confusion matrix, ROC, precision. F-measure etc. This study assures that new researchers can ﬁnd everything regarding their churn prediction model requirements at one place. This study provides a comprehensive view by extensively detailing work which has happened in this area and will act as a rich repositorory of all knowledge regarding churn prediction in telecom sector.},
	language = {en},
	number = {4},
	urldate = {2023-10-03},
	journal = {Telecommunication Systems},
	author = {Jain, Hemlata and Khunteta, Ajay and Srivastava, Sumit},
	month = apr,
	year = {2021},
	pages = {613--630},
	file = {Jain et al. - 2021 - Telecom churn prediction and used techniques, data.pdf:C\:\\Users\\franc\\Zotero\\storage\\TF3XXA3V\\Jain et al. - 2021 - Telecom churn prediction and used techniques, data.pdf:application/pdf},
}
11
@article{seymen_customer_2023,
	title = {Customer {Churn} {Prediction} {Using} {Ordinary} {Artificial} {Neural} {Network} and {Convolutional} {Neural} {Network} {Algorithms}: {A} {Comparative} {Performance} {Assessment}},
	volume = {36},
	issn = {2147-1762},
	shorttitle = {Customer {Churn} {Prediction} {Using} {Ordinary} {Artificial} {Neural} {Network} and {Convolutional} {Neural} {Network} {Algorithms}},
	url = {http://dergipark.org.tr/en/doi/10.35378/gujs.992738},
	doi = {10.35378/gujs.992738},
	abstract = {Churn studies have been used for many years to increase profitability as well as to make customercompany relations sustainable. Ordinary artificial neural network (ANN) and convolution neural network (CNN) are widely used in churn analysis due to their ability to process large amounts of customer data. In this study, an ANN and a CNN model are proposed to predict whether customers in the retail industry will churn in the future. The models we proposed were compared with many machine learning methods that are frequently used in churn prediction studies. The results of the models were compared via accuracy classification tools, which are precision, recall, and AUC. The study results showed that the proposed deep learning-based churn prediction model has a better classification performance. The CNN model produced a 97.62\% of accuracy rate which resulted in a better classification and prediction success than other compared models.},
	language = {en},
	number = {2},
	urldate = {2023-10-03},
	journal = {Gazi University Journal of Science},
	author = {Seymen, Omer Faruk and Ölmez, Emre and Doğan, Onur and Er, Orhan and Hiziroğlu, Kadir},
	month = jun,
	year = {2023},
	pages = {720--733},
	file = {Seymen et al. - 2023 - Customer Churn Prediction Using Ordinary Artificia.pdf:C\:\\Users\\franc\\Zotero\\storage\\BFMRWD2T\\Seymen et al. - 2023 - Customer Churn Prediction Using Ordinary Artificia.pdf:application/pdf},
}

@Article{effects2003,
  title = {Effect Displays in {R} for Generalised Linear
    Models},
  author = {John Fox},
  journal = {Journal of Statistical Software},
  year = {2003},
  volume = {8},
  number = {15},
  pages = {1--27},
  doi = {10.18637/jss.v008.i15},
}

@article{hand_simple_2001,
    title = {A {Simple} {Generalisation} of the {Area} {Under} the {ROC} {Curve} for {Multiple} {Class} {Classification} {Problems}},
    abstract = {The area under the ROC curve, or the equivalent Gini index, is a widely used measure of performance of supervised classiﬁcation rules. It has the attractive property that it side-steps the need to specify the costs of the different kinds of misclassiﬁcation. However, the simple form is only applicable to the case of two classes. We extend the deﬁnition to the case of more than two classes by averaging pairwise comparisons. This measure reduces to the standard form in the two class case. We compare its properties with the standard measure of proportion correct and an alternative deﬁnition of proportion correct based on pairwise comparison of classes for a simple artiﬁcial case and illustrate its application on eight data sets. On the data sets we examined, the measures produced similar, but not identical results, reﬂecting the different aspects of performance that they were measuring. Like the area under the ROC curve, the measure we propose is useful in those many situations where it is impossible to give costs for the different kinds of misclassiﬁcation.},
    language = {en},
    author = {Hand, David J},
    month = jan,
    year = {2001},
}

@Manual{R-effects,
  title = {effects: Effect Displays for Linear, Generalized
    Linear, and Other Models},
  author = {John Fox and Sanford Weisberg and Brad Price
    and Michael Friendly and Jangman Hong},
  year = {2022},
  note = {R package version 4.2-2},
  url = {https://www.r-project.org},
}

@article{minastireanu_light_2019,
	title = {Light {GBM} {Machine} {Learning} {Algorithm} to {Online} {Click} {Fraud} {Detection}},
	issn = {21659923, 21659923},
	url = {https://ibimapublishing.com/articles/JIACS/2019/263928/},
	doi = {10.5171/2019.263928},
	abstract = {In the current web advertising activities, the fraud increases the number of risks for online marketing, advertising industry and e-business. The click fraud is considered one of the most critical issues in online advertising. Even if the online advertisers make permanent efforts to improve the traffic filtering techniques, they are still looking for the best protection methods to detect click frauds.},
	language = {en},
	urldate = {2023-10-31},
	journal = {Journal of Information Assurance \& Cybersecurity},
	author = {Minastireanu, Elena-Adriana and Mesnita, Gabriela},
	month = apr,
	year = {2019},
	pages = {1--12},
	file = {Minastireanu and Mesnita - 2019 - Light GBM Machine Learning Algorithm to Online Cli.pdf:C\:\\Users\\franc\\Zotero\\storage\\58BQHEFH\\Minastireanu and Mesnita - 2019 - Light GBM Machine Learning Algorithm to Online Cli.pdf:application/pdf},
}

@article{microsoft_corporation_light_2023,
	title = {Light {GBMs} {Documentation}},
	shorttitle = {{LightGBMsDocumentation}},
	url = {https://lightgbm.readthedocs.io/en/stable/index.html},
	author = {Microsoft Corporation, Microsof},
	year = {2023},
}

@article{masui_all_2022,
	title = {All {You} {Need} to {Know} about {Gradient} {Boosting} {Algorithm} − {Part} 1. {Regression}},
	url = {https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502},
	author = {Masui, Tomonori},
	year = {2022},
}

@misc{friedman_greedy_1999,
	title = {Greedy {Function} {Approximation}: {A} {Gradient} {Boosting} {Machine}},
	publisher = {IMS 1999 Reitz Lecture},
	author = {Friedman, Jerome H.},
	year = {1999},
	file = {trebst.pdf:C\:\\Users\\franc\\Zotero\\storage\\PEIPSYTE\\trebst.pdf:application/pdf},
}

@article{saha_xgboost_2023,
	title = {{XGBoost} vs {LightGBM}: {How} {Are} {They} {Different}},
	url = {https://neptune.ai/blog/xgboost-vs-lightgbm},
	author = {Saha, Sumit},
	year = {2023},
}

@article{brownlee_information_2020,
	title = {Information {Gain} and {Mutual} {Information} for {Machine} {Learning}},
	url = {https://machinelearningmastery.com/information-gain-and-mutual-information/#:~:text=Information%20gain%20is%20the%20reduction,before%20and%20after%20a%20transformation.},
	author = {Brownlee, Jason},
	year = {2020},
}

@article{ruto_hot_2022,
	title = {Hot to {Get} {Started} with the {Boruta} {Algorithm} in {Machine} {Learning}},
	url = {https://www.section.io/engineering-education/getting-started-with-boruta-algorithm/},
	author = {Ruto, Nelson},
	year = {2022},
}

@article{ke_lightgbm_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efﬁciency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a signiﬁcant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that ﬁnding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
	language = {en},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	month = jan,
	year = {2017},
}

@misc{hastie_introduction_2023,
    title = {An {Introduction} to {GLMNET}},
    copyright = {Standford University},
    url = {https://stanford.io/3QGGqcD},
    abstract = {glmnet},
    language = {en},
    urldate = {2023-11-01},
    author = {Hastie, Trevor and Quian, Junyang and Tay, Kennet},
    year = {2023},
    file = {Snapshot:C\:\\Users\\wchavarria\\Zotero\\storage\\YG5RBKRR\\glmnet.html:text/html},
}

@book{herrera_multilabel_2016,
    address = {New York, NY},
    title = {Multilabel classification},
    isbn = {978-3-319-41110-1},
    publisher = {Springer Berlin Heidelberg},
    author = {Herrera, Francisco},
    year = {2016},
}

@misc{pawley_github_2022,
    title = {{GitHub} - stevenpawley/colino: {Recipes} {Steps} for {Supervised} {Filter}-{Based} {Feature} {Selection}},
    shorttitle = {{GitHub} - stevenpawley/colino},
    url = {https://github.com/stevenpawley/colino},
    abstract = {Recipes Steps for Supervised Filter-Based Feature Selection - GitHub - stevenpawley/colino: Recipes Steps for Supervised Filter-Based Feature Selection},
    language = {en},
    urldate = {2023-11-01},
    journal = {GitHub},
    author = {Pawley, Steven},
    year = {2022},
    file = {Snapshot:C\:\\Users\\wchavarria\\Zotero\\storage\\P6TLY78H\\colino.html:text/html},
}

@book{mitchell_machine_2013,
    address = {New York},
    edition = {Nachdr.},
    series = {{McGraw}-{Hill} series in {Computer} {Science}},
    title = {Machine learning},
    isbn = {978-0-07-115467-3 978-0-07-042807-2},
    language = {eng},
    publisher = {McGraw-Hill},
    author = {Mitchell, Tom M.},
    year = {2013},
}

@book{stanczyk_advances_2017,
    address = {New York, NY},
    title = {Advances in feature selection for data and pattern recognition},
    isbn = {978-3-319-67587-9},
    publisher = {Springer Berlin Heidelberg},
    author = {Stanczyk, Urszula and Zielosko, Beata and Jain, Lakhmi C.},
    year = {2017},
}

@article{opitz_macro_2021,
    title = {Macro {F1} and {Macro} {F1}},
    url = {http://arxiv.org/abs/1911.03347},
    abstract = {The 'macro F1' metric is frequently used to evaluate binary, multi-class and multi-label classification problems. Yet, we find that there exist two different formulas to calculate this quantity. In this note, we show that only under rare circumstances the two computations can be considered equivalent. More specifically, one formula well 'rewards' classifiers which produce a skewed error type distribution. In fact, the difference in outcome of the two computations can be as high as 0.5. The two computations may not only diverge in their scalar result but can also lead to different classifier rankings.},
    language = {en},
    urldate = {2023-11-01},
    author = {Opitz, Juri and Burst, Sebastian},
    month = feb,
    year = {2021},
    note = {arXiv:1911.03347 [cs, stat]},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{oliva_navarro_metaheuristics_2021,
    address = {Cham},
    series = {Studies in computational intelligence},
    title = {Metaheuristics in machine learning: theory and applications},
    isbn = {978-3-030-70541-1},
    shorttitle = {Metaheuristics in machine learning},
    language = {eng},
    number = {volume 967},
    publisher = {Springer},
    editor = {Oliva Navarro, Diego Alberto and Houssein, Essam H. and Hinojosa, Salvador},
    year = {2021},
}

@book{fernandez_learning_2018,
    address = {New York, NY},
    title = {Learning from imbalanced data sets},
    isbn = {978-3-319-98073-7},
    publisher = {Springer Science+Business Media},
    author = {Fernandez, Alberto and García, Salvador},
    year = {2018},
}


@article{chawla_smote_2002,
    title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
    volume = {16},
    issn = {1076-9757},
    shorttitle = {{SMOTE}},
    url = {https://www.jair.org/index.php/jair/article/view/10302},
    doi = {10.1613/jair.953},
    abstract = {An approach to the construction of classiﬁers from imbalanced datasets is described. A dataset is imbalanced if the classiﬁcation categories are not approximately equally represented. Often real-world data sets are predominately composed of “normal” examples with only a small percentage of “abnormal” or “interesting” examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classiﬁer to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classiﬁer performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classiﬁer performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classiﬁer. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
    language = {en},
    urldate = {2023-11-01},
    journal = {Journal of Artificial Intelligence Research},
    author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
    month = jun,
    year = {2002},
    pages = {321--357},
}







