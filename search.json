[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Close the Loop",
    "section": "",
    "text": "Resumen\nEn el ámbito de las telecomunicaciones, la mejora de la experiencia del cliente (CX) es esencial para mantener una ventaja competitiva. Este estudio se centra en la aplicación de modelos de aprendizaje automático para optimizar diversas métricas relacionadas con CX. Utilizamos el método de envoltura Boruta y una implementación específica de Lightgbm, un algoritmo de árbol de decisión de impulso de gradiente (GBDT), para predecir y diagnosticar correctamente los distintos tipos de detractores de red. Nuestros experimentos demuestran que el método de envoltura (wrapper) Boruta con Lightgbm supera a otros algoritmos en términos de f1-score macro, precisión y recuperación, tanto en conjuntos de entrenamiento como de prueba, sin signos de sobreajuste. Específicamente, el f1-score en el conjunto de prueba fue de 97.34%, con una precisión de 97.67% y un Recall de 97.12%. Estos resultados demuestran la eficacia de nuestro enfoque, que tiene aplicaciones significativas en la mejora de CX."
  },
  {
    "objectID": "introduccion.html#justificación",
    "href": "introduccion.html#justificación",
    "title": "1  Introducción",
    "section": "1.1 Justificación",
    "text": "1.1 Justificación\nEn un entorno donde los consumidores tienen más opciones que nunca, las empresas como DSF Nicaragura enfrentan el reto de ofrecer no solo un servicio de alta calidad sino también una experiencia de usuario excepcional. Este estudio se centra en aplicar modelos de aprendizaje automático para identificar a los usuarios detractores de la red móvil de DSF Nicaragua en la unidad de negocio B2C y diagnosticar las causas de su insatisfacción.\nLa experiencia del usuario se ha convertido en un pilar crucial para las empresas en la actualidad. Mariano Alonso, director de Customer Experience en DSF, subraya la importancia de cada interacción con el cliente en cada punto de contacto. La tecnología nos permite capturar y analizar estas interacciones, lo cual no solo mejora la experiencia del cliente, sino que también tiene el potencial de aumentar los ingresos para la empresa.\nAdemás, este proyecto tiene objetivos estratégicos claros. Primero, busca determinar en qué sitios se debe invertir en CaPex para infraestructura y mejoras de red con el fin de elevar el CX. Esto es crucial para construir y solidificar la marca. Segundo, en un mercado competitivo donde nuestro principal competidor está cerca de alcanzar nuestro market share, reducir el abandono de clientes (churn) es más importante que nunca. Tercero, la automatización de esta clasificación, que actualmente se realiza de forma manual, se traducirá en un apalancamiento operativo significativo, adaptándose a las nuevas realidades del negocio. (Kuhn and Johnson 2020)"
  },
  {
    "objectID": "introduccion.html#objetivos",
    "href": "introduccion.html#objetivos",
    "title": "1  Introducción",
    "section": "1.2 Objetivos",
    "text": "1.2 Objetivos\n\n1.2.1 General\n\nDesarrollar un modelo de machine learning de clasificación multinomial para automatizar la clasificación de detractores de red móvil en DSF Nicaragua.\n\n\n\n1.2.2 Específicos\n\nAlcanzar un F1-Score macro de al menos el 90% en la clasificación de detractores de red. Este umbral se establece para asegurar que el modelo tenga una alta precisión y recall.\nDiagnosticar y predecir las causas de insatisfacción de red en las siguientes categorías: capacidad, cobertura, optimización y disponibilidad, así como los usuarios que serán promotores.\nImplementar un sistema de priorización predictiva basado en la experiencia actual de los usuarios para guiar las acciones de los equipos técnicos."
  },
  {
    "objectID": "introduccion.html#hipótesis",
    "href": "introduccion.html#hipótesis",
    "title": "1  Introducción",
    "section": "1.3 Hipótesis",
    "text": "1.3 Hipótesis\n\nLa implementación de modelos de machine learning puede automatizar de manera efectiva la clasificación de detractores de red móvil en DSF Nicaragua.\nLa información derivada de la clasificación automatizada de detractores de red móvil puede ser utilizada para priorizar acciones de mejora en la red de DSF Nicaragua, resultando en una mejora cuantificable en indicadores clave de rendimiento (KPIs).\n\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. Boca Raton London New York: CRC Press, Taylor & Francis Group."
  },
  {
    "objectID": "antecedentes.html",
    "href": "antecedentes.html",
    "title": "2  Antecedentes",
    "section": "",
    "text": "El estudio de la satisfacción del cliente en la compañía es un indicador crítico de gran relevancia (Muhammad, Farid Shamsudin, and Hadi 2016). Entender el estado actual de la satisfacción permite a la empresa dirigir sus esfuerzos para fortalecer la relación con el cliente, fomentar la recompra y la lealtad hacia la marca (Zhou et al. 2019). Asimismo, proporciona la capacidad de identificar y abordar los problemas que los clientes puedan estar experimentando.\nExisten distintos enfoques para el estudio de los clientes en el ámbito de las telecomunicaciones. Uno de ellos se enfoca en la generación de conocimiento para una mejor comprensión de los usuarios. El análisis de (Mustafa, Sook Ling, and Abdul Razak 2021) sobre el NPS y su relación con el porcentaje de usuarios que abandonan la compañía (churn) en Malasia, utilizando un modelo de Clasificación y Árboles de Regresión (CART), revela que los bajos puntajes en NPS incrementan la probabilidad de que los clientes terminen su relación con la empresa. Una limitación de este indicador en DSF Nicaragua es que no todos los clientes reportan sus malas experiencias, lo que reduce la visibilidad de problemas potenciales.\nLa predicción de churn es otra área con extensa investigación. Determinar la probabilidad de que un cliente deje la empresa es una información valiosa (Ullah et al. 2019). Este estudio clasifica la probabilidad de churn en categorías de bajo, medio y alto riesgo, lo que ayuda a diseñar estrategias de retención de clientes diferenciadas. La calidad de un modelo de churn se basa en una ingeniería de datos adecuada (Jain, Khunteta, and Srivastava 2021). Para la evaluación de estos modelos, se sugiere el uso de métricas que refuercen la precisión, como el área bajo la curva ROC y el f1-score. Además, se ha comprobado que las Redes Neuronales Convolucionales (CNN) son algoritmos efectivos para procesar grandes volúmenes de datos de clientes (Seymen et al. 2023).\nIncrementar la lealtad del cliente es un objetivo deseado por las compañías, ya que esto se traduce en mayores ingresos. La fiabilidad del servicio es un predictor directo de la lealtad del cliente, a diferencia de la garantía de nivel de servicio, que no lo es (Izogo 2017). La fiabilidad se define como el correcto funcionamiento de un sistema durante un tiempo establecido (Ahmed et al. 2017). Un funcionamiento constante de la red de telecomunicaciones en DSF Nicaragua podría contribuir a mantener y aumentar su cuota de mercado.\nEn la investigación de variables para predecir el churn, (Ahmad, Jafar, and Aljoumaa 2019) concluye que factores como los días desde la última transacción saliente, el acceso a tecnologías 2G, 3G y 4G, y el saldo en la billetera móvil, son significativos para mejorar la predicción del churn. El estudio también señala la relevancia de variables vinculadas al análisis de la red social del usuario, como la medida de similitud del coseno entre los nodos de la red.\nCon el conocimiento de las razones por las cuales los clientes se convierten en detractores, se implementan modelos de clasificación de aprendizaje supervisado. Algoritmos útiles en este contexto incluyen árboles de decisión, bosques aleatorios, GBM y XGBOOST (Ahmad, Jafar, and Aljoumaa 2019). XGBOOST ha demostrado ser un algoritmo de predicción eficaz en la clasificación de usuarios de una red. Estos algoritmos tienen la ventaja de ser resistentes a conjuntos de datos desbalanceados, una situación común en estudios de detracción de clientes.\n\n\n\n\nAhmad, Abdelrahim Kasem, Assef Jafar, and Kadan Aljoumaa. 2019. “Customer Churn Prediction in Telecom Using Machine Learning in Big Data Platform.” Journal of Big Data 6 (1): 28. https://doi.org/10.1186/s40537-019-0191-6.\n\n\nAhmed, Waqar, Osman Hasan, Usman Pervez, and Junaid Qadir. 2017. “Reliability Modeling and Analysis of Communication Networks.” Journal of Network and Computer Applications 78 (January): 191–215. https://doi.org/10.1016/j.jnca.2016.11.008.\n\n\nIzogo, Ernest Emeka. 2017. “Customer Loyalty in Telecom Service Sector: The Role of Service Quality and Customer Commitment.” The TQM Journal 29 (1): 19–36. https://doi.org/10.1108/TQM-10-2014-0089.\n\n\nJain, Hemlata, Ajay Khunteta, and Sumit Srivastava. 2021. “Telecom Churn Prediction and Used Techniques, Datasets and Performance Measures: A Review.” Telecommunication Systems 76 (4): 613–30. https://doi.org/10.1007/s11235-020-00727-0.\n\n\nMuhammad, Irfan, Mohammad Farid Shamsudin, and Noor Ul Hadi. 2016. “How Important Is Customer Satisfaction? Quantitative Evidence from Mobile Telecommunication Market.” International Journal of Business and Management 11 (6): 57. https://doi.org/10.5539/ijbm.v11n6p57.\n\n\nMustafa, Nurulhuda, Lew Sook Ling, and Siti Fatimah Abdul Razak. 2021. “Customer Churn Prediction for Telecommunication Industry: A Malaysian Case Study.” F1000Research 10 (December): 1274. https://doi.org/10.12688/f1000research.73597.1.\n\n\nSeymen, Omer Faruk, Emre Ölmez, Onur Doğan, Orhan Er, and Kadir Hiziroğlu. 2023. “Customer Churn Prediction Using Ordinary Artificial Neural Network and Convolutional Neural Network Algorithms: A Comparative Performance Assessment.” Gazi University Journal of Science 36 (2): 720–33. https://doi.org/10.35378/gujs.992738.\n\n\nUllah, Irfan, Basit Raza, Ahmad Kamran Malik, Muhammad Imran, Saif Ul Islam, and Sung Won Kim. 2019. “A Churn Prediction Model Using Random Forest: Analysis of Machine Learning Techniques for Churn Prediction and Factor Identification in Telecom Sector.” IEEE Access 7: 60134–49. https://doi.org/10.1109/ACCESS.2019.2914999.\n\n\nZhou, Ronggang, Xiaorui Wang, Yuhan Shi, Renqian Zhang, Leyuan Zhang, and Haiyan Guo. 2019. “Measuring e-Service Quality and Its Importance to Customer Satisfaction and Loyalty: An Empirical Study in a Telecom Setting.” Electronic Commerce Research 19 (3): 477–99. https://doi.org/10.1007/s10660-018-9301-3."
  },
  {
    "objectID": "marco-teorico.html#clasificación-multiclase-desbalanceada",
    "href": "marco-teorico.html#clasificación-multiclase-desbalanceada",
    "title": "3  Marco Teórico",
    "section": "3.1 Clasificación Multiclase Desbalanceada",
    "text": "3.1 Clasificación Multiclase Desbalanceada\nUn conjunto de datos se considera desbalanceado cuando existe una distribución desigual significativa, o en algunos casos extrema, entre el número de ejemplos de cada clase. En este escenario desbalanceado, el desafío más notable es la presencia de múltiples clases minoritarias y mayoritarias. Esta situación hace que ya no sea posible centrarse en reforzar el aprendizaje hacia una sola clase. Sin embargo, esta no es la única dificultad al abordar conjuntos de datos multiclase desbalanceados. Cualquier característica intrínseca del conjunto de datos que degrade el rendimiento en el caso binario se acentúa aún más en el contexto multiclase.\nEntre las estrategias de descomposición más populares para tratar el desbalance en la clasificación multiclase, destacan los enfoques One-vs-One (OVO) y One-vs-All (OVA). El enfoque OVO divide el problema original en tantos pares de clases como sea posible, ignorando los ejemplos que no pertenecen a las clases relacionadas. Estos se aprenden de manera independiente mediante los llamados aprendices o clasificadores base del conjunto. Por otro lado, el enfoque OVA toma una clase como “positiva” y el conjunto de las restantes como “negativas”, resultando en tantos clasificadores como clases, cada uno dedicado a reconocer una única clase (Fernandez and García 2018).\n\n3.1.1 SMOTE\nSMOTE (Synthetic Minority Over-sampling TEchnique), es un enfoque que aborda el problema de desbalance en conjuntos de datos al generar ejemplos sintéticos de la clase minoritaria en lugar de simplemente replicar los existentes. Este método opera en el “espacio de características” y crea nuevas muestras sintéticas al tomar en cuenta los vecinos más cercanos de cada muestra de la clase minoritaria. Concretamente, para cada muestra de la clase minoritaria, se generan ejemplos sintéticos a lo largo de los segmentos de línea que unen a cualquiera o todos los \\(k\\) vecinos más cercanos de la misma clase. Estos vecinos se seleccionan al azar según la cantidad de sobremuestreo necesario. La generación de ejemplos sintéticos se realiza mediante una combinación lineal del vector de características de la muestra en cuestión y su vecino más cercano, utilizando un número aleatorio entre 0 y 1 como coeficiente.\nEste enfoque tiene varias ventajas. Primero, ayuda a generalizar la región de decisión de la clase minoritaria, permitiendo que los clasificadores tengan un rendimiento más robusto. Segundo, la combinación de SMOTE con técnicas de submuestreo suele tener un rendimiento superior al submuestreo simple. Además, los resultados experimentales en diversos conjuntos de datos han demostrado que SMOTE generalmente supera a otros métodos como Ripper o el Clasificador Bayesiano Ingenuo, que también tratan de manejar la distribución sesgada de clases (Chawla et al. 2002)."
  },
  {
    "objectID": "marco-teorico.html#métricas-para-clasificación-multiclase-desbalanceada",
    "href": "marco-teorico.html#métricas-para-clasificación-multiclase-desbalanceada",
    "title": "3  Marco Teórico",
    "section": "3.2 Métricas para clasificación multiclase desbalanceada",
    "text": "3.2 Métricas para clasificación multiclase desbalanceada\nAl igual que en la evaluación de clasificadores binarios, es posible utilizar métricas como la Precisión, Recall, y F1-Score para evaluar el rendimiento de un clasificador multiclase. Sin embargo, tener más de dos clases requiere prestar más atención a las métricas utilizadas.\nLas macro-métricas o métricas de “macro-average” calculan cada métrica para cada clase y luego toman la media aritmética para obtener un valor global. Las fórmulas para todas estas métricas son similares a sus contrapartes en la clasificación binaria, simplemente se dividen por el número de clases, asignando el mismo peso a cada clase (Oliva Navarro, Houssein, and Hinojosa 2021).\nEn el caso de conjuntos de datos desbalanceados o donde algunas clases son más raras que otras, las métricas de “macro-average” son especialmente útiles. Estas métricas aseguran que todas las clases tengan igual peso en el cálculo de la métrica global, independientemente de su frecuencia en el conjunto de datos. Por lo tanto, si todas las clases son igualmente importantes para tu problema, el uso de “macro-average” proporciona una medida de rendimiento más equitativa.\n\n3.2.1 Precisión y Recall\nLa precisión y el Recall en “macro-average” se calcula como:\n\\[\nPrecision_{M} = \\frac{1}{n} \\sum_{i = 1}^{n} \\frac{TP_{i}}{TP_{i} + FP_{i}}\n\\tag{3.1}\\]\n\\[\nRecall_{M} = \\frac{1}{n} \\sum_{i = 1}^{n} \\frac{TP_{i}}{TP_{i} + FN_{i}}\n\\tag{3.2}\\]\nAquí \\(n\\) es el número de clases y \\(TP_{i}\\), \\(FP_{i}\\), \\(FN_{i}\\) son los Verdaderos Positivos, Falsos Positivos y Falsos Negativos para la i-ésima clase, respectivamente.\n\n\n3.2.2 F1-Score Macro\nEl F1-Score Macro, o simplemente “Macro F1,” es una métrica de evaluación de clasificadores que es especialmente útil cuando se trabaja con conjuntos de datos desbalanceados. Según un estudio realizado por (Opitz and Burst 2021), Macro F1 es recomendable para asignar un peso igual a clases frecuentes e infrecuentes. La métrica se calcula tomando el promedio aritmético de los F1-Scores individuales para cada clase. En términos matemáticos, el F1-Score Macro se define como:\n\\[\n\\mathcal{F}_{1} = \\frac{1}{n} \\sum_{x} F1_{x} = \\frac{1}{n} \\sum_{x}\n\\frac{2P_{x}R_{x}}{P_{x} + R_{x}}\n\\tag{3.3}\\]\nAquí, \\(n\\) es el número de clases, \\(P_{x}\\) es la precisión y \\(R_{x}\\) es el recall para la clase \\(x\\). Este enfoque es más robusto en términos de distribución de tipos de error y no solo puede llevar a puntuaciones absolutas diferentes sino también a diferentes clasificaciones de clasificadores. Los autores del estudio recomiendan utilizar Macro F1 para evaluar clasificadores en situaciones de desbalance de clases."
  },
  {
    "objectID": "marco-teorico.html#algoritmos",
    "href": "marco-teorico.html#algoritmos",
    "title": "3  Marco Teórico",
    "section": "3.3 Algoritmos",
    "text": "3.3 Algoritmos\n\n3.3.1 LightGBM\nLightGBM es una implementación más eficiente de Gradient Boosting Decision Tree (GBDT) creada por investigadores de Microsoft en cooperación con la universidad de Pekín, el cual implementa dos nuevas técnicas: Gradient-based One-Side Sampling (GOSS) y Exclusive Feature Bundling (EFB), las cuales pueden ayudar a reducir hasta 20 veces el tiempo de entrenamiento versus un GBDT puro, alcanzando casi la misma precisión. (Ke et al. 2017). La idea fundamental para hacer que LightGBM sea más eficiente es reducir el número de instancias y atributos.\nAunque GBDT alcanza una gran eficiencia, en especial en tareas de clasificación multiclase, tiende a ser muy lento cuando los datos son altamente dimensionales debido a que por cada atributo realiza un scan de todas las instancias con el fin de estimar el information gain de cada punto posible de división, por lo que la complejidad computacional estará determinada tanto por el número de atributos como por el número de instancias.\nPara resolver esto se usan dos nuevos métodos:\nGradient-based One-Side Sampling (GOSS): Se basa en la idea de que las instancias con un gradiente más grande contribuirán más al information gain por lo que al realizar un muestreo de las instancias lo que se hace es retener estas que tienen un gradiente grande (ej. basado en algún umbral predefinido) y con el resto de instancias que tienen un gradiente pequeño, realizar un muestreo. Este sistema ayudará a reducir el tiempo de forma considerable a la vez que conlleva aun estimación más precisa de la ganancia que utilizando un muestreo uniforme.\nExclusive Feature Bundling (EFB): Es una técnica que aprovecha los atributos dispersos en los datos para reducir su dimensionalidad sin sacrificar mucha información. Funciona particularmente bien en escenarios donde hay muchas características categóricas codificadas como one-hot, lo que a menudo da lugar a una matriz dispersa. La clave está en identificar features que son “exclusivos”, lo que significa que raramente toman valores distintos de cero al mismo tiempo. Estos features exclusivos se pueden agrupar en un solo feature.\nLightGBM es más rápido que XGBoost.\n\n\n3.3.2 Glmnet\nGlmnet (Generalized Linear Model with NETwork penalties) es un algoritmo altamente eficiente y rápido que ajusta modelos lineales generalizados mediante la maximización penalizada de la verosimilitud. Glmnet implementa una forma regularizada (penalizada) de regresión logística. La ruta de regularización se calcula para las penalizaciones del lasso o elastic net en una cuadrícula de valores (en la escala logarítmica) para el parámetro de regularización \\(\\lambda\\) Este algoritmo es particularmente veloz y puede aprovechar la dispersión en la matriz de entrada \\(x\\). Las penalizaciones, que pueden ser \\(L1\\) (Lasso) y \\(L2\\) (Ridge), actúan como un mecanismo para prevenir el uso excesivo de variables en el modelo. De esta manera, Glmnet es útil para evitar el sobreajuste, dando como resultado modelos más simples y robustos (parsimoniosos). (Hastie, Quian, and Tay 2023)\nEl modelo multinomial extiende el binomial cuando el número de clases es mayor a dos. Suponga que la variable de respuesta tiene \\(K\\) niveles \\({\\cal G} = {1,2,\\ldots,K}\\). Aquí se modela:\n\\[\nPr\\left (G = k | X = x \\right ) = \\frac{e^{\\beta_{0k}+\\beta_{k^{x}}^{T}}}{\\sum_{\\ell = 1}^{K} e^{\\beta_{0\\ell}+\\beta^{T}_{\\ell^{x}}}}\n\\tag{3.4}\\]\n\n\n3.3.3 Random Forest\nEl Random Forest es un algoritmo basado en un conjunto de árboles de decisión. Cada árbol se desarrolla utilizando una muestra aleatoria del conjunto de datos original, lo que elimina la correlación entre los aprendices básicos. Además, cada división dentro del árbol se crea utilizando solo un subconjunto aleatorio de atributos. La cantidad de estos atributos influye en el equilibrio entre el sesgo y la varianza para el conjunto de entrenamiento."
  },
  {
    "objectID": "marco-teorico.html#técnicas-de-selección-de-características",
    "href": "marco-teorico.html#técnicas-de-selección-de-características",
    "title": "3  Marco Teórico",
    "section": "3.4 Técnicas de selección de características",
    "text": "3.4 Técnicas de selección de características\nLas técnicas de selección de características se dividen en tres clases generales: métodos intrínsecos (o implícitos), métodos de filtro y métodos de envoltura.\n\n3.4.1 Métodos Implícitos\nLos métodos intrínsecos integran la selección de características en el propio proceso de modelado, lo que ofrece varias ventajas. Algunos ejemplos de métodos intrínsecos incluyen:\nModelos basados en árboles y reglas: Estos modelos buscan el mejor predictor y punto de división para que los resultados sean más homogéneos dentro de cada nueva partición. Si un predictor no se usa en ninguna división, es funcionalmente independiente y se excluye del modelo. Ejemplos populares de este tipo de modelos incluyen LightGBM, Random Forest y XGBoost, los cuales son altamente efectivos en la selección intrínseca de características.\nModelos de regularización: Estos modelos utilizan penalizaciones para reducir o eliminar coeficientes de predictores, como en el caso del método Lasso, que reduce algunos coeficientes a cero absoluto, excluyendo así esas características del modelo final. Un ejemplo notable en este ámbito es Glmnet, que implementa regularización eficiente y rápida.\nLa principal ventaja de los métodos intrínsecos es su eficiencia, ya que el proceso de selección está incrustado en la fase de ajuste del modelo, eliminando la necesidad de herramientas de selección de características externas. Sin embargo, una desventaja importante es que la selección de características es dependiente del modelo. Si el conjunto de datos se ajusta mejor a un tipo de modelo que no tiene selección de características intrínsecas, el rendimiento predictivo podría no ser óptimo (Kuhn and Johnson 2020).\n\n\n3.4.2 Métodos basados en filtros\nLos métodos de selección de características basados en filtros realizan un análisis preliminar supervisado de los predictores para identificar cuáles son importantes para el modelado posterior. Este análisis se lleva a cabo generalmente una sola vez antes de pasar al proceso de modelado. Los métodos de filtro pueden considerar cada predictor de forma independiente o en conjunto, aunque el enfoque individual es más común. Estos métodos emplean diversas técnicas de puntuación para cuantificar la importancia de cada predictor en relación con la variable objetivo. Aunque son rápidos y efectivos para captar grandes tendencias en los datos, son propensos a la sobreselección de predictores. Además, la medida de “importancia” en muchos casos podría no estar alineada con el rendimiento predictivo del modelo. Para mitigar las falsas selecciones positivas, a menudo se utiliza un conjunto de datos independiente para evaluar las características seleccionadas.\n\n3.4.2.1 Information Gain\nDado que la entropía es una medida de la impureza en una colección de ejemplos de entrenamiento, ahora podemos definir una medida de la efectividad de un atributo en la clasificación de los datos de entrenamiento. La medida que usaremos, llamada information gain, es simplemente la reducción esperada en la entropía causada por la partición de los ejemplos según este atributo. Más precisamente, la ganancia de información, Gain(S,A), de un atributo A, en relación con una colección de ejemplos S, se define como:\n\\[\n\\text{Entropy}(S) \\equiv \\sum_{i = 1}^{c} - p_{i}\\;\\log_{2}\\;p_{i}\n\\tag{3.5}\\]\ndonde \\(p_{i}\\) es la proporción de \\(S\\) que pertenece a la clase \\(i\\). Hay que tener en cuenta que el logaritmo sigue siendo en base 2 porque la entropía es una medida de la longitud de codificación esperada medida en bits.\n\\[\n\\text{Information Gain}, (S, A) = Entropy, (S)\\; - \\sum_{v \\in Values(A)}^{} \\; \\frac{|S_{v}|}{|S|} Entropy(S_{v})\n\\tag{3.6}\\]\ndonde \\(Values(A)\\) es el conjunto de todos los posibles valores para el atributo \\(A\\), y \\(S_{v}\\) es el subconjunto de \\(S\\) para el cual el atributo \\(A\\) tiene el valor \\(v\\) (es decir, \\(S_{v} = {S \\in S|A(S) = v}\\)) (Mitchell 2013).\nUn ejemplo específico de un método de filtro es la función step_select_infgain() del paquete colino, que utiliza information gain como métrica para evaluar la importancia de un predictor. Este paquete se centra principalmente en métodos de selección de características basados en filtros y está diseñado para integrarse con el paquete tidymodels para recetas de modelado (Pawley 2022).\n\n\n\n3.4.3 Métodos Wrappers\nLos métodos de envoltura (wrappers) evalúan múltiples modelos utilizando procedimientos que agregan y/o eliminan predictores para encontrar la combinación óptima que maximiza el rendimiento del modelo. En esencia, los métodos de envoltura son algoritmos de búsqueda que tratan a los predictores como las entradas y utilizan el rendimiento del modelo como la salida a optimizar.\nEstos métodos pueden adoptar enfoques de búsqueda “voraces” o “no voraces”. Los voraces eligen rápidamente subconjuntos de predictores basados en el rendimiento inmediato del modelo, aunque corren el riesgo de quedar atrapados en óptimos locales. Un ejemplo es la eliminación de características recursiva (RFE), que elimina iterativamente los predictores menos importantes.\nLa principal ventaja de los métodos de envoltura es su capacidad para explorar una amplia gama de subconjuntos de predictores. Sin embargo, son computacionalmente intensivos y tienen un mayor riesgo de sobreajuste, lo cual requiere validación externa.\n\n3.4.3.1 Boruta\nBoruta es una herramienta que explora datos y genera múltiples árboles de decisión basados en muestras, combinándolos mediante votación mayoritaria. Utiliza modelos de Random Forest para estimar la relevancia de las características. En el paquete Boruta, se crean variables aleatorias utilizando múltiples combinaciones de otras variables en el conjunto de datos. Estas nuevas variables se combinan con las originales para entrenar un Random Forest diferente. La importancia de las distintas características se obtiene comparando la importancia de las variables aleatorias con la de las variables originales. Solo las variables con una importancia mayor que la de las variables aleatorias se consideran importantes. El paquete Boruta puede ser muy exigente en tiempo si el número de variables es alto, especialmente porque el algoritmo crea aún más variables para clasificar sus características (Stanczyk, Zielosko, and Jain 2017).\n\n\n\n\nChawla, N. V., K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. 2002. “SMOTE: Synthetic Minority Over-Sampling Technique.” Journal of Artificial Intelligence Research 16 (June): 321–57. https://doi.org/10.1613/jair.953.\n\n\nFernandez, Alberto, and Salvador García. 2018. Learning from Imbalanced Data Sets. New York, NY: Springer Science+Business Media.\n\n\nHastie, Trevor, Junyang Quian, and Kennet Tay. 2023. “An Introduction to GLMNET.” https://stanford.io/3QGGqcD.\n\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. “LightGBM: A Highly Efficient Gradient Boosting Decision Tree,” January.\n\n\nKuhn, Max, and Kjell Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nMitchell, Tom M. 2013. Machine Learning. Nachdr. McGraw-Hill Series in Computer Science. New York: McGraw-Hill.\n\n\nOliva Navarro, Diego Alberto, Essam H. Houssein, and Salvador Hinojosa, eds. 2021. Metaheuristics in Machine Learning: Theory and Applications. Studies in Computational Intelligence, volume 967. Cham: Springer.\n\n\nOpitz, Juri, and Sebastian Burst. 2021. “Macro F1 and Macro F1,” February. http://arxiv.org/abs/1911.03347.\n\n\nPawley, Steven. 2022. “GitHub - Stevenpawley/Colino: Recipes Steps for Supervised Filter-Based Feature Selection.” GitHub. https://github.com/stevenpawley/colino.\n\n\nStanczyk, Urszula, Beata Zielosko, and Lakhmi C. Jain. 2017. Advances in Feature Selection for Data and Pattern Recognition. New York, NY: Springer Berlin Heidelberg."
  },
  {
    "objectID": "metodologia.html#información-de-red",
    "href": "metodologia.html#información-de-red",
    "title": "4  Metodologia",
    "section": "4.1 Información de Red",
    "text": "4.1 Información de Red\nEl equipo de Quality Strategy recibe la información de los detractores de red y presenta quincenalmente un informe sobre el plan de acción para mejorar el NPS. Se revisan indicadores provenientes de diversas fuentes, incluidas las estadísticas de red 2G, 3G y LTE almacenadas en el servidor Taishan de Huawei, así como datos de disponibilidad obtenidos de una herramienta llamada Tivoli."
  },
  {
    "objectID": "metodologia.html#sec-framework",
    "href": "metodologia.html#sec-framework",
    "title": "4  Metodologia",
    "section": "4.2 Framework",
    "text": "4.2 Framework\nUn diagrama de trabajo fue realizado y se muestra en Figure 4.2. Este diagrama detalla el proceso de la metodología, desde las fuentes de información hasta los resultados obtenidos. A continuación, los procesos empleados para la obtención de los resultados del proyecto son detallados con mayor granularidad.\n\n\n\nFigure 4.2: Framework Data Flow"
  },
  {
    "objectID": "metodologia.html#sec-met-analisis",
    "href": "metodologia.html#sec-met-analisis",
    "title": "4  Metodologia",
    "section": "4.3 Metodología de análisis",
    "text": "4.3 Metodología de análisis\nTodo este trabajo fue realizado en la infraestructura de Aws Redshift para la obtención de los datos. Se utiliza Google Colab, R studio y SageMaker para la obtención de los datos, estudio de estos y las pruebas de los distintos modelos. Para la parte de limpieza y modelado se utilizó el lenguaje R y el framework tidymodel. Como referencia, el proceso definido en Figure 4.3 fue utilizado.\n\n\n\nFigure 4.3: Modelado\n\n\n\n4.3.1 Recetas\nLas recetas son objetos (e.g data set de entrenamiento) que sufren transformaciones (e.g normalización) y que posteriormente se integran al workflow para su uso. Son pasos de preprocesamiento encapsulados en un objeto para su posterior integración con los motores dentro de un workflow.\n\n\n4.3.2 Motores\nMotor es la combinación de un paquete y modo de trabajo de un modelo computacional. Por ejemplo, el motor boost_tree encapsula distintos paquetes que manejan árboles ensamblados, tales como xgboost para clasificación o lightgbm para regresión. Esto permite una amplia flexibilidad en términos de unificación y estandarización de la interfaz, lo que permite que se estandaricen hiperparámetros globales y secundarios. Un único modelo puede tener diferentes implementaciones en términos de paquetes, es decir, puede haber un paquete que implemente xgboost como una API a su contraparte en C o Python y puede haber otro paquete que implemente lo mismo, pero con variaciones que optimizan otros aspectos.\n\n\n4.3.3 Workflows\nPara poder combinar los diferentes preprocesamientos con los modelos candidatos, el framework utilizado utiliza el concepto de workflow en el que todo el ajuste se encuentra dentro de un mismo pipeline. Esto permite una mayor coherencia y reproducibilidad, ya que todas las etapas del modelado se definen y ejecutan de manera conjunta. Dentro de un workflow, se pueden incorporar “recetas” para el preprocesamiento de datos, que incluyen tareas como la imputación de valores faltantes, la codificación de variables categóricas y la normalización de características.\n\n\n4.3.4 Data Sources\nPara la extracción de la información, gestores de Huawei que ofrecen métricas por hora fueron utilizados por los analistas. Estos registros se encuentran almacenados en nuestro datalake en Amazon S3 y pueden ser consultados mediante Amazon Redshift o Amazon Athena. Athena fue elegido por la agilidad en la extracción de datos y su integración con Amazon SageMaker facilitó la edición programática de SQL en Python. El histórico de navegación de los usuarios fue almacenado en formato parquet para compartir con el equipo.\nLos datos del clima se extraen de una fuente externa proveniente de consultorías realizadas dos años atrás. Los datos se encontraban por días y se cuenta con la cantidad de centímetros cúbicos de precipitación que caen en la latitud de un sitio.\nAmazon SageMaker fue utilizado como herramienta de ETL y limpieza de datos. Los archivos de Excel, utilizados por los analistas entre 2021 y 2022, fueron leídos y transformados para la concatenación con registros de fechas posteriores. Para 2023, una tabla de Redshift fue consultada, y la información fue almacenada en un dataframe de pandas en formato wide. Tras la limpieza y transformación, un CSV fue exportado para compartir con el equipo."
  },
  {
    "objectID": "metodologia.html#data-collection",
    "href": "metodologia.html#data-collection",
    "title": "4  Metodologia",
    "section": "4.4 Data Collection",
    "text": "4.4 Data Collection\nLa variable objetivo fue generada a través del sistema Medallia, en el cual un muestreo estratificado de los clientes de DSF fue creado para el envío de una encuesta de NPS. Si un usuario es clasificado como detractor por razones de red, una llamada telefónica es realizada por Close The Loop para obtener retroalimentación. Este proceso ha sido mantenido desde 2020. Los resultados fueron almacenados en Excel desde 2021 hasta 2022, y desde 2023 en adelante, son almacenados en una plataforma web para la visualización de datos y resultados. En el apéndice Appendix A, se presentan los atributos iniciales generados por el ETL obtenidos desde las fuentes descritas, detallando su relevancia para el modelo desarrollado.\nPosterior a comprender el proceso de etiquetado de parte del equipo CTL, se concluyó que era necesario obtener los datos con una granularidad en el orden de las horas. Por lo que al final quedaron tres tablas, a como se observa en la figura fig:data-collection. La tabla principal contiene más de 70 mil registros, muchos de ellos sin etiquetas. La cantidad de usuarios únicos era de 4,675.\n\n\n\n\n\nerDiagram\n  DIAGNOSTICO ||--o{ METRICAS_RBS : has\n  DIAGNOSTICO ||--o{ DATOS_CLIMA : has\n  DIAGNOSTICO {\n    int msisdn_dd\n    int srvy_id\n  }\n  METRICAS_RBS {\n    int msisdn_dd\n    int srvy_id\n  }\n  DATOS_CLIMA {\n    int msisdn_dd\n    int srvy_id\n  }\n\n\n\n\n\nLa tabla de métricas se agrupaba de la siguiente manera: a cada usuario se le extrajo el top 10 de celdas a las que más se conecta en el mes. Por cada celda habían 30 días multiplicado por 18 horas de datos, por lo que cada usuario tenía en total un promedio de 5400 registros de métricas. Si tenemos 4,675 usuarios, el total del archivo final era de 25,245,000 el cual pesaba 5 gigabytes. Se creó un archivo parquet para poder ingresar el archivo a R."
  },
  {
    "objectID": "metodologia.html#data-preparation",
    "href": "metodologia.html#data-preparation",
    "title": "4  Metodologia",
    "section": "4.5 Data Preparation",
    "text": "4.5 Data Preparation\n\n4.5.1 Limpieza y transformación\nEl archivo parquet se unió con el archivo principal de usuarios y con los datos de clima. Posteriormente se realizaron operaciones de limpieza y transformación, tales como la conversión de tipos de datos, filtros iniciales y principalmente la selección de un primer conjunto de atributos, el cual fue validado con los SMEs.\nLos datos tenían una estructura jerárquica en la que un usuario tenía diferentes niveles de anidamiento. Este tipo de datos es mejor conocido en la literatura como datos multinivel o profile data (Kuhn and Johnson 2020). El procesamiento de este tipo de datos requiere un tratamiento especial en cuanto a que es indispensable colapsar los datos a nivel de usuario para poder tener todo en el mismo que la unidad de predicción. Lo relevante era poder resumir esta estructura en una única fila sin que se perdiera información predictiva.\nEl siguiente paso fue detectar la celda que cumplía las características que los SMEs nos brindaron en entrevistas. Este fue el punto más crucial de todo el proyecto. El etiquetador no dejaba plasmada en los datos la celda de la cual había obtenido el diagnóstico. Esto implicaba que, si simplemente resumíamos los datos, muchas celdas con buenos indicadores, cancelarían las celdas con malos indicadores y tendríamos mucho ruido de fondo, sin una señal clara que los modelos pudieran capturar.\nLo que se hizo fue identificar las celdas fuera de rango por medio de una heurística sencilla y luego comparar las celdas que cumplían ese criterio, para luego filtrarlas. De esta forma se colapsarían únicamente las celdas de los usuarios que sí estuvieran fuera de los rangos.\n\n\n4.5.2 Ingeniería de Atributos\nLa ingeniería de atributos fue realizada con base en el análisis exploratorio de los datos multinivel. La selección inicial de atributos fue realizada en colaboración con los SMEs. Estadísticas descriptivas como la media, mediana, rango intercuartílico, valores máximos y mínimos, y percentiles específicos fueron empleadas para crear nuevos atributos.\nAdicionalmente, la latitud y longitud de las celdas fueron convertidas a coordenadas polares, extrayendo el radio y el coeficiente theta.\n\n\n4.5.3 División de los datos\nLos datos se dividieron en 60% para datos de entrenamiento, 20% para el conjunto de validación y 20% para el conjunto de prueba. La razón para elegir esta partición es que el conjunto de datos es relativamente grande en términos de representatividad de la población bajo estudio. Otras razones incluyen el hecho de que se utilizó k-fold cross-validation con 10 folds y que si el tamaño del conjunto de entrenamiento era muy grande, habría un costo computacional innecesariamente grande.\n\n\n4.5.4 Análisis Exploratorio\nEn el análisis exploratorio llevado a cabo, se examinaron inicialmente los valores faltantes y las estadísticas resumidas. Posteriormente, la exploración se subdividió en dos fases principales: análisis univariado y análisis bivariado.\n\n4.5.4.1 Análisis Univariado\nSe adoptó un enfoque metódico y estructurado para el análisis de las variables numéricas y categóricas. En el caso de las variables numéricas, se comenzó con la variable de respuesta, evaluando su distribución y el nivel de desbalance entre las clases. Las distribuciones de los predictores continuos se examinaron en colaboración con los expertos en la materia (SMEs), identificando anomalías y valores atípicos que requerían eliminación.\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. Boca Raton London New York: CRC Press, Taylor & Francis Group."
  },
  {
    "objectID": "preparacion.html",
    "href": "preparacion.html",
    "title": "Preparación",
    "section": "",
    "text": "El proceso de preparación de datos en un flujo de trabajo de aprendizaje automático es una etapa crítica que implica varias tareas destinadas a convertir los datos crudos en un formato adecuado para el modelado. Este proceso puede incluir las siguientes tareas:\n\nLimpieza de Datos: Corregir o eliminar registros incorrectos, incompletos, corruptos, inexactos o irrelevantes. Esto puede incluir la corrección de errores tipográficos o de sintaxis, verificación de consistencia y completitud, y tratamiento de valores faltantes.\nTransformaciones Generales: Convertir medidas a escalas uniformes (por ejemplo, kilobytes a megabytes) o realizar cálculos para cambiar proporciones a porcentajes. Estas son transformaciones básicas para homogeneizar las unidades de medida o la escala de los datos.\nIngeniería de Atributos (Feature Engineering): Crear nuevas variables que podrían ser útiles para el modelado, tales como la extracción de componentes de fechas y horas, o la creación de indicadores basados en lógica de negocio o conocimientos de expertos en la materia (SMEs). Esto puede incluir el uso de expresiones regulares para extraer información de campos de texto o calcular nuevas métricas a partir de los datos existentes.\nPre-selección de Atributos: Decidir qué variables incluir en el modelo basándose en el conocimiento del dominio y la relevancia para el problema en cuestión, antes de cualquier selección basada en técnicas de modelado o datos. Esto puede involucrar consultas con expertos en la materia para identificar qué características son importantes para predecir el resultado deseado.\nAgregaciones: Sumarizar datos a un nivel superior, como convertir medidas diarias en medidas mensuales, o combinar categorías similares en una sola categoría para simplificar el análisis y el modelado posterior.\nIntegración de Datos: Combinar diferentes fuentes de datos en un solo conjunto de datos coherente que esté listo para el análisis. Esto podría incluir la fusión de tablas basadas en claves comunes, la unión de conjuntos de datos complementarios, o la consolidación de múltiples registros relacionados con la misma entidad.\n\nEstas tareas son cruciales para asegurar que los datos estén listos para las siguientes etapas del flujo de trabajo de machine learning, que incluirían el análisis exploratorio de datos (EDA), el preprocesamiento de datos (por ejemplo, normalización, codificación de variables categóricas, tratamiento de valores faltantes), y finalmente, el modelado."
  },
  {
    "objectID": "importacion.html#diagnósticos",
    "href": "importacion.html#diagnósticos",
    "title": "5  Importación",
    "section": "5.1 Diagnósticos",
    "text": "5.1 Diagnósticos\n\ndiag_00 &lt;- read_csv(diag_f, col_types = cols_diag)\n\n\ndiag_00 |&gt;\n select(-msisdn_dd) |&gt; \n glimpse(width = 75)\n\nRows: 72,644\nColumns: 5\n$ fct_srvy_dt &lt;chr&gt; \"20220103\", \"20221026\", \"20221119\", \"20230614\", \"2022…\n$ srvy_id     &lt;int&gt; 539124531, 685399274, 698188789, 811943714, 677228448…\n$ class_desc  &lt;chr&gt; \"NEUTRO\", \"PROMOTOR\", \"DETRACTOR\", \"PROMOTOR\", \"PROMO…\n$ time_lte    &lt;dbl&gt; 0.9904817, 0.9884153, 0.0000000, 0.9326159, 0.6964744…\n$ diag        &lt;chr&gt; \"no_diag\", \"no_diag\", \"no_diag\", \"no_diag\", \"no_diag\"…\n\n\nEn el apéndice A se pueden revisar el significado de cada variable"
  },
  {
    "objectID": "importacion.html#métricas-rbs",
    "href": "importacion.html#métricas-rbs",
    "title": "5  Importación",
    "section": "5.2 Métricas RBS",
    "text": "5.2 Métricas RBS\n\nhourly_metrics_raw &lt;- read_fst(path = metr_f) |&gt; as_tibble()\n\n\nhourly_metrics_raw |&gt;\n select(-msisdn_dd) |&gt; \n glimpse(width = 75)\n\nRows: 10\nColumns: 29\n$ fct_srvy_dt            &lt;chr&gt; \"20220104\", \"20220112\", \"20220102\", \"20220…\n$ srvy_id                &lt;int&gt; 542654190, 546333723, 540796453, 549967036…\n$ r1                     &lt;int&gt; 4, 17, 3, 10, 7, 2, 9, 3, 5, 9\n$ cll_prctg              &lt;dbl&gt; 0.032281439, 0.007013118, 0.152949317, 0.0…\n$ bts_sh_nm              &lt;chr&gt; \"OAVP811A\", \"LGUA298B\", \"LGTA025A\", \"OJTP0…\n$ prttn_hr               &lt;chr&gt; \"2021121719\", \"2021121719\", \"2021122110\", …\n$ rate_prb_dl            &lt;dbl&gt; 0.48648, 0.61776, 0.27068, 0.67176, 0.6717…\n$ cell_load              &lt;dbl&gt; 0.13119228, 0.27534060, 0.14068818, 0.1821…\n$ rrc_success_rate       &lt;dbl&gt; 0.9949593, 0.9989575, 1.0000000, 0.9983299…\n$ erab_success_rate      &lt;dbl&gt; 0.9952722, 0.9994387, 1.0000000, 0.9989012…\n$ service_drop_rate      &lt;dbl&gt; 0.00192794313, 0.00037406484, 0.0000000000…\n$ l_ul_interference_avg  &lt;dbl&gt; -118, -104, -113, -114, -114, -115, -118, …\n$ thoughput_dl           &lt;dbl&gt; 13324.383, 8501.190, 6915.360, 5634.725, 5…\n$ thpughput_ul           &lt;dbl&gt; 126.7266, 1745.8053, 258.4719, 223.3298, 2…\n$ corrected_cqi          &lt;dbl&gt; 13, 10, 8, 10, 10, 10, 12, 12, 11, 12\n$ ra_ta_ue_index1        &lt;dbl&gt; 0.0119986287, 0.6181172291, 0.2095238095, …\n$ ra_ta_ue_index2        &lt;dbl&gt; 0.021597532, 0.335257549, 0.323249300, 0.0…\n$ ra_ta_ue_index3        &lt;dbl&gt; 0.09770312, 0.01776199, 0.19215686, 0.0173…\n$ ra_ta_ue_index4        &lt;dbl&gt; 0.4748028797, 0.0003700414, 0.0605042017, …\n$ ra_ta_ue_index5        &lt;dbl&gt; 0.284710319, 0.000000000, 0.049299720, 0.6…\n$ ra_ta_ue_index6        &lt;dbl&gt; 0.0675351388, 0.0000000000, 0.0700280112, …\n$ ra_ta_ue_index7        &lt;dbl&gt; 0.040109702, 0.000000000, 0.007282913, 0.0…\n$ ra_ta_ue_total         &lt;dbl&gt; 5834, 13512, 1785, 19175, 19175, 5732, 143…\n$ volte_erlang           &lt;dbl&gt; 1.91277778, 8.04652778, 0.79697222, 0.0827…\n$ modulation_64qam_ratio &lt;dbl&gt; 0.8543821, 0.6106413, 0.3532195, 0.5319436…\n$ modulation_16qam_ratio &lt;dbl&gt; 0.1087508, 0.2855035, 0.2447876, 0.3256994…\n$ modulation_qpsk_ratio  &lt;dbl&gt; 0.03686716, 0.10385525, 0.40199284, 0.1423…\n$ cell_unavail           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cell_unavail_s1fail    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n\n\nSe aprecian los nombres originales de los atributos.\n\n\n\n\n\n\nImportante\n\n\n\nEn el caso de la tabla hourly_metrics estamos viendo una muestra de únicamente 10 registros debido a que el tamaño real del dataset es de más de 44 millones de filas y cargarlo en este reporte web es muy tardado."
  },
  {
    "objectID": "importacion.html#datos-ubicación",
    "href": "importacion.html#datos-ubicación",
    "title": "5  Importación",
    "section": "5.3 Datos Ubicación",
    "text": "5.3 Datos Ubicación\n\n# Conexión a base de datos\ncon &lt;- conectigo::conectar_msql()\n\n# Query para extraer información geográfica y clima\nsitiosfs_raw &lt;- tbl(con, in_schema(\"fieldservice\", \"site_collate\")) |&gt;\n select(\n  twr = cilocation, lat = latitude, lon = longitude,\n  dpto = department, municipalidad = municipality) |&gt; \n collect() |&gt;\n distinct(twr, .keep_all = TRUE)\n\n# Estandarización de columnas\nsitiosfs &lt;- sitiosfs_raw |&gt;\n  mutate(across(dpto:municipalidad, \\(x) estandarizar_columnas(x))) |&gt;\n  rename(city = \"municipalidad\")\n\n\nsitiosfs %&gt;% glimpse(width = 75)\n\nRows: 5,412\nColumns: 6\n$ twr    &lt;chr&gt; \"GUA001\", \"GUA002\", \"GUA003\", \"GUA004\", \"GUA006\", \"GUA007\"…\n$ status &lt;chr&gt; \"OPERATING\", \"OPERATING\", \"OPERATING\", \"OPERATING\", \"OPERA…\n$ lat    &lt;dbl&gt; 14.61550, 14.53500, 14.63681, 14.59050, 14.59250, 14.58750…\n$ lon    &lt;dbl&gt; -90.53310, -90.57030, -90.46111, -90.50340, -90.52090, -90…\n$ dpto   &lt;chr&gt; \"GUATEMALA\", \"GUATEMALA\", \"GUATEMALA\", \"GUATEMALA\", \"GUATE…\n$ city   &lt;chr&gt; \"GUATEMALA\", \"VILLA_NUEVA\", \"GUATEMALA\", \"GUATEMALA\", \"GUA…"
  },
  {
    "objectID": "prep.html#diagnóstico",
    "href": "prep.html#diagnóstico",
    "title": "6  Prep",
    "section": "6.1 Diagnóstico",
    "text": "6.1 Diagnóstico\nEl primer paso consiste en la transformación de los datos de diagnóstico.\n\n# Preprocesamiento y filtrado inicial\ndiagnosticos &lt;- diag_00 |&gt;\n mutate(\n1  across(diag, toupper)) |&gt;\n2 filter(diag %in% etiquetas) |&gt;\n3 split(~ diag) |&gt;\n4 map_at(\"NO_DIAG\", \\(df) df |&gt;\n5 filter(class_desc == \"PROMOTOR\") |&gt;\n6 drop_na() |&gt;\n7 distinct(msisdn_dd, .keep_all = TRUE)) |&gt;\n # Combinar listas en un solo data frame\n8 list_rbind() |&gt;\n # Transformaciones finales\n mutate(\n  across(diag,\n9   \\(x) case_match(x, \"NO_DIAG\" ~ \"PROMOTOR\", .default = diag))) |&gt;\n10 select(-class_desc) |&gt;\n11 distinct(msisdn_dd, .keep_all = TRUE)\n\n\n1\n\nConvertir a mayúscula todas las etiquetas ground truth labels.\n\n2\n\nDejar únicamente las etiquetas de interés para el proyecto.\n\n3\n\nSeparar en un df distinto cada etiqueta para recibir transformaciones.\n\n4\n\nAplicar una transformación específicamente al df que contienen los NO_DIAG.\n\n5\n\nEn ese df dejar únicamente los promotores.\n\n6\n\nDebido a que hay muchos promotores, podemos eliminar los valores NAs.\n\n7\n\nEliminar registros duplicados (ej. clientes que tienen más de una encuesta)\n\n8\n\nUnir los data-frames que están en una lista en un solo data-frame\n\n9\n\nRenombrar los NO_DIAG como PROMOTOR\n\n10\n\nEliminar la columna class_desc\n\n11\n\nRetirar los duplicados de todo el data-frame"
  },
  {
    "objectID": "prep.html#integración",
    "href": "prep.html#integración",
    "title": "6  Prep",
    "section": "6.2 Integración",
    "text": "6.2 Integración\n\nctl_00 &lt;- diagnosticos |&gt;\n inner_join(hourly_metrics_raw, join_by(fct_srvy_dt, msisdn_dd, srvy_id))\n\nUna vez que los diagnósticos han sido procesados, los uniremos con las métricas RBS utilizando un inner_join para no tener columnas en blanco."
  },
  {
    "objectID": "prep.html#preparación",
    "href": "prep.html#preparación",
    "title": "6  Prep",
    "section": "6.3 Preparación",
    "text": "6.3 Preparación\nA continuación realizaremos las primeras transformaciones, así como limpieza de los datos ya unidos. Manejaremos una secuencia de objetos enumerados con un sufijo. El objetivo es mantener los fragmentos de código realacionados a la preparación del tamaño adecuado.\n\nctl_01 &lt;- ctl_00 |&gt;\n mutate(\n1  tad   = ra_ta_ue_index6 + ra_ta_ue_index7,\n2  fecha = str_sub(prttn_hr, 1, 8) |&gt; ymd(),\n3  hora  = str_sub(prttn_hr, 9, 10),\n4  date  = str_c(fecha, hora, sep = \" \") |&gt; ymd_h(),\n5  twr   = str_sub(bts_sh_nm, start = 2, end = 7),\n6  across(all_of(enteros), as.integer),\n7  across(thoughput_dl, \\(x) x / 1e3),\n8  across(all_of(porcentajes), \\(col) col * 100)) |&gt;\n9 left_join(sitiosfs, join_by(twr)) |&gt;\n10 select(-all_of(metrics_to_remove)) |&gt;\n11 rename(any_of(lookup)) |&gt;\n12 relocate(\n  user, date, poll, fecha, hora, dpto, city,twr, bts, time, lat, lon, prb,\n  thp, rrc,erb, drp, tad, lod, erf, cqi, m64, m16, psk, vol, dis, lte, diag)\n\n\n1\n\nFeature Engineering: Crear un nuevo atributo derivado de la suma de los timing advanced con index 6 y 7. Esto se validó con los SMEs como una forma válida de determinar problemas de capacidad y cobertura.\n\n2\n\nExtraer la fecha de la columna prttn_hr debido a que venía concatenada con la hora.\n\n3\n\nExtraer la hora\n\n4\n\nConvertir a fecha el atributo extraído.\n\n5\n\nExtraer el sitio del id de la celda\n\n6\n\nConvertir a enteros los predictores que lo ameriten.\n\n7\n\nEl throuput_dl está en kilobytes, así que debemos pasarlo a megabytes\n\n8\n\nLas columnas que están en proporción (ej. 0 a 1) las pasaremos a porcentaje para mayor facilidad.\n\n9\n\nUnir los datos con la tabla que contiene información geográfica (ej. latitud, longitud, etc).\n\n10\n\nRemover variables validadas con SMEs que no serían relevantes para el modelo.\n\n11\n\nRenombrar a un formato más corto para facilidad en el modelado.\n\n12\n\nReubicar las columnas para facilitar la aplicación de transformación y EDA.\n\n\n\n\n\nctl_02 &lt;- ctl_01 |&gt;\n filter(\n1  !twr %in% sitios_ruedas,\n2  !if_all(prb:dis, ~ .x == 0),\n3  !(dis == 0 & diag == \"DISPONIBILIDAD\")) |&gt;\n4 arrange(user, fecha, hora, desc(time)) |&gt;\n5 group_by(user, fecha, hora) |&gt;\n6 slice_head(n = 10) |&gt;\n7 ungroup()\n\n\n1\n\nRemover sitios que se mueven de un lugar a otro en plataformas portátiles.\n\n2\n\nLos contadores para 3G no están habilitados, por lo que no hay valores en sus métricas. Estos dan cero, por lo que deben removerse.\n\n3\n\nLos casos donde la disponibilidad es igual a cero y el diagnóstico es igual a disponibilidad son errores de digitación.\n\n4\n\nReordenar.\n\n5\n\nAgrupar por usuario, fecha y hora.\n\n6\n\nSeleccionar únicamente el top 10 de celdas de cada usuario.\n\n7\n\nDesagrupar."
  },
  {
    "objectID": "prep.html#reglas-de-negocio",
    "href": "prep.html#reglas-de-negocio",
    "title": "6  Prep",
    "section": "6.4 Reglas de negocio",
    "text": "6.4 Reglas de negocio\nLas reglas de negocio sirven para poder identificar que celdas son las que utilizó el SME para determinar el diagnóstico.\n\n# Definir reglas mutuamente excluyentes\ncapacidad &lt;- expr(prb &gt;= 85 & thp &lt;= 2.7 & tad &lt;= 15)\ncobertura &lt;- expr((prb &gt;= 85 & thp &lt;= 2.7) | tad &gt;= 15)\ndisponibi &lt;- expr(if_all(prb:vol, ~ .x == 0) | dis &gt; 0)\noptimizac &lt;- expr(erb &lt; 90 | rrc &lt; 90 | cqi &lt; 7 | erf &gt; -95 | drp &gt; 1.5 |\n (prb == 0 & thp &gt;  0 & lod == 0) | (thp == 0 & prb &gt; 0 & vol == 0) |\n (prb == 0 & thp == 0 & vol == 0) |  psk &gt; m64)\npromotor  &lt;- expr(prb &lt; 85 & thp &gt; 2.7 & tad &lt; 15 & lte &gt; 95 & erf &lt;= -95 &\n cqi &gt; 7 & dis == 0)\n\n\n\n\n\n\n\nImportante\n\n\n\nLa implementación de reglas de negocio explícitas para identificar las celdas de diagnóstico seleccionadas por el experto en la materia (SME) ha suscitado debate. Podría parecer, a primera vista, que si se aplican estas reglas, el uso de un modelo de aprendizaje automático (ML) se vuelve redundante. Sin embargo, esta perspectiva no considera un factor crítico: la ausencia de etiquetas específicas para las celdas y los periodos en que se manifestaron los síntomas de las distintas categorías. Al consolidar los datos de 30 días y 10 celdas sin distinción, las estadísticas descriptivas se calculan tanto para celdas con comportamiento normal como anormal, lo que resulta en una pérdida de variabilidad y un incremento de ruido en los datos.\nEsta hipótesis se vio confirmada cuando los primeros modelos arrojaron un rendimiento inferior al 35%. Al no diferenciar entre datos ‘buenos’ y ‘malos’, se diluye la varianza, impidiendo que el modelo distinga señales claras asociadas a cada categoría.\nPor tanto, lejos de ‘ayudar’ al modelo, la creación de estas reglas es un intento de replicar el proceso de diagnóstico del SME. Esto no solo es un punto crucial para mejorar el proceso, sino que también es esencial para garantizar que el modelo de ML esté estimando las categorías basándose en las mismas premisas utilizadas por el experto durante su evaluación.\nEn la literatura de aprendizaje automático y ciencia de datos, la aplicación de heurísticas para mejorar el rendimiento de los modelos es un enfoque bien establecido y documentado. Estas heurísticas a menudo toman la forma de ingeniería de características, selección de instancias, o la creación de reglas de negocio que reflejan el conocimiento del dominio."
  },
  {
    "objectID": "prep.html#aprendizaje-semi-supervisado",
    "href": "prep.html#aprendizaje-semi-supervisado",
    "title": "6  Prep",
    "section": "6.5 Aprendizaje Semi-supervisado",
    "text": "6.5 Aprendizaje Semi-supervisado\nEn este caso crearemos una nueva columna y no se modificaran las etiquetas originales.\n\nctl_03 &lt;- ctl_02 |&gt;\n1 drop_na() |&gt;\n mutate(\n2  diag2 = case_when(\n   eval(optimizac) ~ \"OPTIMIZACION\",                  \n   eval(capacidad) ~ \"CAPACIDAD\",                     \n   eval(promotor)  ~ \"PROMOTOR\",                      \n   eval(cobertura) ~ \"COBERTURA\",                     \n   eval(disponibi) ~ \"DISPONIBILIDAD\",                \n3  .default = diag\n  ),\n4  across(diag:diag2, as.factor)\n )\n\n\n1\n\nDebido a que las métricas de RBS se extraen de los gestores, hay muy pocos valores perdidos (NAs), por lo que es mejor descartarlos.\n\n2\n\nCrear una columna nueva llamada diag2 y evaluar cada una de las reglas generadas anteriormente. El cumplimiento de cada regla genera una etiqueta paralela a la etiqueta ground truth.\n\n3\n\nEn caso de que no encuentre ninguna de las reglas, colocar la etiqueta ground truth.\n\n4\n\nConvertir a variable de tipo factor las dos columnas.\n\n\n\n\n\nctl_04 &lt;- ctl_03 |&gt;\n1 group_by(user) |&gt;\n2 filter(diag == diag2) |&gt;\n3 ungroup()\n\n\n1\n\nAgrupar por usuario.\n\n2\n\nFiltrar para dejar las filas en las que las etiquetas originales (ground truth) generadas por los SMEs coinciden con las etiquetas generadas por la heurística.\n\n3\n\nDesagrupar.\n\n\n\n\nEste paso es el más crucial y es al que se le puede atribuir el incremento en el desempeño del modelado. La detección de las instancias que él SME posiblemente utilizó como base para realizar el diagnóstico son las que quedan.\n\n\n\n\n\n\nImportante\n\n\n\nDurante las entrevistas con los expertos en la materia (SMEs), se confirmó que la inclusión de etiquetas específicas que identifican la celda de origen del diagnóstico, así como las horas en que se detectaron anomalías en las métricas, deberían ser incorporadas como un estándar en el proceso de etiquetado. Anteriormente, esta práctica no se llevaba a cabo de manera sistemática.\nLa importancia de este ajuste en el proceso de etiquetado es significativa. Al etiquetar de manera precisa y detallada desde el inicio, las instancias de datos se preparan de forma más adecuada, lo que facilita la identificación de patrones y anomalías por parte de los modelos de aprendizaje automático. Esta mejora en la calidad de los datos iniciales puede conducir a un incremento en la precisión y la eficacia de los diagnósticos automatizados.\nAdemás, este cambio en la metodología de etiquetado refleja una alineación más estrecha con las prácticas óptimas de gestión de datos. Al asegurar que cada instancia de datos esté correctamente etiquetada con la información relevante, se establece una base sólida para el análisis predictivo y se potencia la capacidad del modelo para aprender de los datos más fidedignos."
  },
  {
    "objectID": "prep.html#agregación",
    "href": "prep.html#agregación",
    "title": "6  Prep",
    "section": "6.6 Agregación",
    "text": "6.6 Agregación\nUna vez realizada la discriminación de instancias que provocan ruido, al no ser las que se utilizaron para derivar el diagnóstico, se procede con la fase de agregación. Aquí el objetivo es colapsar el dataset de +44 millones de registros utilizando estadísticas descriptivas con la finalidad de capturar la mayor cantidad de variabilidad (ver Kuhn and Johnson 2020 Capítulo 9)\n\n# Definir listado de funciones básicas\nfun_basicas &lt;- list(\n  mean   = ~mean(.x, na.rm = TRUE),\n  median = ~median(.x, na.rm = TRUE),\n  sd     = ~sd(.x, na.rm = TRUE),\n  mim    = ~min(.x, na.rm = TRUE),\n  max    = ~max(.x, na.rm = TRUE),\n  iqr    = ~IQR(.x, na.rm = TRUE),\n  p1     = ~quantile(.x, probs = 0.01, na.rm = TRUE),\n  p5     = ~quantile(.x, probs = 0.05, na.rm = TRUE),\n  p10    = ~quantile(.x, probs = 0.10, na.rm = TRUE),\n  p25    = ~quantile(.x, probs = 0.25, na.rm = TRUE),\n  p75    = ~quantile(.x, probs = 0.75, na.rm = TRUE),\n  p90    = ~quantile(.x, probs = 0.90, na.rm = TRUE),\n  p95    = ~quantile(.x, probs = 0.95, na.rm = TRUE),\n  p99    = ~quantile(.x, probs = 0.99, na.rm = TRUE)\n)\n\nEstas estadística se definieron con base a lo observado en los gestores que usan los SMEs para determinar una anomalía. Se observó que los valores extremos son particularmente útiles en muchos escenarios, por lo que el uso de IQR podría ser relevante.\nPosterior a definir la lista de estadístico, se creo una función que los aplicará de forma automática a todos los atributos. Para poder colapsar las variables de tipo categórica se optó por el primer valor utilizando la función first 1\n\n# Función para sumarizar métricas\nsummarize_metrics &lt;- function(df) {\n df |&gt;\n  summarise(\n\n1  across(prb:lte, fun_basicas),\n\n2  lat = first(lat),\n3  lon = first(lon),\n\n  # coordenadas polares\n4  r = sqrt(lat^2 + lon^2),\n  theta = atan2(lon, lat)\n  )\n}\n\n\n1\n\nAplicar lista de funciones estadísticas\n\n2\n\nPrimera latitud.\n\n3\n\nPrimera longitud.\n\n4\n\nConvertir coordenadas cartesianas a coordenadas polares2\n\n\n\n\n\nctl &lt;- ctl_04 |&gt;\n1 group_by(user) |&gt;\n2 summarize_metrics() |&gt;\n3 drop_na() |&gt;\n4 mutate(across(where(is.character), as.factor))\n\n\n1\n\nAgrupar por usuario.\n\n2\n\nAplicar función de métricas sumarizadas.\n\n3\n\nEliminar valores faltantes debido a que son muy pocos.\n\n4\n\nConvertir las variables de tipo caracter en de tipo factor.\n\n\n\n\n\n\n\n\nKuhn, Max, and Kjell Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. Boca Raton London New York: CRC Press, Taylor & Francis Group."
  },
  {
    "objectID": "division.html",
    "href": "division.html",
    "title": "7  División",
    "section": "",
    "text": "# Semilla\nset.seed(2023)\n\n\n1ctl_split &lt;- initial_validation_split(data = ctl, strata = diag)\n2ctl_train &lt;- training(ctl_split)\n3ctl_valid &lt;- validation(ctl_split)\n4ctl_valse &lt;- validation_set(ctl_split)\n\n\n1\n\nCreación de un objeto que contendrá tres datasets: entrenamiento, validación y prueba. Todos estratificados a través de la variable respuesta diag.\n\n2\n\nExtraer el conjunto de entrenamiento del objeto ctl_split.\n\n3\n\nExtraer el conjunto de validación del objeto ctl_split.\n\n4\n\nCrear un conjunto de validación para alimentar a la función last_fit().\n\n\n\n\nDentro del framework de Tidymodels, es muy sencillo ajustar los datos de validación una vez entrenado el modelo. La función validation_set() es un objeto que ayuda a que se ajusten los modelos entrenados con validación cruzada al conjunto de validación para que se pueda iterar mientras se afinan los hiperparámetros.\n\nctl_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;3396/1133/1135/5664&gt;\n\n\nVemos que el objeto ctl_split contiene la división 60% para el conjunto de entrenamiento, 20% para validación y el conjunto de prueba con el restante 20%."
  },
  {
    "objectID": "analisis-exploratorio.html#estructura",
    "href": "analisis-exploratorio.html#estructura",
    "title": "8  Análisis Exploratorio",
    "section": "8.1 Estructura",
    "text": "8.1 Estructura\nLos datos de entrenamiento, los cuales fueron preparados anteriormente constan de 3396 filas y 216 columnas.\n\nctl_train |&gt; \n select(-user) |&gt; \n glimpse(width = 77)\n\nRows: 3,396\nColumns: 215\n$ prb_mean   &lt;dbl&gt; 37.48692, 48.09889, 80.19165, 44.20385, 39.41634, 35.794…\n$ prb_median &lt;dbl&gt; 34.2560, 41.3640, 87.7440, 44.6650, 31.0550, 30.6470, 54…\n$ prb_sd     &lt;dbl&gt; 22.265082, 20.114658, 25.667274, 24.678172, 25.066668, 2…\n$ prb_mim    &lt;dbl&gt; 0.996, 22.332, 0.000, 0.000, 4.880, 2.370, 2.714, 2.264,…\n$ prb_max    &lt;dbl&gt; 83.452, 81.176, 99.032, 95.428, 96.222, 95.388, 91.642, …\n$ prb_iqr    &lt;dbl&gt; 40.19650, 23.52400, 12.08200, 32.84850, 38.15700, 29.417…\n$ prb_p1     &lt;dbl&gt; 4.620080, 22.491680, 0.000000, 0.000000, 5.128187, 5.802…\n$ prb_p5     &lt;dbl&gt; 8.1250, 23.1304, 0.0000, 0.0000, 7.5222, 8.8162, 18.2466…\n$ prb_p10    &lt;dbl&gt; 10.53080, 23.92880, 53.53600, 5.67040, 12.04173, 11.6164…\n$ prb_p25    &lt;dbl&gt; 16.5650, 38.6840, 82.2860, 28.6920, 20.2310, 18.9835, 44…\n$ prb_p75    &lt;dbl&gt; 56.76150, 62.20800, 94.36800, 61.54050, 58.38800, 48.400…\n$ prb_p90    &lt;dbl&gt; 68.7876, 71.8224, 97.4100, 77.3244, 79.9552, 66.1566, 74…\n$ prb_p95    &lt;dbl&gt; 74.04460, 76.49920, 97.90000, 86.83040, 84.18680, 86.825…\n$ prb_p99    &lt;dbl&gt; 78.49528, 80.24064, 98.58100, 91.93556, 95.24902, 93.355…\n$ thp_mean   &lt;dbl&gt; 9.626637, 6.246259, 2.656247, 6.077118, 17.275567, 9.004…\n$ thp_median &lt;dbl&gt; 8.339577, 5.819104, 2.546632, 4.986531, 13.198847, 8.135…\n$ thp_sd     &lt;dbl&gt; 7.324359, 2.338850, 1.744567, 3.523195, 11.868242, 5.520…\n$ thp_mim    &lt;dbl&gt; 2.1279259, 3.1664280, 0.0000000, 0.0000000, 2.9998839, 0…\n$ thp_max    &lt;dbl&gt; 63.915619, 9.375950, 15.806763, 27.436141, 59.218512, 46…\n$ thp_iqr    &lt;dbl&gt; 6.8614639, 3.4317906, 1.9547859, 3.1303675, 10.5432179, …\n$ thp_p1     &lt;dbl&gt; 2.4581545, 3.1918101, 0.0000000, 0.0000000, 3.0920136, 1…\n$ thp_p5     &lt;dbl&gt; 3.1884884, 3.2933382, 0.0000000, 2.7838676, 5.2387282, 1…\n$ thp_p10    &lt;dbl&gt; 3.8376699, 3.4202484, 0.8641600, 3.2415870, 6.4828465, 2…\n$ thp_p25    &lt;dbl&gt; 4.8137313, 4.9715162, 1.4993906, 4.0075835, 9.5293055, 5…\n$ thp_p75    &lt;dbl&gt; 11.675195, 8.403307, 3.454177, 7.137951, 20.072523, 11.1…\n$ thp_p90    &lt;dbl&gt; 15.037150, 9.257066, 4.445485, 10.980280, 34.189592, 16.…\n$ thp_p95    &lt;dbl&gt; 22.876909, 9.316508, 5.679906, 12.726643, 42.930224, 20.…\n$ thp_p99    &lt;dbl&gt; 41.086418, 9.364061, 8.115333, 17.669865, 53.775880, 26.…\n$ rrc_mean   &lt;dbl&gt; 99.45819, 99.89075, 92.70681, 99.21025, 99.78224, 99.573…\n$ rrc_median &lt;dbl&gt; 99.52494, 100.00000, 99.80047, 99.86581, 100.00000, 99.8…\n$ rrc_sd     &lt;dbl&gt; 0.44291257, 0.15786045, 25.54922782, 7.97607579, 0.63233…\n$ rrc_mim    &lt;dbl&gt; 96.55797, 99.62647, 0.00000, 0.00000, 95.78947, 90.11858…\n$ rrc_max    &lt;dbl&gt; 100.00000, 100.00000, 100.00000, 100.00000, 100.00000, 1…\n$ rrc_iqr    &lt;dbl&gt; 0.47290652, 0.14903130, 0.42946085, 0.17555090, 0.097953…\n$ rrc_p1     &lt;dbl&gt; 97.62913, 99.62676, 0.00000, 99.46579, 97.00235, 97.2488…\n$ rrc_p5     &lt;dbl&gt; 98.85912, 99.62794, 0.00000, 99.63047, 98.52515, 98.2011…\n$ rrc_p10    &lt;dbl&gt; 98.99140, 99.62941, 99.14826, 99.68601, 99.40025, 98.709…\n$ rrc_p25    &lt;dbl&gt; 99.27418, 99.85097, 99.52076, 99.77237, 99.90205, 99.476…\n$ rrc_p75    &lt;dbl&gt; 99.74709, 100.00000, 99.95022, 99.94792, 100.00000, 99.9…\n$ rrc_p90    &lt;dbl&gt; 99.91263, 100.00000, 100.00000, 100.00000, 100.00000, 10…\n$ rrc_p95    &lt;dbl&gt; 100.00000, 100.00000, 100.00000, 100.00000, 100.00000, 1…\n$ rrc_p99    &lt;dbl&gt; 100.00000, 100.00000, 100.00000, 100.00000, 100.00000, 1…\n$ erb_mean   &lt;dbl&gt; 98.86197, 99.72027, 90.99714, 90.52237, 99.62969, 99.667…\n$ erb_median &lt;dbl&gt; 99.03876, 99.68254, 99.83254, 99.81326, 100.00000, 99.91…\n$ erb_sd     &lt;dbl&gt; 0.98937643, 0.24284243, 28.30700278, 29.00137466, 1.4267…\n$ erb_mim    &lt;dbl&gt; 94.14286, 99.28571, 0.00000, 0.00000, 90.95023, 90.46653…\n$ erb_max    &lt;dbl&gt; 100.00000, 100.00000, 100.00000, 100.00000, 100.00000, 1…\n$ erb_iqr    &lt;dbl&gt; 1.49891210, 0.33836341, 0.33828287, 0.24211545, 0.092196…\n$ erb_p1     &lt;dbl&gt; 96.11161, 99.30260, 0.00000, 0.00000, 92.13249, 97.28849…\n$ erb_p5     &lt;dbl&gt; 97.07034, 99.37016, 0.00000, 0.00000, 98.16216, 98.26485…\n$ erb_p10    &lt;dbl&gt; 97.53370, 99.45460, 99.13043, 98.98997, 99.75605, 98.997…\n$ erb_p25    &lt;dbl&gt; 98.21571, 99.63537, 99.61449, 99.68022, 99.90780, 99.652…\n$ erb_p75    &lt;dbl&gt; 99.71463, 99.97373, 99.95277, 99.92233, 100.00000, 100.0…\n$ erb_p90    &lt;dbl&gt; 99.96521, 100.00000, 100.00000, 100.00000, 100.00000, 10…\n$ erb_p95    &lt;dbl&gt; 100.00000, 100.00000, 100.00000, 100.00000, 100.00000, 1…\n$ erb_p99    &lt;dbl&gt; 100.00000, 100.00000, 100.00000, 100.00000, 100.00000, 1…\n$ drp_mean   &lt;dbl&gt; 0.18795463, 0.04007125, 0.06378263, 0.09105586, 0.051762…\n$ drp_median &lt;dbl&gt; 0.174753289, 0.029129042, 0.043821209, 0.037743164, 0.00…\n$ drp_sd     &lt;dbl&gt; 0.16257385, 0.05284377, 0.16300360, 0.30891188, 0.140355…\n$ drp_mim    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ drp_max    &lt;dbl&gt; 0.9281640, 0.1612903, 3.6167513, 3.2363574, 1.0826772, 1…\n$ drp_iqr    &lt;dbl&gt; 0.24993261, 0.05327651, 0.07370780, 0.08153286, 0.033799…\n$ drp_p1     &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.00…\n$ drp_p5     &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.00…\n$ drp_p10    &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.00…\n$ drp_p25    &lt;dbl&gt; 0.043258550, 0.000000000, 0.006399373, 0.000000000, 0.00…\n$ drp_p75    &lt;dbl&gt; 0.29319116, 0.05327651, 0.08010717, 0.08153286, 0.033799…\n$ drp_p90    &lt;dbl&gt; 0.39399330, 0.08800719, 0.13373454, 0.12603007, 0.126055…\n$ drp_p95    &lt;dbl&gt; 0.46867020, 0.12464876, 0.17345424, 0.18951102, 0.273392…\n$ drp_p99    &lt;dbl&gt; 0.6963890, 0.1539620, 0.2984848, 1.9740613, 0.5885526, 0…\n$ tad_mean   &lt;dbl&gt; 0.59571709, 0.17141014, 1.42872507, 22.00375097, 2.12989…\n$ tad_median &lt;dbl&gt; 0.31468531, 0.00000000, 0.89208007, 33.28678535, 0.95217…\n$ tad_sd     &lt;dbl&gt; 0.8279423, 0.3019385, 1.9700587, 19.3219590, 3.1387700, …\n$ tad_mim    &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.000000…\n$ tad_max    &lt;dbl&gt; 9.0966519, 0.8413462, 14.4844125, 57.2833724, 14.2222222…\n$ tad_iqr    &lt;dbl&gt; 0.60574977, 0.09990010, 1.68947731, 38.59727602, 1.93749…\n$ tad_p1     &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0…\n$ tad_p5     &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.0000…\n$ tad_p10    &lt;dbl&gt; 0.053007505, 0.000000000, 0.000000000, 0.000000000, 0.00…\n$ tad_p25    &lt;dbl&gt; 0.148152794, 0.000000000, 0.083175908, 0.000000000, 0.39…\n$ tad_p75    &lt;dbl&gt; 0.75390256, 0.09990010, 1.77265322, 38.59727602, 2.32840…\n$ tad_p90    &lt;dbl&gt; 1.4555681, 0.5832548, 3.3227848, 42.5877773, 5.6842842, …\n$ tad_p95    &lt;dbl&gt; 1.91225745, 0.71230049, 4.98382291, 44.43823631, 10.8906…\n$ tad_p99    &lt;dbl&gt; 3.4809335, 0.8155370, 11.0012228, 50.4719008, 12.9160879…\n$ lod_mean   &lt;dbl&gt; 25.181140, 16.376049, 74.191793, 26.850977, 13.417087, 1…\n$ lod_median &lt;dbl&gt; 15.056535, 14.022232, 54.453634, 24.759515, 10.644404, 9…\n$ lod_sd     &lt;dbl&gt; 18.951034, 6.534124, 69.171383, 18.063381, 10.776891, 23…\n$ lod_mim    &lt;dbl&gt; 2.661101, 8.816542, 0.000000, 0.000000, 3.548135, 5.3222…\n$ lod_max    &lt;dbl&gt; 87.69439, 27.97170, 490.18018, 100.88630, 67.12023, 285.…\n$ lod_iqr    &lt;dbl&gt; 31.678749, 9.231134, 55.680572, 24.837195, 10.654926, 3.…\n$ lod_p1     &lt;dbl&gt; 3.451753, 8.897523, 0.000000, 0.000000, 3.548135, 6.1691…\n$ lod_p5     &lt;dbl&gt; 7.246538, 9.221450, 0.000000, 0.000000, 3.548135, 6.1691…\n$ lod_p10    &lt;dbl&gt; 8.816542, 9.626357, 0.000000, 0.000000, 3.548135, 7.2465…\n$ lod_p25    &lt;dbl&gt; 10.004342, 12.022922, 36.648839, 12.578082, 6.991691, 8.…\n$ lod_p75    &lt;dbl&gt; 41.683092, 21.254056, 92.329411, 37.415277, 17.646617, 1…\n$ lod_p90    &lt;dbl&gt; 51.41031, 24.08086, 173.81652, 52.71300, 23.92554, 20.24…\n$ lod_p95    &lt;dbl&gt; 54.85181, 26.02628, 195.79042, 61.69363, 30.89167, 56.67…\n$ lod_p99    &lt;dbl&gt; 79.70102, 27.58261, 320.74738, 72.58377, 53.74435, 131.1…\n$ erf_mean   &lt;dbl&gt; -104.2561, -113.7778, -101.0435, -105.5529, -116.0254, -…\n$ erf_median &lt;dbl&gt; -104.0, -113.0, -110.0, -112.0, -116.5, -116.0, -115.0, …\n$ erf_sd     &lt;dbl&gt; 6.043993, 2.818589, 31.073261, 23.172126, 2.373147, 2.38…\n$ erf_mim    &lt;dbl&gt; -118, -118, -117, -116, -120, -125, -118, -120, -118, -1…\n$ erf_max    &lt;dbl&gt; -95, -110, 0, 0, -107, -105, -98, -95, -97, -105, -105, …\n$ erf_iqr    &lt;dbl&gt; 9.0, 3.0, 4.0, 6.0, 3.0, 4.0, 2.0, 8.0, 5.0, 6.0, 3.0, 1…\n$ erf_p1     &lt;dbl&gt; -118.00, -118.00, -116.00, -116.00, -119.83, -119.00, -1…\n$ erf_p5     &lt;dbl&gt; -117.0, -118.0, -115.0, -115.0, -119.0, -119.0, -118.0, …\n$ erf_p10    &lt;dbl&gt; -113.4, -118.0, -114.0, -114.0, -119.0, -118.0, -117.0, …\n$ erf_p25    &lt;dbl&gt; -108.0, -115.0, -112.0, -113.0, -118.0, -118.0, -116.0, …\n$ erf_p75    &lt;dbl&gt; -99, -112, -108, -107, -115, -114, -114, -108, -109, -11…\n$ erf_p90    &lt;dbl&gt; -98.0, -110.8, -106.0, -104.0, -113.0, -112.0, -108.0, -…\n$ erf_p95    &lt;dbl&gt; -97.00, -110.40, 0.00, -101.00, -111.85, -112.00, -103.2…\n$ erf_p99    &lt;dbl&gt; -96.00, -110.08, 0.00, 0.00, -108.34, -110.51, -99.64, -…\n$ cqi_mean   &lt;dbl&gt; 9.215259, 9.888889, 8.310223, 7.435897, 11.661017, 10.38…\n$ cqi_median &lt;dbl&gt; 9, 10, 9, 8, 12, 11, 10, 10, 11, 11, 11, 11, 11, 8, 9, 1…\n$ cqi_sd     &lt;dbl&gt; 0.7892629, 1.1666667, 2.7890755, 2.4289043, 1.1033774, 0…\n$ cqi_mim    &lt;int&gt; 8, 8, 0, 0, 9, 8, 9, 8, 9, 8, 7, 8, 8, 0, 0, 8, 8, 8, 8,…\n$ cqi_max    &lt;int&gt; 13, 12, 12, 13, 14, 13, 13, 14, 15, 12, 12, 12, 13, 11, …\n$ cqi_iqr    &lt;dbl&gt; 0.00, 1.00, 2.00, 1.00, 1.75, 1.00, 1.00, 1.00, 2.00, 2.…\n$ cqi_p1     &lt;dbl&gt; 8.00, 8.08, 0.00, 0.00, 9.00, 8.00, 9.00, 8.00, 9.00, 8.…\n$ cqi_p5     &lt;dbl&gt; 8.0, 8.4, 0.0, 0.0, 10.0, 9.0, 9.0, 8.0, 10.0, 9.0, 8.0,…\n$ cqi_p10    &lt;dbl&gt; 9.0, 8.8, 8.0, 7.0, 10.0, 9.0, 10.0, 9.0, 10.0, 9.0, 8.0…\n$ cqi_p25    &lt;dbl&gt; 9, 9, 8, 7, 11, 10, 10, 9, 10, 10, 8, 10, 10, 6, 9, 11, …\n$ cqi_p75    &lt;dbl&gt; 9.00, 10.00, 10.00, 8.00, 12.75, 11.00, 11.00, 10.00, 12…\n$ cqi_p90    &lt;dbl&gt; 10.0, 11.2, 11.0, 10.0, 13.0, 12.0, 12.0, 13.0, 12.0, 12…\n$ cqi_p95    &lt;dbl&gt; 11.0, 11.6, 11.0, 11.0, 13.0, 12.0, 12.0, 13.0, 13.0, 12…\n$ cqi_p99    &lt;dbl&gt; 12.34, 11.92, 11.00, 12.00, 13.83, 12.00, 12.18, 14.00, …\n$ m64_mean   &lt;dbl&gt; 44.89873, 58.24060, 38.40047, 26.80058, 76.31929, 61.342…\n$ m64_median &lt;dbl&gt; 43.39673, 61.32330, 34.40925, 24.28990, 76.84731, 63.394…\n$ m64_sd     &lt;dbl&gt; 9.698269, 12.591087, 20.363911, 16.873688, 12.378953, 13…\n$ m64_mim    &lt;dbl&gt; 30.94235, 42.03163, 0.00000, 0.00000, 45.28808, 28.76731…\n$ m64_max    &lt;dbl&gt; 90.82736, 82.58227, 80.52400, 93.24300, 97.39987, 94.569…\n$ m64_iqr    &lt;dbl&gt; 8.749693, 14.927282, 32.785113, 11.233352, 18.240969, 20…\n$ m64_p1     &lt;dbl&gt; 31.91061, 42.30473, 0.00000, 0.00000, 46.29397, 31.80753…\n$ m64_p5     &lt;dbl&gt; 33.92042, 43.39716, 0.00000, 0.00000, 52.72627, 36.69374…\n$ m64_p10    &lt;dbl&gt; 35.374808, 44.762704, 18.394982, 13.481201, 57.588889, 4…\n$ m64_p25    &lt;dbl&gt; 38.619124, 47.565217, 23.603871, 17.976758, 68.522559, 5…\n$ m64_p75    &lt;dbl&gt; 47.36882, 62.49250, 56.38898, 29.21011, 86.76353, 71.927…\n$ m64_p90    &lt;dbl&gt; 55.10100, 70.32728, 64.21456, 55.90757, 91.54501, 77.115…\n$ m64_p95    &lt;dbl&gt; 66.00790, 76.45478, 69.24536, 66.93950, 92.90575, 79.568…\n$ m64_p99    &lt;dbl&gt; 86.59251, 81.35677, 74.13244, 79.22310, 94.81734, 83.711…\n$ m16_mean   &lt;dbl&gt; 35.18364, 21.10646, 32.80972, 33.97866, 17.64001, 25.328…\n$ m16_median &lt;dbl&gt; 35.84276, 19.85714, 33.68667, 37.02408, 17.29190, 24.258…\n$ m16_sd     &lt;dbl&gt; 5.203023, 4.497056, 12.804592, 11.550452, 8.702843, 7.43…\n$ m16_mim    &lt;dbl&gt; 7.5511541, 14.4189240, 0.0000000, 0.0000000, 2.3901809, …\n$ m16_max    &lt;dbl&gt; 46.66894, 29.03077, 52.77981, 76.64844, 39.76586, 54.094…\n$ m16_iqr    &lt;dbl&gt; 3.670762, 4.914433, 15.999362, 7.191692, 13.252273, 11.2…\n$ m16_p1     &lt;dbl&gt; 11.773813, 14.690344, 0.000000, 0.000000, 4.637156, 11.3…\n$ m16_p5     &lt;dbl&gt; 26.083059, 15.776026, 0.000000, 0.000000, 5.735980, 14.5…\n$ m16_p10    &lt;dbl&gt; 30.575224, 17.133129, 20.226541, 23.725627, 7.236646, 16…\n$ m16_p25    &lt;dbl&gt; 33.84320, 18.17742, 27.41668, 32.95932, 10.22894, 19.673…\n$ m16_p75    &lt;dbl&gt; 37.51396, 23.09185, 43.41604, 40.15101, 23.48121, 30.877…\n$ m16_p90    &lt;dbl&gt; 40.06997, 26.76147, 46.04141, 42.57644, 30.27416, 36.108…\n$ m16_p95    &lt;dbl&gt; 41.67208, 27.89612, 47.04958, 44.60849, 33.58483, 38.169…\n$ m16_p99    &lt;dbl&gt; 44.42825, 28.80384, 49.22442, 48.94161, 36.35043, 42.276…\n$ psk_mean   &lt;dbl&gt; 19.917631, 20.652931, 20.094164, 31.688701, 6.040698, 13…\n$ psk_median &lt;dbl&gt; 20.009491, 19.233237, 18.835103, 34.001752, 5.411669, 12…\n$ psk_sd     &lt;dbl&gt; 5.729643, 13.031483, 11.642831, 15.235847, 4.284353, 7.5…\n$ psk_mim    &lt;dbl&gt; 1.1143483, 2.9988066, 0.0000000, 0.0000000, 0.2099483, 0…\n$ psk_max    &lt;dbl&gt; 34.96912, 39.79095, 45.68945, 59.02762, 19.06067, 45.955…\n$ psk_iqr    &lt;dbl&gt; 6.629984, 20.857374, 19.876361, 15.597391, 5.190912, 11.…\n$ psk_p1     &lt;dbl&gt; 2.5051579, 3.5304132, 0.0000000, 0.0000000, 0.5406669, 2…\n$ psk_p5     &lt;dbl&gt; 8.7878654, 5.6568396, 0.0000000, 0.0000000, 1.0768613, 3…\n$ psk_p10    &lt;dbl&gt; 13.528083, 8.314873, 5.499492, 5.032897, 1.400817, 4.705…\n$ psk_p25    &lt;dbl&gt; 17.028782, 9.644613, 10.956800, 27.660760, 2.737518, 7.1…\n$ psk_p75    &lt;dbl&gt; 23.658765, 30.501987, 30.833161, 43.258151, 7.928431, 18…\n$ psk_p90    &lt;dbl&gt; 26.89947, 35.71610, 33.92600, 48.57697, 12.05614, 24.525…\n$ psk_p95    &lt;dbl&gt; 28.35405, 37.75353, 36.22171, 51.20470, 14.94215, 27.976…\n$ psk_p99    &lt;dbl&gt; 30.56762, 39.38347, 41.47907, 55.56988, 18.03397, 32.182…\n$ vol_mean   &lt;dbl&gt; 4.4438106, 0.8981019, 7.7268418, 3.8684795, 0.7413225, 1…\n$ vol_median &lt;dbl&gt; 3.16766667, 0.73752778, 6.91147222, 3.46243056, 0.152680…\n$ vol_sd     &lt;dbl&gt; 3.7352340, 0.8331168, 7.5678281, 2.8814852, 1.2766673, 1…\n$ vol_mim    &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, …\n$ vol_max    &lt;dbl&gt; 14.420917, 2.016056, 40.468889, 19.614139, 7.250917, 13.…\n$ vol_iqr    &lt;dbl&gt; 6.5919167, 1.3402778, 11.2731944, 3.5566181, 0.9582083, …\n$ vol_p1     &lt;dbl&gt; 0.00000000000, 0.00002666667, 0.00000000000, 0.000000000…\n$ vol_p5     &lt;dbl&gt; 0.0021055556, 0.0001333333, 0.0000000000, 0.1732180556, …\n$ vol_p10    &lt;dbl&gt; 0.10488888889, 0.00026666667, 0.00127777778, 0.647061111…\n$ vol_p25    &lt;dbl&gt; 1.218430556, 0.173083333, 0.086402778, 1.757006944, 0.00…\n$ vol_p75    &lt;dbl&gt; 7.8103472, 1.5133611, 11.3595972, 5.3136250, 0.9593611, …\n$ vol_p90    &lt;dbl&gt; 9.823144, 2.005833, 17.881889, 7.341772, 2.244422, 3.688…\n$ vol_p95    &lt;dbl&gt; 10.560189, 2.010944, 21.935500, 8.987101, 2.990542, 4.52…\n$ vol_p99    &lt;dbl&gt; 11.889142, 2.015033, 31.128333, 12.908034, 4.955404, 6.7…\n$ dis_mean   &lt;dbl&gt; 255.694822888, 47.777777778, 0.005875441, 18.269230769, …\n$ dis_median &lt;dbl&gt; 195.0, 40.0, 0.0, 0.0, 432.5, 0.0, 155.0, 0.0, 0.0, 0.0,…\n$ dis_sd     &lt;dbl&gt; 260.9335794, 24.8886409, 0.1713978, 222.7733798, 1091.39…\n$ dis_mim    &lt;int&gt; 5, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 30, 50, 0, 0, 0, 0, 0, …\n$ dis_max    &lt;int&gt; 2265, 80, 5, 3600, 3280, 110, 2755, 0, 0, 30, 0, 1570, 2…\n$ dis_iqr    &lt;dbl&gt; 262.5, 35.0, 0.0, 0.0, 1882.5, 0.0, 1325.0, 0.0, 0.0, 0.…\n$ dis_p1     &lt;dbl&gt; 5.0, 7.4, 0.0, 0.0, 26.7, 0.0, 5.0, 0.0, 0.0, 0.0, 0.0, …\n$ dis_p5     &lt;dbl&gt; 15, 17, 0, 0, 105, 0, 5, 0, 0, 0, 0, 94, 86, 0, 0, 0, 0,…\n$ dis_p10    &lt;dbl&gt; 25, 29, 0, 0, 105, 0, 6, 0, 0, 0, 0, 113, 90, 0, 0, 0, 0…\n$ dis_p25    &lt;dbl&gt; 77.5, 35.0, 0.0, 0.0, 115.0, 0.0, 40.0, 0.0, 0.0, 0.0, 0…\n$ dis_p75    &lt;dbl&gt; 340.0, 70.0, 0.0, 0.0, 1997.5, 0.0, 1365.0, 0.0, 0.0, 0.…\n$ dis_p90    &lt;dbl&gt; 555, 80, 0, 0, 2775, 0, 2319, 0, 0, 0, 0, 448, 2300, 0, …\n$ dis_p95    &lt;dbl&gt; 745.0, 80.0, 0.0, 0.0, 3145.0, 0.0, 2436.5, 0.0, 0.0, 0.…\n$ dis_p99    &lt;dbl&gt; 1147.3, 80.0, 0.0, 0.0, 3270.0, 0.0, 2599.2, 0.0, 0.0, 0…\n$ lte_mean   &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_median &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_sd     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lte_mim    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_max    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_iqr    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lte_p1     &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p5     &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p10    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p25    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p75    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p90    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p95    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lte_p99    &lt;dbl&gt; 99.86997, 99.96593, 98.38563, 99.79480, 98.39689, 92.007…\n$ lat        &lt;dbl&gt; 14.60020, 14.56400, 15.81190, 14.48720, 16.91020, 14.779…\n$ lon        &lt;dbl&gt; -90.51580, -90.73730, -89.87600, -90.45580, -89.90830, -…\n$ r          &lt;dbl&gt; 91.68575, 91.89868, 91.25630, 91.60858, 91.48474, 91.987…\n$ theta      &lt;dbl&gt; -1.410874, -1.411646, -1.396648, -1.411987, -1.384885, -…\n$ diag       &lt;fct&gt; DISPONIBILIDAD, DISPONIBILIDAD, OPTIMIZACION, OPTIMIZACI…\n\n\n\n\nMostrar Código\nctl_train |&gt; \n select(2:9) |&gt; \n slice_head(n = 5) |&gt; \n gt() |&gt; \n tab_header(\n    title = md(\"**Muestra Aleatoria - Datos de Entrenamiento**\"),\n    subtitle = md(\"Tabla principal\")\n  ) |&gt; \n gt_theme_538() |&gt; \n # fmt_integer(columns = user, use_seps = FALSE) |&gt; \n fmt_number(columns = 2:8, decimals = 2) |&gt; \n cols_align_decimal() |&gt; \n cols_align(align = \"center\", columns = where(~ is.numeric(.x)))\n\n\n\n\n\nTable 8.1:  Solo 8 columnas y 5 filas \n  \n    \n      Muestra Aleatoria - Datos de Entrenamiento\n    \n    \n      Tabla principal\n    \n    \n      prb_mean\n      prb_median\n      prb_sd\n      prb_mim\n      prb_max\n      prb_iqr\n      prb_p1\n      prb_p5\n    \n  \n  \n    37.48692\n34.26\n22.27\n 1.00\n83.45\n40.20\n 4.62\n 8.12\n    48.09889\n41.36\n20.11\n22.33\n81.18\n23.52\n22.49\n23.13\n    80.19165\n87.74\n25.67\n 0.00\n99.03\n12.08\n 0.00\n 0.00\n    44.20385\n44.66\n24.68\n 0.00\n95.43\n32.85\n 0.00\n 0.00\n    39.41634\n31.05\n25.07\n 4.88\n96.22\n38.16\n 5.13\n 7.52\n  \n  \n  \n\n\n\n\n\n\n\nCódigo\nplot_intro(\n ctl_train,\n ggtheme = yunkel,\n title = \"Resumen Descriptivo\",\n geom_label_args = list(label.size = 0.8, size = 7),\n theme_config = list(\n axis.text = element_text(size = 40)))\n\n\n\n\n\nFigure 8.1: Se puede apreciar que la gran mayoría de las columnas están completas. Todas las columnas son de tipo continuo."
  },
  {
    "objectID": "analisis-exploratorio.html#valores-faltantes",
    "href": "analisis-exploratorio.html#valores-faltantes",
    "title": "8  Análisis Exploratorio",
    "section": "8.2 Valores faltantes",
    "text": "8.2 Valores faltantes\nDebido a que en la fase de preparación determinamos que la cantidad de valores faltantes era de dos o tres filas, optamos por eliminarlas sin riesgo de algún detrimento por pérdida de información.\nUtilizaremos un procedimiento que nos permita evaluar rápidamente si hay valores faltantes en algunos de los predictores\n\nctl_train |&gt; \n inspect_na() |&gt; \n with(all(cnt != 0))\n\n[1] FALSE"
  },
  {
    "objectID": "analisis-exploratorio.html#sec-estadisticas",
    "href": "analisis-exploratorio.html#sec-estadisticas",
    "title": "8  Análisis Exploratorio",
    "section": "8.3 Resumen estadístico",
    "text": "8.3 Resumen estadístico\n\n\n\nMostrar Código\nctl_train |&gt; \n select(-user, -diag) |&gt; \n resumir() |&gt; \n arrange(variable) |&gt; \n gt() |&gt; \n tab_header(\n  title = md(\"**Resumen Estadístico**\"),\n    subtitle = md(\"para *features* Numéricos\")\n ) |&gt;\n gt_theme_538() |&gt; \n cols_align(align = \"center\", columns = where(~ is.numeric(.x))) |&gt; \n fmt_number(columns = where(is.numeric), decimals = 2)\n\n\n\n\n\nTable 8.2:  Resumen estadístico \n  \n    \n      Resumen Estadístico\n    \n    \n      para features Numéricos\n    \n    \n      variable\n      media\n      mediana\n      maximo\n      minimo\n      sd\n    \n  \n  \n    cqi_iqr\n1.49\n1.00\n11.00\n0.00\n0.90\n    cqi_max\n13.00\n13.00\n15.00\n9.00\n0.95\n    cqi_mean\n10.32\n10.42\n13.60\n4.19\n0.92\n    cqi_median\n10.33\n10.00\n14.00\n0.00\n1.05\n    cqi_mim\n7.25\n8.00\n13.00\n0.00\n2.73\n    cqi_p1\n7.83\n8.00\n13.00\n0.00\n2.28\n    cqi_p10\n8.92\n9.00\n13.00\n0.00\n1.50\n    cqi_p25\n9.59\n10.00\n13.00\n0.00\n1.08\n    cqi_p5\n8.53\n9.00\n13.00\n0.00\n1.80\n    cqi_p75\n11.09\n11.00\n14.00\n7.00\n1.03\n    cqi_p90\n11.72\n12.00\n14.00\n8.00\n1.01\n    cqi_p95\n12.01\n12.00\n14.00\n8.00\n0.97\n    cqi_p99\n12.46\n12.89\n15.00\n9.00\n0.91\n    cqi_sd\n1.18\n1.09\n5.89\n0.00\n0.55\n    dis_iqr\n76.27\n0.00\n3,600.00\n0.00\n325.04\n    dis_max\n698.07\n0.00\n3,600.00\n0.00\n1,215.24\n    dis_mean\n68.07\n0.00\n2,116.65\n0.00\n222.79\n    dis_median\n37.62\n0.00\n3,600.00\n0.00\n188.77\n    dis_mim\n3.79\n0.00\n530.00\n0.00\n18.96\n    dis_p1\n4.21\n0.00\n530.00\n0.00\n19.72\n    dis_p10\n7.61\n0.00\n1,180.00\n0.00\n36.40\n    dis_p25\n15.37\n0.00\n2,490.00\n0.00\n88.18\n    dis_p5\n5.98\n0.00\n955.00\n0.00\n29.16\n    dis_p75\n91.64\n0.00\n3,600.00\n0.00\n366.99\n    dis_p90\n161.66\n0.00\n3,600.00\n0.00\n558.93\n    dis_p95\n214.24\n0.00\n3,600.00\n0.00\n680.93\n    dis_p99\n323.17\n0.00\n3,600.00\n0.00\n889.29\n    dis_sd\n104.48\n0.00\n1,799.93\n0.00\n258.00\n    drp_iqr\n0.12\n0.07\n12.67\n0.00\n0.35\n    drp_max\n2.28\n1.37\n100.00\n0.00\n5.30\n    drp_mean\n0.12\n0.07\n9.27\n0.00\n0.27\n    drp_median\n0.06\n0.03\n3.23\n0.00\n0.13\n    drp_mim\n0.00\n0.00\n0.09\n0.00\n0.00\n    drp_p1\n0.00\n0.00\n0.09\n0.00\n0.01\n    drp_p10\n0.01\n0.00\n1.75\n0.00\n0.04\n    drp_p25\n0.02\n0.01\n2.42\n0.00\n0.06\n    drp_p5\n0.00\n0.00\n1.25\n0.00\n0.02\n    drp_p75\n0.14\n0.08\n12.67\n0.00\n0.36\n    drp_p90\n0.30\n0.16\n35.23\n0.00\n0.89\n    drp_p95\n0.46\n0.23\n48.95\n0.00\n1.27\n    drp_p99\n0.98\n0.49\n64.95\n0.00\n2.28\n    drp_sd\n0.22\n0.11\n16.84\n0.00\n0.53\n    erb_iqr\n0.61\n0.18\n99.98\n0.00\n5.47\n    erb_max\n100.00\n100.00\n100.00\n93.41\n0.12\n    erb_mean\n99.14\n99.81\n100.00\n22.95\n3.25\n    erb_median\n99.78\n99.91\n100.00\n0.00\n2.46\n    erb_mim\n85.17\n96.69\n100.00\n0.00\n30.94\n    erb_p1\n91.50\n98.90\n100.00\n0.00\n25.19\n    erb_p10\n97.49\n99.57\n100.00\n0.00\n13.23\n    erb_p25\n99.30\n99.78\n100.00\n0.00\n5.75\n    erb_p5\n95.82\n99.38\n100.00\n0.00\n17.49\n    erb_p75\n99.90\n99.97\n100.00\n0.00\n1.73\n    erb_p90\n99.98\n100.00\n100.00\n92.95\n0.14\n    erb_p95\n99.99\n100.00\n100.00\n93.18\n0.12\n    erb_p99\n99.99\n100.00\n100.00\n93.37\n0.12\n    erb_sd\n2.21\n0.28\n49.91\n0.00\n6.48\n    erf_iqr\n4.63\n4.00\n118.00\n0.00\n6.61\n    erf_max\n−92.14\n−103.00\n0.00\n−119.00\n33.63\n    erf_mean\n−112.72\n−113.73\n−46.45\n−120.08\n4.66\n    erf_median\n−113.77\n−115.00\n0.00\n−120.00\n4.03\n    erf_mim\n−119.32\n−119.00\n−101.00\n−149.00\n4.29\n    erf_p1\n−117.96\n−118.00\n−100.93\n−126.51\n1.73\n    erf_p10\n−116.78\n−117.00\n−93.00\n−122.00\n2.23\n    erf_p25\n−115.63\n−116.00\n−90.00\n−121.00\n2.80\n    erf_p5\n−117.25\n−118.00\n−94.00\n−122.00\n2.00\n    erf_p75\n−111.00\n−112.00\n0.00\n−120.00\n7.33\n    erf_p90\n−107.05\n−109.15\n0.00\n−119.00\n15.25\n    erf_p95\n−104.34\n−108.00\n0.00\n−119.00\n19.63\n    erf_p99\n−98.88\n−106.00\n0.00\n−119.00\n27.25\n    erf_sd\n4.92\n3.15\n57.53\n0.00\n6.74\n    lat\n14.89\n14.70\n17.23\n13.87\n0.54\n    lod_iqr\n23.71\n15.43\n590.78\n0.23\n29.50\n    lod_max\n205.53\n106.63\n8,428.06\n6.17\n317.29\n    lod_mean\n33.85\n21.14\n448.65\n3.95\n31.77\n    lod_median\n26.83\n16.02\n409.53\n0.00\n26.21\n    lod_mim\n6.58\n5.32\n48.55\n0.00\n6.41\n    lod_p1\n8.93\n6.03\n70.74\n0.00\n8.70\n    lod_p10\n13.39\n7.25\n144.89\n0.00\n12.83\n    lod_p25\n17.97\n10.00\n202.36\n0.00\n16.69\n    lod_p5\n11.53\n6.92\n110.62\n0.00\n11.15\n    lod_p75\n41.68\n26.30\n693.42\n3.62\n41.90\n    lod_p90\n62.94\n40.93\n852.69\n4.41\n65.05\n    lod_p95\n80.04\n50.57\n1,015.31\n5.91\n86.34\n    lod_p99\n120.37\n70.89\n1,904.30\n6.12\n143.03\n    lod_sd\n25.09\n14.89\n400.82\n0.47\n30.52\n    lon\n−90.75\n−90.60\n−88.40\n−92.16\n0.70\n    lte_iqr\n0.00\n0.00\n0.00\n0.00\n0.00\n    lte_max\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_mean\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_median\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_mim\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p1\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p10\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p25\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p5\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p75\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p90\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p95\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_p99\n92.07\n98.38\n100.00\n0.00\n14.96\n    lte_sd\n0.00\n0.00\n0.00\n0.00\n0.00\n    m16_iqr\n11.25\n10.86\n38.78\n0.73\n4.69\n    m16_max\n49.12\n48.86\n100.00\n5.69\n9.21\n    m16_mean\n26.05\n25.94\n51.65\n3.88\n5.18\n    m16_median\n26.16\n26.10\n51.47\n0.00\n6.05\n    m16_mim\n6.86\n5.44\n45.47\n0.00\n5.87\n    m16_p1\n10.23\n9.64\n46.39\n0.00\n6.08\n    m16_p10\n15.66\n15.12\n48.37\n0.00\n6.57\n    m16_p25\n20.45\n19.85\n50.20\n0.00\n6.36\n    m16_p5\n13.50\n12.87\n47.71\n0.00\n6.50\n    m16_p75\n31.71\n31.99\n52.91\n5.10\n5.60\n    m16_p90\n36.02\n36.45\n73.31\n5.45\n5.30\n    m16_p95\n38.26\n38.65\n93.74\n5.57\n5.29\n    m16_p99\n41.99\n42.21\n96.89\n5.66\n5.58\n    m16_sd\n7.97\n7.93\n20.17\n1.03\n2.32\n    m64_iqr\n19.46\n18.74\n86.17\n0.39\n8.83\n    m64_max\n89.30\n91.56\n100.00\n39.71\n8.06\n    m64_mean\n59.98\n61.14\n95.12\n17.36\n10.96\n    m64_median\n60.41\n62.04\n94.73\n0.00\n12.91\n    m64_mim\n26.95\n28.19\n93.26\n0.00\n12.99\n    m64_p1\n32.52\n32.78\n93.29\n0.00\n12.84\n    m64_p10\n42.21\n41.86\n93.55\n0.00\n12.71\n    m64_p25\n50.44\n51.00\n94.00\n0.00\n12.68\n    m64_p5\n38.16\n37.93\n93.40\n0.00\n12.71\n    m64_p75\n69.90\n72.15\n96.86\n19.49\n12.21\n    m64_p90\n77.14\n79.10\n98.29\n28.21\n10.90\n    m64_p95\n80.26\n81.97\n99.54\n33.70\n9.92\n    m64_p99\n84.71\n86.32\n99.99\n38.04\n8.49\n    m64_sd\n13.60\n13.36\n42.76\n0.56\n4.25\n    prb_iqr\n26.89\n28.54\n89.28\n0.25\n15.15\n    prb_max\n94.18\n96.51\n99.81\n15.37\n8.13\n    prb_mean\n57.32\n52.09\n97.42\n5.16\n20.13\n    prb_median\n57.60\n52.24\n97.75\n0.00\n22.60\n    prb_mim\n14.57\n4.20\n87.39\n0.00\n23.20\n    prb_p1\n21.28\n9.15\n90.16\n0.00\n26.52\n    prb_p10\n33.67\n21.86\n96.85\n0.00\n27.71\n    prb_p25\n44.04\n34.39\n97.53\n0.00\n26.20\n    prb_p5\n28.56\n16.46\n95.89\n0.00\n27.72\n    prb_p75\n70.93\n70.44\n98.95\n1.19\n18.03\n    prb_p90\n80.76\n82.88\n99.44\n6.23\n13.79\n    prb_p95\n85.09\n87.67\n99.55\n15.16\n11.69\n    prb_p99\n90.26\n92.99\n99.63\n15.33\n9.30\n    prb_sd\n18.47\n20.08\n44.56\n1.39\n7.60\n    psk_iqr\n9.50\n8.60\n50.26\n0.01\n5.35\n    psk_max\n35.80\n34.68\n100.00\n1.24\n11.79\n    psk_mean\n13.39\n12.29\n53.05\n1.01\n6.22\n    psk_median\n12.41\n10.84\n63.15\n0.00\n7.23\n    psk_mim\n1.88\n1.12\n19.67\n0.00\n2.27\n    psk_p1\n2.99\n2.33\n19.76\n0.00\n2.60\n    psk_p10\n5.44\n4.39\n38.31\n0.00\n4.15\n    psk_p25\n8.21\n6.78\n43.46\n0.00\n5.69\n    psk_p5\n4.37\n3.54\n35.40\n0.00\n3.48\n    psk_p75\n17.71\n16.36\n68.69\n0.92\n8.26\n    psk_p90\n22.79\n22.22\n84.39\n1.23\n8.88\n    psk_p95\n25.67\n25.35\n97.35\n1.24\n9.22\n    psk_p99\n30.11\n29.67\n99.89\n1.24\n9.86\n    psk_sd\n6.94\n6.62\n32.04\n0.01\n2.87\n    r\n91.96\n91.78\n93.39\n89.76\n0.69\n    rrc_iqr\n0.66\n0.20\n99.99\n0.00\n5.93\n    rrc_max\n100.00\n100.00\n100.00\n98.40\n0.04\n    rrc_mean\n99.19\n99.79\n100.00\n26.76\n3.16\n    rrc_median\n99.76\n99.90\n100.00\n0.00\n2.54\n    rrc_mim\n85.88\n96.63\n100.00\n0.00\n30.07\n    rrc_p1\n92.28\n98.87\n100.00\n0.00\n23.79\n    rrc_p10\n97.76\n99.55\n100.00\n0.00\n12.29\n    rrc_p25\n99.26\n99.77\n100.00\n0.00\n5.98\n    rrc_p5\n96.27\n99.36\n100.00\n0.00\n16.37\n    rrc_p75\n99.92\n99.97\n100.00\n61.23\n0.69\n    rrc_p90\n99.97\n100.00\n100.00\n98.24\n0.07\n    rrc_p95\n99.99\n100.00\n100.00\n98.32\n0.06\n    rrc_p99\n99.99\n100.00\n100.00\n98.39\n0.04\n    rrc_sd\n2.03\n0.28\n49.91\n0.00\n6.14\n    tad_iqr\n6.25\n2.64\n93.83\n0.00\n11.17\n    tad_max\n24.48\n14.95\n100.00\n0.00\n24.97\n    tad_mean\n5.63\n2.41\n90.13\n0.00\n9.55\n    tad_median\n4.15\n0.54\n92.57\n0.00\n9.94\n    tad_mim\n0.12\n0.00\n61.73\n0.00\n1.38\n    tad_p1\n0.30\n0.00\n64.78\n0.00\n1.96\n    tad_p10\n0.94\n0.00\n86.41\n0.00\n3.90\n    tad_p25\n1.90\n0.04\n89.90\n0.00\n6.15\n    tad_p5\n0.63\n0.00\n82.06\n0.00\n3.10\n    tad_p75\n8.14\n3.02\n100.00\n0.00\n14.55\n    tad_p90\n12.70\n7.31\n100.00\n0.00\n18.18\n    tad_p95\n15.34\n10.09\n100.00\n0.00\n19.64\n    tad_p99\n19.42\n13.69\n100.00\n0.00\n21.54\n    tad_sd\n5.25\n3.29\n44.04\n0.00\n6.50\n    theta\n−1.41\n−1.41\n−1.38\n−1.42\n0.01\n    thp_iqr\n5.62\n5.01\n34.27\n0.13\n4.30\n    thp_max\n34.53\n32.44\n166.45\n2.63\n23.27\n    thp_mean\n8.35\n8.19\n30.12\n0.85\n4.60\n    thp_median\n7.23\n7.12\n36.76\n0.00\n4.02\n    thp_mim\n1.47\n1.18\n17.01\n0.00\n1.42\n    thp_p1\n2.19\n2.10\n17.04\n0.00\n1.55\n    thp_p10\n3.58\n3.39\n17.99\n0.00\n2.04\n    thp_p25\n4.96\n4.74\n20.25\n0.00\n2.67\n    thp_p5\n2.97\n2.86\n17.31\n0.00\n1.79\n    thp_p75\n10.58\n10.26\n43.77\n1.18\n6.26\n    thp_p90\n14.73\n13.95\n79.52\n1.63\n9.32\n    thp_p95\n17.36\n16.26\n87.32\n2.11\n11.00\n    thp_p99\n22.82\n21.66\n102.36\n2.62\n14.23\n    thp_sd\n4.81\n4.33\n29.01\n0.15\n3.36\n    vol_iqr\n3.28\n2.43\n35.94\n0.00\n3.57\n    vol_max\n15.24\n13.36\n78.42\n0.00\n9.80\n    vol_mean\n2.63\n1.97\n25.71\n0.00\n2.51\n    vol_median\n1.59\n0.63\n31.67\n0.00\n2.59\n    vol_mim\n0.03\n0.00\n5.95\n0.00\n0.27\n    vol_p1\n0.06\n0.00\n8.74\n0.00\n0.37\n    vol_p10\n0.24\n0.01\n15.42\n0.00\n0.87\n    vol_p25\n0.57\n0.08\n19.48\n0.00\n1.45\n    vol_p5\n0.14\n0.00\n12.23\n0.00\n0.62\n    vol_p75\n3.85\n2.77\n36.13\n0.00\n4.17\n    vol_p90\n6.66\n5.42\n42.32\n0.00\n5.41\n    vol_p95\n8.47\n7.14\n48.38\n0.00\n6.17\n    vol_p99\n11.39\n9.78\n63.48\n0.00\n7.62\n    vol_sd\n2.93\n2.45\n18.60\n0.00\n2.10\n  \n  \n  \n\n\n\n\n\nEn la tabla Table 8.2 hemos extraído las estadísticas de resumen. Lo interesante es que estas son estadísticas descriptivas aplicadas a estadísticas. En el contexto de este proyecto tiene sentido realizar este análisis ya que estos features servirán de Input para el modelo, por lo que su distribución y forma son relevantes."
  },
  {
    "objectID": "analisis-exploratorio.html#análisis-univariado",
    "href": "analisis-exploratorio.html#análisis-univariado",
    "title": "8  Análisis Exploratorio",
    "section": "8.4 Análisis univariado",
    "text": "8.4 Análisis univariado\n\n8.4.1 Variable dependiente\nUno de los primeros pasos del proceso de análisis exploratorios cuando el propósito final es predecir una respuesta es crear visualizaciones que ayuden a dilucidar el conocimiento de la respuesta y luego descubrir relaciones entre los predictores y la respuesta (Kuhn and Johnson 2020).\n\n\nCódigo\nctl_train |&gt; \n barra(diag) +\n theme(legend.position = \"none\") +\n labs(title = \"Clasificación variable politómica\")\n\n\n\n\n\nFigure 8.2: Distribución de la variable dependiente\n\n\n\n\nTal como se muestra en la Figure 8.2 se observa un desbalance en la variable respuesta. Este desequilibrio puede ocasionar que el modelo se sesgue hacia las clases más frecuentes, como “CAPACIDAD” y “PROMOTOR”, lo que podría llevar a un rendimiento deficiente en la clasificación de otras clases. La aplicación de técnicas de remuestreo para balancear las clases será clave en la fase de preprocesamiento.\n\n\n8.4.2 Mutual information\nLa Información Mutua es una métrica de utilidad de características que va más allá de las correlaciones convencionales de Pearson y Spearman. Mientras que estas últimas son excelentes para detectar relaciones lineales o monótonas, tienen sus limitaciones. Por ejemplo, no pueden capturar relaciones más complejas o no lineales entre variables. Además, las correlaciones tradicionales suelen centrarse en variables numéricas, dejando de lado las categóricas.\nEn contraste, la Información Mutua es una herramienta poderosa que puede capturar cualquier tipo de relación entre variables, ya sean numéricas o categóricas. Funciona evaluando cómo el conocimiento de una característica reduce la incertidumbre sobre la variable objetivo. En otras palabras, si conoces el valor de una característica, ¿cuánto más seguro te sentirías sobre el valor de la variable objetivo? Esta capacidad la convierte en una métrica invaluable, especialmente en las etapas iniciales del desarrollo de características, donde aún no sabemos qué modelo vamos a utilizar.\nEs importante tener en cuenta que la Información Mutua es una métrica univariada, lo que significa que evalúa cada característica de forma individual en relación con la variable objetivo. No puede detectar interacciones entre características, pero sigue siendo una herramienta extremadamente útil para una primera ronda de selección de características.\nPara poder tener un dataset más amplio en cuanto a features, realizaremos un procedimiento con la fecha llamado feature template (Duboue 2020, pag. 30). En este caso, lo que se realizará es la descomposición de la fecha en sus parte constituyentes.\n\nmi &lt;- information_gain(\n formula      = diag ~ ., \n data         = ctl_train |&gt; select(-user), \n type         = \"infogain\", # Symmetrical Uncertainty (SU)\n discIntegers = FALSE) |&gt; \n as_tibble() |&gt; \n arrange(-importance)\n\n\nmi |&gt; \n slice_head(n = 20) |&gt; \n mutate(feature = fct_reorder(attributes, importance, .desc = FALSE)) |&gt; \n ggplot(aes(x = importance, y = feature)) +\n geom_point() +\n labs(title = \"Information Gain\") +\n drako +\n theme(axis.text.y = element_text(size = 30),\n       axis.text.x = element_text(size = 30))\n\n\n\n\nFigure 8.3: Information Gain\n\n\n\n\nA partir de lo identificado en MI, podemos explorar algunas de las variables con mayor poder predictivo.\n\n\n8.4.3 Variables numéricas\nAnalizar la distribución de cada predictor nos puede orientar sobre si necesitamos hacer ingeniería de características mediante transformaciones antes del análisis. (Kuhn and Johnson 2020)\n\npredictores &lt;- c(\"dis_mim\", \"cqi_mim\", \"tad_max\", \"drp_max\", \"dis_mean\",\n                 \"tad_p95\", \"lod_p5\", \"thp_p5\", \"thp_max\", \"lod_p99\")\n\n\ndob &lt;- ctl_train |&gt; \n select(all_of(predictores))\n\n\n# parámetros para graficar distribuciones\npal &lt;- allcolors[1:ncol(dob)]\nndv &lt;- names(dob)\nnar &lt;- str_to_title(ndv)\n\n\n# crear distribuciones\ndist_predictores &lt;- list(ndv, pal, nar) |&gt;\n  pmap(~ estimar_densidad(df = dob, d = ..1, color = ..2, titulo = ..1))\n\n\n\nMostrar Código\ndist_predictores[1:2] |&gt; \n reduce(.f = `+`) +\n plot_layout(ncol = 2) +\n plot_annotation(title    = \"Distribución predictores numéricos\")\n\n\n\n\n\n\ndis_mim: Representa la distribución de los valores mínimos de disponibilidad. Esta variable extrae por cada usuario el valor mínimo de disponibilidad. Podría ser relevante para saber cuando no es un problema de este tipo. La distribución muestra un sesgo positivo con valores atípicos que llegan hasta los 530 segundos (ver tabla Table 8.2).\ncqi_mim: El corrected_cqi Evalúa la calidad de una celda en una red móvil. Este índice toma en cuenta varios factores como la potencia de la señal, el nivel de interferencia y otros parámetros clave para dar un indicador comprensivo de cómo está funcionando una celda en particular. Aunque los detalles pueden variar según el fabricante y la implementación, en esencia, mide qué tan bien una celda puede manejar el tráfico y proporcionar un servicio de alta calidad a los usuarios. En este caso, la distribución de los valores mínimos evidencia una media de 7.2 y mediana de 8, lo cual indica que la gran mayoría de usuarios se encuentran en los rangos correctos. Se observa, sin embargo, que hay picos entre 0 y 1, lo que podría indicar valores donde se existan problemas de optimización.\n\n\n\n\n\n\n\ntad_max: El feature tad se construyó en la etapa de preparación como el resultado de la suma de los índices 6 y 7. Los SMEs indicaron que podría ser útil para determinar problemas de cobertura o capacidad en vista de que valores muy altos podrían asociarse casi siempre a problemas con la cobertura y valores bajos interactuando con otras métricas podrían relacionarse con temas de capacidad. Aquí vemos que la distribución de los valores máximo tiene un pico muy acentuado en 14%, lo cual indica que la gran mayoría de los usuarios se encuentran, según la gráfica, en promedio en este punto o por debajo.\ndrp_max: El service_drop_rate representa perdida en los servicios de llamadas o internet, es una unidad de optimización que causa inestabilidad en los servicios de llamadas y datos. Se mide en porcentaje y la perdida de servicios deberia estar en 0% o en valores muy cercanos a 0%, si se ve aun aumento súbito que sea constante es un problema, no sé tiene un porcentaje de umbral mano, sino se basa en la tendencia del KPI. Los valores máximos pueden ayudar a determinar problemas de optimización en la red móvil. La distribución muestra un fuerte sesgo positivo con valores máximos de 100% para algunos usuarios. Los SMEs indican que valores por encima de 1.5% ya son motivo de preocupación.\n\n\n\n\n\n\n\ndis_mean: Al igual que con la distribución de dis_max, la distribución del los valores promedios podrían ser relevantes para determinar problemas de disponibilidad. La distribución es plausible y el sesgo indica un rango de valores amplios debido a la presencia de atípicos en algunos usuarios.\ntad_p95: La distribución del timing advanced para el percentil 95 tiene un sesgo positivo con algunos valores atípicos mayores al promedio.\n\n\n\n8.4.4 Variables categóricas\nLas variables categóricas fueron removidas por los SMEs y por el product owner en vista de que serían variables que probablemente no vendrían en el dataset en producción. En su lugar se tomó en cuenta la latitud y longitud del sitio."
  },
  {
    "objectID": "analisis-exploratorio.html#análisis-bivariado",
    "href": "analisis-exploratorio.html#análisis-bivariado",
    "title": "8  Análisis Exploratorio",
    "section": "8.5 Análisis bivariado",
    "text": "8.5 Análisis bivariado\nAunque los boxplots ofrecen una visión más completa de la distribución de los datos, no proporcionan el mismo nivel de detalle sobre la incertidumbre en torno a la media que los gráficos de confianza o el MI. Usar tanto MI, como gráficos de confianza y boxplot, proporcionará una visión más completa de los datos. Los gráficos de confianza dan detalles sobre la media y su incertidumbre, mientras que los boxplots muestran la distribución completa.\n\n8.5.1 Dependiente vs numéricas\n\n8.5.1.1 Intervalos\nDebido al desbalance de clases presente en la variable respuesta, será necesario reflejar claramente el grado de incertidumbre que esto ocasiona. Si una clase tiene más observaciones que otra entonces la comparación no sería justa y la variabilidad podría tener que ver con esto más que con la variabilidad inherente que intentamos capturar. Para hacer esto, primero recordemos que una estimación de intervalo describe un rango de valores dentro del cual es posible que esté un parámetro de la población y un intervalo de confianza es la probabilidad que asociamos con una estimación de intervalo (Levin and Rubin 1998).\nEn este sentido, los gráficos de confianza son esenciales para entender no solo la relación de la variable categórica diag con las métricas numéricas, sino también para capturar el grado de incertidumbre asociado con estas métricas. Estos gráficos nos permiten ir más allá de las simples medias y observar cómo la variabilidad en los datos afecta nuestras conclusiones.\nLos gráficos de confianza nos permiten ver cómo cada métrica, se relaciona directamente con las categorías de diag. Por ejemplo, si observamos que los intervalos de confianza para thp_required_lte son significativamente bajos en la categoría “CAPACIDAD”, podemos inferir con cierto grado de confianza que un thp_required_lte bajo generalmente indica un problema de capacidad.\nEl intervalo da una idea de dónde podrían caer las verdaderas medias de thp_required_lte para cada categoría si tuvieras acceso a toda la población, en lugar de solo una muestra.\nEn el análisis univariado de la sección Section 8.4.3, así como en el resumen estadístico Section 8.3 observamos que las distribuciones se encuentran con mucho sesgo.\nEn el análisis exploratorio de datos (EDA), la presencia de valores atípicos puede distorsionar significativamente las interpretaciones y conclusiones. Los valores atípicos pueden tener un impacto desproporcionado en la media y la varianza, lo que a su vez afectará las visualizaciones y las medidas de tendencia central y dispersión. En este contexto, la transformación de variables se convierte en una herramienta invaluable para mitigar estos efectos y permitir un análisis más robusto.\nEl paquete bestNormalize en R (Ryan Andrew Peterson 2023) es especialmente útil para este propósito. Según su documentación (Ryan A. Peterson 2021), el paquete intenta encontrar la mejor transformación para normalizar una variable numérica. Utiliza una variedad de transformaciones, como transformación de Box-Cox, transformación de Yeo-Johnson, transformación de logaritmo, entre otras, y selecciona la que minimiza algún criterio de ajuste, como el error cuadrático medio (MSE) o el estadístico de Shapiro-Wilk para la normalidad.\nUna de las ventajas de utilizar bestNormalize es que realiza una búsqueda exhaustiva a través de múltiples métodos y selecciona la transformación más óptima para cada característica. Esto es especialmente útil cuando se trabaja con conjuntos de datos con múltiples características que pueden requerir diferentes tipos de transformaciones.\nAl aplicar la transformación más adecuada, las visualizaciones de los datos transformados ofrecen una representación más precisa de las relaciones subyacentes entre las variables. Esto es especialmente útil para técnicas que asumen la normalidad de los datos. Además, al reducir el impacto de los valores atípicos, las transformaciones permiten que las métricas como la media y la varianza sean más representativas del conjunto de datos, lo que facilita la interpretación y el análisis posterior.\nEn resumen, la transformación de variables utilizando bestNormalize mejora la calidad del análisis exploratorio al hacer que las visualizaciones y las métricas sean más robustas y representativas, permitiendo así una mejor comprensión de la estructura subyacente de los datos.\n\n# Función para calcular la media y el intervalo de confianza\nmean_ci &lt;- function(df, metrica, conf.level = 0.95) {\n x  &lt;- df[[metrica]][!is.na(df[[metrica]])]\n m  &lt;- mean(x)\n ci &lt;- t.test(x)$conf.int\n data.frame(media = m, lower = ci[1], upper = ci[2])\n}\n\n\n# Seleccionar algunos predictores con un information gain alto\nbest_mi &lt;- c(\"lod_mim\", \"tad_max\", \"prb_mim\", \"thp_mim\")\n\n\nctl_normalizados &lt;- ctl_train |&gt;\n select(diag, where(is.numeric), -user) |&gt; \n relocate(diag) |&gt; \n recipe(diag ~ ., data = _) |&gt; \n step_select(all_of(best_mi), diag) |&gt;\n step_zv(all_predictors()) |&gt; \n step_nzv(all_predictors(), unique_cut = 2) |&gt; \n step_corr(threshold = 0.77) |&gt; \n step_orderNorm(all_predictors()) |&gt; \n ver()\n\n\nctl_norm &lt;- ctl_normalizados |&gt; \n select(lod_mim, tad_max, prb_mim, thp_mim, diag)\n\n\n# Función para crear un dataframe con los intervalos por cada métrica\ncalcular_ci &lt;- function(kpi) {\n ctl_normalizados |&gt; \n group_by(diag) %&gt;%\n do(mean_ci(., kpi)) %&gt;%\n ungroup() %&gt;%\n arrange(media)\n}\n\n\n# Guardar las métricas que vamos a comparar\nnm &lt;- names(ctl_normalizados)[-length(ctl_normalizados)]\n\n\n# Crear lista de dataframes con intervalos de confianza\nlista_ci &lt;- nm |&gt; \n map(~ calcular_ci(kpi = .x)) |&gt; \n set_names(nm)\n\n\n# Crear una lista con las gráficas de cada métrica\nanalisis_bivariado &lt;- lista_ci |&gt; \n map2(.y = nm, ~ ggplot(.x, aes(x = diag, y = media)) +\n geom_point() +\n geom_errorbar(aes(ymin = lower, ymax = upper), width = .1) +\n theme(axis.text = element_text(size = 8)) +\n xlab(\"Diagnóstico\") +\n labs(title = .y) +\n drako)\n\nAplicar la transformación antes de realizar los gráficos de confianza puede ser beneficioso por las siguientes razones:\n\nRobustez frente a Atípicos: Como se mencionó anteriormente, los valores atípicos pueden tener un impacto significativo en la media. Al transformar los datos, se reduce la influencia de estos puntos extremos, lo que resulta en intervalos de confianza más robustos.\nMejora de la Interpretación: Los datos transformados pueden hacer que las diferencias o similitudes entre grupos sean más claras y fáciles de interpretar en los gráficos de confianza.\nNormalidad Aproximada: Muchas técnicas estadísticas, incluida la estimación de intervalos de confianza, asumen que los datos son aproximadamente normales. Las transformaciones pueden ayudar a cumplir con este supuesto.\nConsistencia en el Análisis: Si ya se han transformado las variables para otros aspectos del análisis exploratorio de datos, mantener esa transformación para los gráficos de confianza asegura que se están evaluando las mismas distribuciones a lo largo de todo el análisis.\nClaridad en la Presentación: Los gráficos de datos transformados pueden ser más fáciles de entender y comunicar, especialmente si reducen la asimetría o la escala de los datos originales.\n\n\n\n\n\n\nFigure 8.4: Porcentaje de tiempo que mantiene el throughput requerido en LTE\n\n\n\n\nTal como se ilustra en la figura Figure 8.4 el intervalo de confianza para la categoría “CAPACIDAD” es relativamente pequeño, lo que sugiere que hay menos incertidumbre en la estimación de la media para esta categoría. Esto podría significar que lod_mim es una predictor fuerte para la categoría de capacidad. Es decir, este predictor es importante para ayudar a distinguir la categoría “capacidad” de las demás.\n\n\n\n\n\nFigure 8.5: Porcentaje de tiempo que el usuario estuvo conectado a la celda.\n\n\n\n\nLos resultados observados en la Figure 8.5 son consistentes con las expectativas. Vemos que la categoría “COBERTURA” se encuentra bien explicada por valores altos en la variable tad_max, lo cual es congruente con la naturaleza de esta categoría.\n\n\n\n\n\nFigure 8.6: Porcentaje de bloques físicos utilizados para transmitir datos desde la RBS hacia los terminales. Una alta tasa de utilización de PRB podría generar problemas de capacidad.\n\n\n\n\nEsta métrica es utilizada principalmente por el equipo CTL para determinar problemas de capacidad en la red LTE. Vemos en la Figure 8.6 que cuando hay valores intermedios de rate-prb-dl podrían presentarse categoría de disponibilidad. No tenemos una explicación sobre esta relación, por lo que se investigará con los SMEs.\n\n\n\n\n\nFigure 8.7: Eficiencia en el establecimiento de una conexión RRC\n\n\n\n\nDado que thp_mim es un indicador de rendimiento que mide la velocidad de descarga por Unidad de Equipo de Usuario en una red celular, generalmente en Mbps, un valor muy alto (en este sentido sería thp bajo en downlink ya que es thp mínimo) podría apuntar a problemas en la disponibilidad. Los microcortes podrían ocasionar un thp bajo y por lo tanto indisponibilidad.\n\n\n8.5.1.2 Densidad\nEs importante realizar una comparación bivariada de las distribuciones y comparar su forma (simetría y sesgo) entre cada una de las categorías.\n\nctl_pivoted &lt;- ctl_normalizados |&gt;\n pivot_longer(\n  cols      = lod_mim:thp_mim,\n  names_to  = \"metric\", \n  values_to = \"value\") |&gt; \n filter(!is.na(value))\n\n\nlista_densidad &lt;- nm |&gt; \n map(~ ctl_normalizados |&gt; \n  ggplot(aes(x  = .data[[.x]], y = diag, fill = after_stat(x))) +\n  geom_density_ridges_gradient(\n   scale = 5, \n   rel_min_height = 0.01,\n   bandwidth = 0.248\n  ) +\n  scale_fill_viridis_c(name = \"Score\", option = \"C\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  coord_cartesian(clip = \"off\") +\n  labs(title = .x) +\n  theme_ridges(font_size = 18, font_family = \"yano\", grid = TRUE) +\n  theme(\n   axis.text.y  = element_text(size = 24),\n   axis.text.x  = element_text(size = 24),\n   axis.title.y = element_blank(),\n   axis.title.x = element_blank(),\n   plot.title   = element_text(size = 50))\n )\n\n\n\n\n\n\nEn este caso es evidente que la distribución de tipo “leptocúrtica” para el caso de optimización de lod_mim, así como la distancia de su media con el del resto, sugiere que hay muchos ejemplos con valores bajos de cell_load que ayudan a determinar problemas de este tipo.\n\n\n\n\n\nCobertura y optimización parecen tener valores más grandes de tad_max\n\n\n\n\n\nLos valores de rate_prb muy bajos podrían implicar problemas de optimización en una buena cantidad de casos.\n\n\n\n\n\nValores bajos de throupt_dl (thp_mim) pareciera que apuntan a situaciones donde hay problemas de optimización.\n\n\n8.5.1.3 Boxplots\nLos boxplots pueden agregar información adicional que no es posible ver solo con los histogramas, las densidades o incluso los gráficos de confianza. En este caso nos interesa analizar el ancho de la distribución, así como la cantidad de puntos que están generando la distribución.\n\nlista_boxplots &lt;- nm |&gt; \n map(~ ctl_normalizados |&gt; \n  drop_na() |&gt; \n  ggplot(aes(x  = .data[[.x]], y = diag)) +\n  geom_boxplot(aes(fill = diag),\n               outlier.colour = \"red\",\n               outlier.shape = 16,\n               outlier.size = 2,\n               notch = F)  +\n   stat_summary(fun = mean, \n               colour = \"darkred\",\n               geom = \"point\", \n               shape = 4, \n               size = 3) +\n  # geom_jitter(width = 0.2, alpha = .1) +\n  labs(title = .x) +\n  drako +\n  theme(legend.position = \"none\",\n        axis.text.y = element_text(size = 25),\n        axis.title.y = element_blank()) \n )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe manera similar a como se ha analizado anteriormente, los predictores más informativos escogidos a través de information gain parecieran tener algún tipo de efecto principal sobre cada una de las categorías. La presencia de valores atípicos es relevante y debe tomarse en cuenta en las transformaciones en la fase de preprocesamiento.\nPara tratar los valores atípicos podemos considerar el aplicar una transformación llamada spatial sign (ver Kuhn and Johnson 2020) y (Kuhn and Johnson 2013) o bien usar modelos robustos a valores atípicos.\n\n\n\n8.5.2 Interacciones\nSegún los SMEs entrevistados, la categoría optimización por lo general depende de un par de variables, es decir, que esta categoría podría ser el resultado de interacciones entre variables. Para la gráfica utilizamos el paquete effects (Fox 2003). Las métricas que principalmente se utilizan para determinar esto son:\n\nctl_interact &lt;- ctl_train |&gt;  \n  select(all_of(best_mi), diag) |&gt; \n  filter(diag %in% c(\"OPTIMIZACION\", \"CAPACIDAD\", \"COBERTURA\")) |&gt; \n  mutate(across(diag, as_factor),\n         across(diag, fct_drop))\n\n\nmodelo_multinom &lt;- multinom(\n diag ~ prb_mim * thp_mim, data = ctl_interact) |&gt; \n suppressMessages()\n\n# weights:  15 (8 variable)\ninitial  value 2199.421802 \niter  10 value 1327.161678\niter  20 value 1286.923560\nfinal  value 1286.851485 \nconverged\n\n\n\nn1 &lt;-  seq(min(ctl_interact$prb_mim), \n           max(ctl_interact$prb_mim), \n           length.out = 4) |&gt; round(2)\n\nn2 &lt;- seq(min(ctl_interact$thp_mim), \n                          max(ctl_interact$thp_mim), \n                          length.out = 4) |&gt; round(2)\n\n\n# Visualizar las interacciones\nefectos_prediccion &lt;- predictorEffects(\n modelo_multinom, ~ prb_mim + thp_mim,\n xlevels = list(\n   prb_mim = n1,\n   thp_mim =  n2))\n\n\nplot(\n efectos_prediccion,\n axes = list(grid = TRUE, x = list(rug = FALSE,\n prb_mim = list(ticks = list(at = n1)),\n thp_mim  = list(ticks = list(at = n2))),\n                 y = list(style = \"stacked\")),\n lines = list(col = c(\"blue\", \"red\", \"orange\", \"brown\", \"black\")),\n lattice = list(key.args = list(columns = 1),\n                strip = list(factor.names = FALSE)))\n\n\n\n\nFigure 8.8: Interacciones\n\n\n\n\nEn la primera gráfica de la izquierda, en el eje superior (los strips) mostrados como 0, 4.02, 9.04 y 12.05 son los cuatro valores en que se partió el thp_mim y en el eje x en la parte inferior están los valores de prb_mim.\nEntonces, si el thp_mim es cero, básicamente no importa el nivel del prb_mim (cualquier valor entre 0 y 87.39), la probabilidad de que sea un problema de capacidad es muy alta.\n\n\n8.5.3 Análisis de correlación\nSe debe realizar la inspección de las correlaciones que se encuentren arriba de 0.75 en valor absoluto. Este umbral es sugerido por algunos autores (Kuhn and Johnson 2013, pag 87).\nDebido a la alta dimensionalidad del dataset, seleccionaremos de manera aleatoria un conjunto de features para evaluar el nivel de correlación encontrado.\n\nanalize_corr &lt;- ctl_train |&gt;\n select(diag, where(is.numeric), -user) |&gt; \n relocate(diag) |&gt; \n recipe(diag ~ ., data = _) |&gt; \n step_zv(all_predictors()) |&gt; \n step_nzv(all_predictors(), unique_cut = 2) |&gt; \n ver() |&gt; \n select(sample.int(n = ncol(analize_corr), size = 11, replace = FALSE)) \n\n\ncorm &lt;- analize_corr |&gt;\n correlate(\n  use    = \"pairwise.complete.obs\", \n  method = \"pearson\", \n  quiet  = TRUE)\n\n\ncorm |&gt; \n shave() |&gt; \n rplot(print_cor = TRUE) +\n theme(\n  axis.text.y = element_text(size = 8),\n  axis.text.x = element_text(size = 8, angle = 90, hjust = -0.3, vjust = 0.4))\n\n\n\n\nFigure 8.9: Análisis de Correlación entre predictores\n\n\n\n\n\n\n8.5.4 K-means\nEl análisis de clústeres, y en particular el método k-means, es una técnica poderosa para evaluar la relevancia de las variables seleccionadas mediante el cálculo de la información mutua o ganancia de información. Este método nos ayuda a comprender si las variables seleccionadas tienen un fuerte efecto principal, es decir, si cada variable por sí sola tiene un impacto significativo en la formación de los grupos, sin necesariamente depender de interacciones complejas con otras variables.\nSi al aplicar k-means encontramos que los clústeres se forman de manera coherente y significativa alrededor de estas variables, podemos inferir que son predictores fuertes y que su inclusión en un modelo predictivo podría mejorar su rendimiento. Esto es especialmente útil cuando queremos incorporar variables de manera gradual en un enfoque de selección de características paso a paso (stepwise feature selection). En este proceso, añadimos o eliminamos predictores basándonos en criterios estadísticos, buscando optimizar algún criterio de selección como el AIC (Criterio de Información de Akaike) o el BIC (Criterio de Información Bayesiano).\nPor lo tanto, si las variables seleccionadas demuestran ser relevantes en el análisis de clústeres con k-means, esto nos proporciona una validación adicional de su importancia y nos da la confianza para incluirlas en el modelo. Esto nos permite realizar una selección de características más informada y metódica, asegurándonos de que cada variable que incluimos contribuye significativamente a la capacidad predictiva del modelo.\nEn resumen, el uso de k-means en conjunto con la ganancia de información nos ofrece una estrategia robusta para identificar las variables más prometedoras para nuestros modelos predictivos, permitiéndonos construir modelos más precisos y eficientes.\nEl primer paso es cargar algunas librerías de apoyo para realizar el análisis.\n\nimport::from(clustertend, hopkins)\nimport::from(factoextra, fviz_nbclust, fviz_cluster)\nimport::from(NbClust, NbClust)\nimport::from(clValid, clValid, optimalScores)\n\nElegiremos únicamente aquellas variables que tienen más información predictiva y que podrían ayudar a realizar de manera correcta la separación de clases.\n\nctl_scaled &lt;- ctl_train |&gt; \n select(diag, all_of(predictores)) |&gt; \n recipe(diag ~ ., data = _) |&gt; \n step_zv(all_predictors()) |&gt; \n # step_nzv(all_predictors(), unique_cut = 2) |&gt; \n step_center(all_numeric()) |&gt; \n step_scale(all_numeric()) |&gt; \n step_smote(diag) |&gt; \n ver()\n\nRealizaremos una prueba con las las categorías que tienden a tener mayor separación en sus métricas.\n\nctl_sd &lt;- ctl_scaled |&gt; \n filter(diag %in% c(\"COBERTURA\", \"CAPACIDAD\", \"DISPONIBILIDAD\")) |&gt; \n select(-diag)\n\n\nctl_dis &lt;- dist(ctl_scaled)\n\nExploremos cual es el efecto de diferentes valores de k probando del 1 al 9. Primero agruparemos los datos 9 veces, en cada una utilizando diferentes tamaños de k, luego agregaremos las columnas que contienen los datos ordenados.\n\nkclusts &lt;- tibble(k = 1:9) |&gt; \n mutate(\n  kclust    = map(k, ~kmeans(ctl_sd, .x)),\n  tidied    = map(kclust, tidy),\n  glanced   = map(kclust, glance),\n  augmented = map(kclust, augment, ctl_sd)\n )\n\n\nkclusts\n\n# A tibble: 9 × 5\n      k kclust   tidied            glanced          augmented            \n  &lt;int&gt; &lt;list&gt;   &lt;list&gt;            &lt;list&gt;           &lt;list&gt;               \n1     1 &lt;kmeans&gt; &lt;tibble [1 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n2     2 &lt;kmeans&gt; &lt;tibble [2 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n3     3 &lt;kmeans&gt; &lt;tibble [3 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n4     4 &lt;kmeans&gt; &lt;tibble [4 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n5     5 &lt;kmeans&gt; &lt;tibble [5 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n6     6 &lt;kmeans&gt; &lt;tibble [6 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n7     7 &lt;kmeans&gt; &lt;tibble [7 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n8     8 &lt;kmeans&gt; &lt;tibble [8 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n9     9 &lt;kmeans&gt; &lt;tibble [9 × 13]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [3,114 × 11]&gt;\n\n\nPodemos convertir esto en tres distintos datasets cada uno representando distintos tipos de datos obtenidos con las funciones tidy(), augment() y glance().\n\nclusters &lt;- kclusts |&gt;\n  unnest(cols = c(tidied))\n\nassignments &lt;- kclusts |&gt; \n  unnest(cols = c(augmented))\n\nclusterings &lt;- kclusts |&gt;\n  unnest(cols = c(glanced))\n\nAhora podemos graficar los puntos originales utilizando la data de augment() con cada punto coloreado de acuerdo al cluster predicho\n\nassignments |&gt; \n ggplot(aes(x = tad_max, y = thp_max)) +\n geom_point(aes(color = .cluster), alpha = 0.8) + \n geom_point(data = clusters, size = 10, shape = \"x\") +\n facet_wrap(~ k)\n\n\n\n\nFigure 8.10: K-Means Clustering\n\n\n\n\nVisualmente observamos en la figura Figure 8.10 que la mejor separación de clases se logra con tres clusters.\nA continuación podemos utilizar la función fviz_nbclust() del paquete factoextra para evaluar por medio del método “silhouette” cuál es la cantidad óptima de clusters.\n\nfviz_nbclust(ctl_sd, kmeans, method = \"silhouette\")\n\n\n\n\nFigure 8.11: Cantidad óptimda de clusters\n\n\n\n\nEn la figura Figure 8.11 se comprueba que la cantidad óptima son tres grupos.\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n\n\n\nFigure 8.12: Cantidad óptimda de clusters\n\n\n\n\nNuevamente en la figura Figure 8.12 comprobamos por el método elbow que tres es el número adecuado de grupos.\nUna vez que hemos validado los grupos, podemos realizar el ajuste nuevamente con la cantidad de clusters para poder graficar adecuadamente.\n\nkclust &lt;- kmeans(ctl_sd, centers = 3)\n\n\nfviz_cluster(kclust, ctl_sd, ellipse.type = \"norm\") +\n yunkel\n\n\n\n\nFigure 8.13: Separación óptima de clases\n\n\n\n\nVemos que, dejando de lado algunas excepciones, las categorías presentan una separación apreciable, sin embargo, hay un cierto solapamiento en las métricas de algunas clases.\n\n\n8.5.5 PCA\nEn la sección de análisis de correlación (Section 8.5.3), notamos una alta correlación entre muchas de las estadísticas descriptivas utilizadas para colapsar el dataset. Esto podría deberse a varias razones. Por ejemplo, las métricas de rendimiento en telecomunicaciones a menudo están interconectadas; un cambio en una métrica puede influir directamente en otra, como la relación entre la calidad de la señal y la tasa de error de bits. Además, la naturaleza de los datos recopilados en intervalos regulares puede llevar a redundancias, donde las medidas repetidas reflejan la misma variabilidad subyacente.\nEl PCA, como técnica de extracción de características, es particularmente útil en este contexto de alta multicolinealidad. Al transformar las variables originales en un nuevo conjunto de variables ortogonales (las componentes principales), PCA nos permite capturar la mayor parte de la variabilidad en los datos con menos predictores. Esto no solo ayuda a aliviar los problemas asociados con predictores correlacionados sino que también simplifica el modelo al reducir la cantidad de variables, lo cual es crucial para evitar el sobreajuste y mejorar la interpretación del modelo.\nAdemás, al aplicar PCA en la fase de preprocesamiento, podemos evaluar la idoneidad de esta técnica para reducir la dimensionalidad antes de proceder al modelado predictivo. Si las primeras componentes principales capturan una cantidad significativa de la variabilidad total, esto indica que PCA podría ser una estrategia efectiva para condensar la información sin perder insights críticos. Esto es especialmente relevante en nuestro proyecto CTL, donde la eficiencia y la claridad en la interpretación de los modelos son de suma importancia para la toma de decisiones basada en datos.\n\nctl_validacion  &lt;- validation(ctl_split)\n\n\nctl_num &lt;- ctl_train |&gt; select(diag, where(is.numeric), -user)\nctl_rec &lt;- recipe(diag ~ ., data = ctl_num) |&gt;\n step_zv(all_numeric_predictors()) |&gt;\n step_orderNorm(all_numeric_predictors()) |&gt;\n step_normalize(all_numeric_predictors())\n\n\nctl_trained &lt;- prep(ctl_rec)\n\n\nplot_validation_results &lt;- function(recipe, dat = ctl_validacion) {\n recipe |&gt; \n prep() |&gt; \n bake(new_data = dat) |&gt; \n ggplot(aes(x = .panel_x, y = .panel_y, color = diag, fill = diag)) +\n geom_point(alpha = 0.8, size = 1) +\n geom_autodensity(alpha = .3) +\n facet_matrix(vars(-diag), layer.diag = 2) +\n scale_color_brewer(palette = \"Dark2\") +\n scale_fill_brewer(palette = \"Dark2\") +\n drako\n}\n\n\nctl_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"Principal Component Analysis\") +\n  theme(legend.position = \"bottom\", \n        legend.text = element_text(size = 25),\n        strip.text = element_text(size = 25))\n\n\n\n\nFigure 8.14: Análisis de Componentes Principales\n\n\n\n\nLa visualización resultante del PCA, presentada en la Figura Figure 8.14, no revela una distinción nítida entre las diferentes clases de diagnóstico. Los dos primeros componentes principales, PC1 y PC2, no logran capturar una separación definida entre las categorías. Esto puede ser indicativo de que las variables seleccionadas están influidas por una variabilidad común que no distingue adecuadamente entre las clases, o que las interacciones entre variables son complejas y no lineales, lo que el PCA, siendo un método lineal, no puede determinar.\nEs interesante notar que la clase ‘disponibilidad’ se distingue ligeramente de las demás, lo cual es coherente con la naturaleza de sus métricas, que tienden a tener un rango de valores más restringido y específico. Esto sugiere que las métricas de ‘disponibilidad’ podrían estar operando en un espacio de características distintas o estar menos correlacionadas con las métricas de otras clases.\nLa superposición observada en las clases podría también reflejar la presencia de patrones subyacentes complejos que requieren métodos más sofisticados para su identificación y separación. Alternativamente, podría ser un indicativo de que se necesitan más componentes para capturar la variabilidad relevante para la diferenciación de clases, o que otras técnicas de reducción de dimensionalidad, como el t-SNE o el UMAP, que son capaces de capturar estructuras no lineales, podrían ser más adecuadas para este conjunto de datos.\nEn resumen, los resultados del PCA nos instan a considerar la posibilidad de explorar otras técnicas de preprocesamiento y modelado que puedan manejar la complejidad inherente a nuestros datos y, por ende, mejorar la separación de clases para fines de diagnóstico en el proyecto CTL\n\n\n\n\nDuboue, Pablo. 2020. The Art of Feature Engineering: Essentials for Machine Learning. First edition. Cambridge ; New York, NY: Cambridge University Press.\n\n\nFox, John. 2003. “Effect Displays in R for Generalised Linear Models.” Journal of Statistical Software 8 (15): 1–27. https://doi.org/10.18637/jss.v008.i15.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive Modeling. New York: Springer.\n\n\n———. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. Boca Raton London New York: CRC Press, Taylor & Francis Group.\n\n\nLevin, Richard I., and David S. Rubin. 1998. Statistics for Management. 7th ed. Upper Saddle River, N.J: Prentice Hall.\n\n\nPeterson, Ryan A. 2021. “Finding Optimal Normalizing Transformations via bestNormalize.” The R Journal 13 (1): 310–29. https://doi.org/10.32614/RJ-2021-041.\n\n\nPeterson, Ryan Andrew. 2023. bestNormalize: Normalizing Transformation Functions. https://petersonr.github.io/bestNormalize/."
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Siguiendo el framework establecido en la sección Section 4.2, el siguiente paso es construir el flujo de trabajo. En este apartado es importante notar las diferencias con otros frameworks, como por ejemplo los pipelines en Python.\nTomemos como ejemplo la aplicación de PCA. En la figura Figure 1 vemos que PCA se considera un paso externo al workflow.\n\n\n\nFigure 1: Modelo Incorrecto\n\n\nSin embargo, en este framework, la aplicación de PCA si forma parte del workflow a como se ve en la figura Figure 2.\n\n\n\nFigure 2: Modelo Correcto\n\n\nEn el apartado del modelado veremos como esta integración contrasta con la posición en que se colocó en el diagrama de la figura Figure 4.2.\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn la Figura Figure 4.2, la etapa de preprocesamiento se ha integrado dentro del módulo de Data Preparation. Aunque esto podría parecer inusual, se tomó esta decisión siguiendo la recomendación de nuestro asesor, quien sugirió que se alinee con la comprensión común y las prácticas establecidas por la mayoría de los lectores potenciales. Esta convención es común en otros entornos y frameworks, como los pipelines de scikit-learn, donde el preprocesamiento a menudo se conceptualiza y se ejecuta como una operación separada y preliminar, tal como se ilustra en la Figura Figure 1. Nuestro enfoque busca armonizar con estas normas establecidas para facilitar la comprensión y la adopción de nuestro flujo de trabajo propuesto"
  },
  {
    "objectID": "validacion-cruzada.html",
    "href": "validacion-cruzada.html",
    "title": "9  Validación Cruzada",
    "section": "",
    "text": "La validación cruzada es una técnica esencial en nuestro proyecto, especialmente debido a la presencia de clases desbalanceadas en nuestro conjunto de datos. Al utilizar la estratificación en un esquema de validación cruzada de 10 folds, garantizamos que cada pliegue mantenga la proporción original de las clases, lo cual es crucial para obtener una evaluación precisa del rendimiento del modelo.\nEn contextos donde las clases están desbalanceadas, como en nuestro caso, modelos entrenados sin estratificación podrían sesgarse hacia las clases mayoritarias, ignorando efectivamente las minoritarias, que a menudo son de mayor interés. Esto podría llevar a un rendimiento aparentemente alto cuando se mide por métricas generales, pero pobre en lo que respecta a la detección de las clases menos representadas.\nAl aplicar la validación cruzada con estratificación, cada conjunto de entrenamiento y validación refleja la distribución de las clases del conjunto de datos completo. Esto no solo mejora la robustez y la generalización del modelo, sino que también asegura que nuestras métricas de rendimiento sean más representativas y confiables. Con 10 folds, aumentamos aún más la confiabilidad de nuestras estimaciones de rendimiento, ya que el modelo se evalúa en una variedad más amplia de subconjuntos de datos.\n\nset.seed(2023)\nctl_folds &lt;- vfold_cv(ctl_train, strata = diag, v = 10)\n\n\nctl_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [3055/341]&gt; Fold01\n 2 &lt;split [3055/341]&gt; Fold02\n 3 &lt;split [3055/341]&gt; Fold03\n 4 &lt;split [3055/341]&gt; Fold04\n 5 &lt;split [3056/340]&gt; Fold05\n 6 &lt;split [3056/340]&gt; Fold06\n 7 &lt;split [3057/339]&gt; Fold07\n 8 &lt;split [3057/339]&gt; Fold08\n 9 &lt;split [3059/337]&gt; Fold09\n10 &lt;split [3059/337]&gt; Fold10"
  },
  {
    "objectID": "modelado.html#métricas",
    "href": "modelado.html#métricas",
    "title": "10  Modelado",
    "section": "10.1 Métricas",
    "text": "10.1 Métricas\nPrimeramente definiremos las métricas a utilizar. Estas se determinaron en la fase de EDA con base a la distribución de la variable respuesta y en especial del hecho de que no está balanceada.\n\nPrecisión: Esta métrica es importante cuando los costos de los falsos positivos son altos. En el contexto de nuestro proyecto, queremos asegurarnos de que las predicciones de las categorías de diagnóstico sean correctas y no queremos alarmar innecesariamente sobre posibles problemas que no existen.\nRecall (Sensibilidad): El recall es crucial cuando es esencial detectar todos los casos positivos. Dado que nuestro proyecto podría estar relacionado con el mantenimiento predictivo, no queremos pasar por alto ninguna instancia que realmente pertenezca a una categoría de diagnóstico crítica.\nF1-Score Macro: Esta métrica combina la precisión y el recall en un solo número, proporcionando un equilibrio entre ambos. Al usar el promedio macro, tratamos todas las clases por igual, dando el mismo peso a cada una, lo cual es importante en nuestro conjunto de datos desbalanceados. Esto asegura que no ignoramos el rendimiento en las clases minoritarias, que a menudo son las más importantes en aplicaciones de diagnóstico.\nROC_AUC: La curva ROC y el área bajo la curva (AUC) proporcionan una medida agregada del rendimiento en todos los umbrales de clasificación. Esto es particularmente útil cuando las clases son desbalanceadas y los costos de los falsos positivos y falsos negativos varían. El ROC_AUC es una medida de la capacidad del modelo para discriminar entre las clases positivas y negativas, y un valor alto indica que el modelo tiene una buena medida de separabilidad.\n\n\nmset &lt;- metric_set(precision, recall, f_meas, roc_auc)"
  },
  {
    "objectID": "modelado.html#control-de-hiperparámetros",
    "href": "modelado.html#control-de-hiperparámetros",
    "title": "10  Modelado",
    "section": "10.2 Control de Hiperparámetros",
    "text": "10.2 Control de Hiperparámetros\nDebemos establecer la configuración de la búsqueda de hiperparámtros. Para este fin utilizaremos un método especial, el cual es más eficiente que el método tradicional.\n\nrace_ctrl &lt;- control_race(\n save_pred     = TRUE,\n parallel_over = \"everything\",\n verbose       = TRUE,\n verbose_elim  = TRUE,\n save_workflow = FALSE)"
  },
  {
    "objectID": "modelado.html#algoritmos",
    "href": "modelado.html#algoritmos",
    "title": "10  Modelado",
    "section": "10.3 Algoritmos",
    "text": "10.3 Algoritmos\nUna de las fortalezas del framework Tidymodels es su capacidad para facilitar la evaluación simultánea de múltiples algoritmos y diversas estrategias de preprocesamiento. Esta flexibilidad es sumamente valiosa, ya que nos permite explorar una extensa gama de combinaciones en un único proceso de modelado. Idealmente, las recetas de preprocesamiento deberían ser personalizadas para cada algoritmo específico, optimizando así su rendimiento. No obstante, en la práctica, y siguiendo el principio del ‘no free lunch theorem’, es recomendable realizar pruebas exhaustivas y ajustes iterativos. Esto se debe a que no existe un único modelo o método que sea el mejor para todos los problemas y conjuntos de datos; la efectividad puede variar según la naturaleza específica de cada tarea de modelado.\n\nrand_forest_ranger &lt;- rand_forest(\n mtry  = tune(),\n min_n = tune()) |&gt;\nset_engine('ranger', importance = \"permutation\") |&gt;\nset_mode('classification')\n\nbt_lightgbm &lt;- boost_tree(\n tree_depth = 2,\n learn_rate = 0.001,\n trees = tune(),\n min_n = 20) |&gt;\nset_engine(engine = \"lightgbm\") |&gt;\nset_mode(mode = \"classification\")\n\nsvm_linear_kernlab &lt;- svm_linear(\n cost = tune(),\n margin = tune()) |&gt;\nset_engine('kernlab') |&gt;\nset_mode('classification')\n\nglmnet_spec &lt;- multinom_reg(\n penalty = tune(),\n mixture = tune()) |&gt;\nset_engine('glmnet') |&gt;\nset_mode('classification')\n\nxgboost_spec &lt;- boost_tree(\n tree_depth     = 3,           \n trees          = tune(),\n learn_rate     = tune(),\n min_n          = tune(),\n loss_reduction = tune(),\n sample_size    = tune(),\n stop_iter      = tune()) |&gt;\nset_engine('xgboost') |&gt;\nset_mode('classification')\n\ndecision_tree_partykit &lt;- decision_tree() |&gt;\nset_engine(engine = \"partykit\") |&gt;\nset_mode(mode = \"classification\")\n\nnearest_neighbor_kknn_spec &lt;- nearest_neighbor(\n neighbors = tune(), \n weight_func = tune(), \n dist_power = tune()) |&gt;\nset_engine('kknn') |&gt;\nset_mode('classification')\n\nEs importante notar que el placeholder tune() sirve para que ahí se evalué una cuadrícula de hiperparámetros.\n\n# lista de modelos\nmodelos &lt;- list(\n rand_forest_ranger     = rand_forest_ranger,\n bt_lightgbm            = bt_lightgbm,\n svm_linear_kernlab     = svm_linear_kernlab,\n glmnet                 = glmnet_spec,\n xgboost                = xgboost_spec,\n decision_tree_partykit = decision_tree_partykit,\n knn                    = nearest_neighbor_kknn_spec)"
  },
  {
    "objectID": "modelado.html#preprocesamiento",
    "href": "modelado.html#preprocesamiento",
    "title": "10  Modelado",
    "section": "10.4 Preprocesamiento",
    "text": "10.4 Preprocesamiento\nCrearemos 6 recetas de preprocesamiento, cada una con distintos pasos y filtros. En esta parte se aplican distintas técnicas de feature selection y de feature scaling.\n\nboruta_regular &lt;-  recipe(diag ~ ., data = ctl_train) |&gt;\n update_role(user, new_role = \"id\") |&gt; \n step_select_boruta(all_predictors(), outcome = \"diag\") |&gt; \n step_smote(diag, skip = TRUE)\n \ninfgain_regular_7 &lt;- recipe(diag ~ ., data = ctl_train) |&gt;\n update_role(user, new_role = \"id\") |&gt;\n step_select_infgain(\n  all_predictors(),\n   outcome = \"diag\",\n   threshold = 0.7) |&gt;\n step_smote(diag, skip = TRUE)\n\ninfgain_regular_9 &lt;- recipe(diag ~ ., data = ctl_train) |&gt;\n update_role(user, new_role = \"id\") |&gt;\n step_select_infgain(\n  all_predictors(),\n   outcome = \"diag\",\n   threshold = 0.7) |&gt;\n step_smote(diag, skip = TRUE)\n\ninfgain_regular_7_nzv &lt;- recipe(diag ~ ., data = ctl_train) |&gt;\n update_role(user, new_role = \"id\") |&gt;\n step_zv(all_predictors()) |&gt; \n step_nzv(all_predictors()) |&gt; \n step_select_infgain(\n  all_predictors(),\n   outcome = \"diag\",\n   threshold = 0.7) |&gt;\n step_smote(diag, skip = TRUE)\n\nmrmr_regular_top20 &lt;- recipe(diag ~ ., data = ctl_train) |&gt;\n update_role(user, new_role = \"id\") |&gt;\n step_select_mrmr(\n  all_predictors(),\n   outcome = \"class\",\n   top_p = 20) |&gt; \n step_smote(diag, skip = TRUE)\n\ninfgain_norm_7 &lt;- recipe(diag ~ ., data = ctl_train) |&gt;\n update_role(user, new_role = \"id\") |&gt;\n step_orderNorm(all_numeric_predictors()) |&gt;\n step_normalize(all_numeric_predictors()) |&gt;\n step_select_infgain(\n  all_predictors(),\n   outcome = \"diag\",\n   threshold = 0.7) |&gt;\n step_smote(diag, skip = TRUE)\n\n\nrecetas &lt;- list(\n boruta_regular        = boruta_regular,\n infgain_regular_7     = infgain_regular_7,\n infgain_regular_9     = infgain_regular_9,\n infgain_regular_7_nzv = infgain_regular_7_nzv,\n mrmr_regular_top20    = mrmr_regular_top20,\n infgain_norm_7        = infgain_norm_7)"
  },
  {
    "objectID": "modelado.html#workflow-set",
    "href": "modelado.html#workflow-set",
    "title": "10  Modelado",
    "section": "10.5 Workflow Set",
    "text": "10.5 Workflow Set\nUna vez que hemos definido los motores (algoritmos) a probar junto con las distintas recetas de preprocesamiento, procederemos a juntar todo dentro de un workflow_set.\n\nctl_set &lt;- workflow_set(preproc = recetas, models  = modelos)\nctl_set |&gt; print(n = Inf)\n\n# A workflow set/tibble: 42 × 4\n   wflow_id                                     info     option    result    \n   &lt;chr&gt;                                        &lt;list&gt;   &lt;list&gt;    &lt;list&gt;    \n 1 boruta_regular_rand_forest_ranger            &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 2 boruta_regular_bt_lightgbm                   &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 3 boruta_regular_svm_linear_kernlab            &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 4 boruta_regular_glmnet                        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 5 boruta_regular_xgboost                       &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 6 boruta_regular_decision_tree_partykit        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 7 boruta_regular_knn                           &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 8 infgain_regular_7_rand_forest_ranger         &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n 9 infgain_regular_7_bt_lightgbm                &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n10 infgain_regular_7_svm_linear_kernlab         &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n11 infgain_regular_7_glmnet                     &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n12 infgain_regular_7_xgboost                    &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n13 infgain_regular_7_decision_tree_partykit     &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n14 infgain_regular_7_knn                        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n15 infgain_regular_9_rand_forest_ranger         &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n16 infgain_regular_9_bt_lightgbm                &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n17 infgain_regular_9_svm_linear_kernlab         &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n18 infgain_regular_9_glmnet                     &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n19 infgain_regular_9_xgboost                    &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n20 infgain_regular_9_decision_tree_partykit     &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n21 infgain_regular_9_knn                        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n22 infgain_regular_7_nzv_rand_forest_ranger     &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n23 infgain_regular_7_nzv_bt_lightgbm            &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n24 infgain_regular_7_nzv_svm_linear_kernlab     &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n25 infgain_regular_7_nzv_glmnet                 &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n26 infgain_regular_7_nzv_xgboost                &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n27 infgain_regular_7_nzv_decision_tree_partykit &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n28 infgain_regular_7_nzv_knn                    &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n29 mrmr_regular_top20_rand_forest_ranger        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n30 mrmr_regular_top20_bt_lightgbm               &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n31 mrmr_regular_top20_svm_linear_kernlab        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n32 mrmr_regular_top20_glmnet                    &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n33 mrmr_regular_top20_xgboost                   &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n34 mrmr_regular_top20_decision_tree_partykit    &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n35 mrmr_regular_top20_knn                       &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n36 infgain_norm_7_rand_forest_ranger            &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n37 infgain_norm_7_bt_lightgbm                   &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n38 infgain_norm_7_svm_linear_kernlab            &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n39 infgain_norm_7_glmnet                        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n40 infgain_norm_7_xgboost                       &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n41 infgain_norm_7_decision_tree_partykit        &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n42 infgain_norm_7_knn                           &lt;tibble&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nPodemos observar que todos los algoritmos se combinaron con todas las recetas de preprocesamiento. En total hay 42 combinaciones."
  },
  {
    "objectID": "modelado.html#ajustar",
    "href": "modelado.html#ajustar",
    "title": "10  Modelado",
    "section": "10.6 Ajustar",
    "text": "10.6 Ajustar\n\n1cl &lt;- makePSOCKcluster(10)\nregisterDoParallel(cl)\n\nfinal_tune &lt;- ctl_set |&gt;\n workflow_map(\n2 fn        = \"tune_race_anova\",\n3 verbose   = TRUE,\n4 resamples = ctl_folds,\n5 control   = race_ctrl,\n6 seed      = 2023,\n7 metrics   = mset,\n8 grid      = 20)\n\n9stopCluster(cl)\nunregister()\n\n\n1\n\nHabilitar el backend para procesamiento paralelo haciendo uso de 10 nucleos físicos del CPU.\n\n2\n\nUtilizar el método de tipo “race” el cual realiza un modelo ANOVA para probar la significancia estadística de las diferentes configuraciones del modelo (ver Racing Methods).\n\n3\n\nEstablecer que se muestre el avance en el ajuste.\n\n4\n\nUtilizar los pliegos definidos con validación cruzada.\n\n5\n\nUtilizar la configuración de búsqueda de hiperparámetros del método “race”.\n\n6\n\nDefinir semilla.\n\n7\n\nEstablecer métricas a calcular.\n\n8\n\nTamaño de la cuadricula de búsqueda de hiperparámetros.\n\n9\n\nDetener el backend de parelelización."
  },
  {
    "objectID": "resultados.html#rendimiento-en-entrenamiento",
    "href": "resultados.html#rendimiento-en-entrenamiento",
    "title": "11  Resultados",
    "section": "11.1 Rendimiento en entrenamiento",
    "text": "11.1 Rendimiento en entrenamiento\n\nmetricas_training &lt;- final_tune |&gt;\n rank_results(select_best = TRUE, rank_metric = \"f_meas\") |&gt;\n select(modelo = wflow_id, .metric, mean, rank) |&gt;\n pivot_wider(names_from = .metric, values_from = mean) |&gt;\n select(modelo, f1_tr = f_meas, prec_tr = precision,\n rec_tr = recall)\n\n\nfinal_tune |&gt;\n autoplot(rank_metric = \"f_meas\", metric = \"f_meas\", select_best = TRUE) +\n ylab(\"f1-score\") +\n xlab(\"Modelos\") +\n labs(title = \"Combinaciones de modelos y preprocesamiento\") +\n theme(\n plot.title  = element_text(size = 30),\n legend.position = \"bottom\",\n legend.text = element_text(size = 30),\n axis.title  = element_text(size = 25),\n axis.text   = element_text(size = 20))\n\n\n\n\nFigure 11.1: Rank Metrics\n\n\n\n\nEn la figura Figure 11.1 se observan los resultados del entrenamiento y una comparación con intervalos de confianza de cada modelo ajustado.\n\n\nMostrar Código\nmetricas_training |&gt; \n gt() |&gt; \n tab_header(\n    title = md(\"**Resultados con Conjunto de Entrenamiento**\"),\n    subtitle = md(\"Modelos sin sobreajuste\")\n  ) |&gt; \n gt_theme_538() |&gt; \n cols_label(\n    f1_tr = md(\"**F1-Score**\"),\n    prec_tr = md(\"**Precisión**\"),\n    rec_tr = md(\"**Recall**\"),\n  ) |&gt; \n data_color(\n    columns = where(~ is.numeric(.x)),\n    method = \"numeric\",\n    palette = \"RdYlGn\", \n    reverse = FALSE) |&gt; \n fmt_number(columns = where(~ is.numeric(.x)), decimals = 4) |&gt; \n cols_align_decimal() |&gt; \n cols_align(align = \"center\", columns = where(~ is.numeric(.x)))\n\n\n\n\n\nTable 11.1:  Resultados entrenamiento \n  \n    \n      Resultados con Conjunto de Entrenamiento\n    \n    \n      Modelos sin sobreajuste\n    \n    \n      modelo\n      F1-Score\n      Precisión\n      Recall\n    \n  \n  \n    boruta_regular_rand_forest_ranger\n0.9760\n0.9784\n0.9742\n    boruta_regular_bt_lightgbm\n0.9669\n0.9716\n0.9639\n    infgain_regular_7_bt_lightgbm\n0.9648\n0.9693\n0.9622\n    infgain_regular_9_bt_lightgbm\n0.9615\n0.9641\n0.9596\n    infgain_regular_7_svm_linear_kernlab\n0.9549\n0.9602\n0.9511\n    infgain_regular_7_glmnet\n0.9506\n0.9531\n0.9490\n    infgain_regular_7_xgboost\n0.9421\n0.9562\n0.9345\n    infgain_regular_9_xgboost\n0.9420\n0.9523\n0.9350\n    boruta_regular_xgboost\n0.9406\n0.9547\n0.9332\n    infgain_norm_7_xgboost\n0.9406\n0.9543\n0.9332\n    infgain_regular_7_decision_tree_partykit\n0.8940\n0.8508\n0.7853\n    mrmr_regular_top20_decision_tree_partykit\n0.8885\n0.8456\n0.7807\n    infgain_regular_7_nzv_xgboost\n0.8838\n0.8886\n0.8829\n    infgain_regular_7_nzv_bt_lightgbm\n0.8823\n0.8871\n0.8813\n    infgain_regular_7_nzv_svm_linear_kernlab\n0.8537\n0.8627\n0.8505\n    infgain_regular_7_nzv_glmnet\n0.8481\n0.8542\n0.8466\n    infgain_regular_7_nzv_knn\n0.8093\n0.8202\n0.8053\n    infgain_regular_7_nzv_decision_tree_partykit\n0.7527\n0.7207\n0.5824\n  \n  \n  \n\n\n\n\n\nPosterior a la revisión de los resultados con los datos de entrenamiento con validación cruzada, procedemos a evaluar los modelos contra el conjunto de validación.\nLo que haremos primero será extraer el nombre de todos los “workflows” y luego extraer los mejores modelos ajustados con validación cruzada. Por mejores se entiende que es la combinación de hiperparámetros que resultó en el f1-score más alto.\n\nbest &lt;- metricas_training |&gt; pull(modelo) %&gt;% set_names(.)\n\n# Seleccionar los modelos con los hiperparámetros que generaron el f1-score\n# más alto.\nlista_mejores &lt;- best |&gt;\n map(~ tune_res |&gt; extract_workflow_set_result(id = .x) |&gt;\n select_best(metric = \"f_meas\"))\n\nEn la sección Chapter 7 mencionamos la necesidad de crear un conjunto de validación para usarlo con la función last_fit(). Es en este punto donde se utiliza el conjunto de validación, el cual se encuentra encapsulado en un objeto de tipo validation_set que es con lo que puede trabjar la función last_fit().\nRecordemos primero el contenido de ctl_split, el cual definimos en la sección Chapter 7\n\nctl_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;3396/1133/1135/5664&gt;\n\n\nEl conjunto de validación es el conjunto de entrenamiento y el conjunto de validación:\n\nctl_validation_set\n\n# A tibble: 1 × 2\n  splits              id        \n  &lt;list&gt;              &lt;chr&gt;     \n1 &lt;split [3396/1133]&gt; validation\n\n\nLo que haremos a continuación será usar los mejores modelos obtenidos en el entrenamiento con validación cruzada y comprobarlos contra el conjunto de validación. Esta tarea en realidad se realizó durante muchas veces, siguiendo la metodología descrita en la sección Section 4.3, iterando hasta obtener los hiperparámetros que no generaban sobreajuste.\n\nfinal_val &lt;- map2(\n .x = best,\n .y = lista_mejores, ~ tune_res %&gt;%\n extract_workflow(id = .x) %&gt;%\n finalize_workflow(.y) %&gt;%\n last_fit(split = ctl_validation_set$splits[[1]], metrics = mset))\n\n\nmetricas_validation &lt;- final_val %&gt;%\n map_dfr(~ collect_metrics(.x), .id = \"modelo\") %&gt;%\n select(-.estimator) |&gt;\n pivot_wider(names_from = .metric, values_from = .estimate) |&gt;\n select(modelo, f1_val = f_meas, prec_val = precision, \n rec_val = recall, roc_auc)\n\nDebido a que es más eficiente realizar de una vez la comparación de las métricas obtenidas en entrenamiento contra validación, no presentaremos los resultados de entrenamiento por separado, sino que crearemos un dataframe que contenga la comparación.\n\ncomparacion &lt;- metricas_training |&gt;\n  left_join(metricas_validation, join_by(modelo)) |&gt;\n  select(-roc_auc) |&gt;\n  relocate(modelo,\n           f1_tr,\n           f1_val,\n           rec_tr,\n           rec_val,\n           prec_tr,\n           prec_val) |&gt;\n  arrange(-f1_val)\n\n\n\nMostrar Código\ncomparacion |&gt; \n gt() |&gt; \n tab_header(\n    title = md(\"**Evaluación con Conjunto de Validación**\"),\n    subtitle = md(\"Filtrado solo modelos sin sobreajuste\")\n  ) |&gt; \n gt_theme_538() |&gt; \n data_color(\n    columns = where(~ is.numeric(.x)),\n    method = \"numeric\",\n    palette = \"RdYlGn\", \n    reverse = FALSE) |&gt; \n fmt_number(columns = where(~ is.numeric(.x)), decimals = 4) |&gt; \n cols_align_decimal() |&gt; \n cols_align(align = \"center\", columns = where(~ is.numeric(.x)))\n\n\n\n\n\nTable 11.2:  Evaluación \n  \n    \n      Evaluación con Conjunto de Validación\n    \n    \n      Filtrado solo modelos sin sobreajuste\n    \n    \n      modelo\n      f1_tr\n      f1_val\n      rec_tr\n      rec_val\n      prec_tr\n      prec_val\n    \n  \n  \n    boruta_regular_rand_forest_ranger\n0.9760\n0.9844\n0.9742\n0.9844\n0.9784\n0.9846\n    boruta_regular_bt_lightgbm\n0.9669\n0.9717\n0.9639\n0.9715\n0.9716\n0.9739\n    infgain_regular_7_bt_lightgbm\n0.9648\n0.9684\n0.9622\n0.9680\n0.9693\n0.9712\n    infgain_regular_9_bt_lightgbm\n0.9615\n0.9650\n0.9596\n0.9634\n0.9641\n0.9670\n    infgain_regular_7_svm_linear_kernlab\n0.9549\n0.9576\n0.9511\n0.9544\n0.9602\n0.9620\n    infgain_regular_7_glmnet\n0.9506\n0.9542\n0.9490\n0.9533\n0.9531\n0.9562\n    infgain_regular_7_xgboost\n0.9421\n0.9514\n0.9345\n0.9475\n0.9562\n0.9611\n    infgain_regular_9_xgboost\n0.9420\n0.9463\n0.9350\n0.9411\n0.9523\n0.9549\n    infgain_norm_7_xgboost\n0.9406\n0.9458\n0.9332\n0.9415\n0.9543\n0.9563\n    boruta_regular_xgboost\n0.9406\n0.9458\n0.9332\n0.9396\n0.9547\n0.9585\n    infgain_regular_7_nzv_bt_lightgbm\n0.8823\n0.9100\n0.8813\n0.9072\n0.8871\n0.9139\n    infgain_regular_7_nzv_xgboost\n0.8838\n0.9084\n0.8829\n0.9058\n0.8886\n0.9120\n    infgain_regular_7_decision_tree_partykit\n0.8940\n0.8978\n0.7853\n0.7865\n0.8508\n0.8555\n    mrmr_regular_top20_decision_tree_partykit\n0.8885\n0.8963\n0.7807\n0.7850\n0.8456\n0.8543\n    infgain_regular_7_nzv_svm_linear_kernlab\n0.8537\n0.8691\n0.8505\n0.8649\n0.8627\n0.8749\n    infgain_regular_7_nzv_glmnet\n0.8481\n0.8662\n0.8466\n0.8652\n0.8542\n0.8684\n    infgain_regular_7_nzv_knn\n0.8093\n0.8365\n0.8053\n0.8337\n0.8202\n0.8402\n    infgain_regular_7_nzv_decision_tree_partykit\n0.7527\n0.7770\n0.5824\n0.5986\n0.7207\n0.7336\n  \n  \n  \n\n\n\n\n\nEn la tabla Table 11.2 se aprecia que ninguno de los modelos presenta sobreajuste.\n\n\nMostrar Código\nmetricas_training |&gt;\n left_join(metricas_validation, join_by(modelo)) |&gt;\n slice(1:5) |&gt;\n select(modelo, roc_auc) |&gt;\n arrange(-roc_auc) |&gt; \n gt() |&gt; \n gt_theme_538() |&gt;\n tab_header(\n    title = md(\"**Evaluación del ROC-AUC**\"),\n    subtitle = md(\"Top 5 Modelos\")\n  ) |&gt; \n fmt_number(columns = where(~ is.numeric(.x)), decimals = 4) |&gt; \n data_color(\n    columns = where(~ is.numeric(.x)),\n    method = \"numeric\",\n    palette = \"RdYlGn\", \n    reverse = FALSE)\n\n\n\n\n\nTable 11.3:  ROC-AUC \n  \n    \n      Evaluación del ROC-AUC\n    \n    \n      Top 5 Modelos\n    \n    \n      modelo\n      roc_auc\n    \n  \n  \n    boruta_regular_rand_forest_ranger\n0.9993\n    boruta_regular_bt_lightgbm\n0.9979\n    infgain_regular_7_bt_lightgbm\n0.9964\n    infgain_regular_9_bt_lightgbm\n0.9950\n    infgain_regular_7_svm_linear_kernlab\n0.9609"
  },
  {
    "objectID": "resultados.html#selección-de-modelos",
    "href": "resultados.html#selección-de-modelos",
    "title": "11  Resultados",
    "section": "11.2 Selección de modelos",
    "text": "11.2 Selección de modelos\nEscogeremos los primeros tres modelos para que puedan competir usando el dataset de prueba.\n\nmejorcitos &lt;- comparacion |&gt;\n  slice_max(f1_val, n = 3) |&gt;\n  pull(modelo)\n\nlista_mejores &lt;- mejorcitos %&gt;%\n  map(~ final_tune %&gt;% extract_workflow_set_result(id = .x) %&gt;%\n  select_best(metric = \"f_meas\")) |&gt;\n  set_names(mejorcitos)\n\nEn esta ocasión utilizaremos el objeto ctl_split y le diremos que haga un último ajuste (last_fit), pero en esta ocasión usando únicamente los tres mejores modelos que resultaron en el conjunto de validación y, adicionalmente que entrene estos modelos con esa combinación específica de hiperparámetros, utilizando tanto los datos de entrenamiento como los datos de validación. Para lograr esto agregamos el parámetro add_validation_set = TRUE.\n\nfinal_test &lt;- map2(\n .x = mejorcitos,\n .y = lista_mejores,\n ~ tune_res |&gt;\n extract_workflow(id = .x) |&gt;\n finalize_workflow(.y) |&gt;\n last_fit(split = ctl_split, metrics = mset, add_validation_set = TRUE))\n\n\nmetricas_test &lt;- final_test |&gt; \n map_dfr(~ collect_metrics(.x), .id = \"modelo\") |&gt; \n select(-.estimator) |&gt;\n pivot_wider(names_from = .metric, values_from = .estimate) |&gt; \n select(modelo, f1_test = f_meas, prec_test = precision,\n rec_test = recall, roc_auc) |&gt;\n mutate(modelo = mejorcitos)\n\n\ncomparacion_final &lt;- metricas_training |&gt;\n inner_join(metricas_test, join_by(modelo)) |&gt;\n relocate(\n  modelo,\n  f1_tr,\n  f1_test,\n  rec_tr,\n  rec_test,\n  prec_tr,\n  prec_test\n  ) |&gt;\n arrange(-f1_test)\n\n\n\nMostrar Código\ncomparacion_final |&gt; \n gt() |&gt; \n gt_theme_538() |&gt; \n tab_header(\n    title = md(\"**Evaluación final con datos de prueba**\"),\n    subtitle = md(\"Top 3 Modelos\")\n  ) |&gt; \n fmt_number(columns = where(~ is.numeric(.x)), decimals = 4) |&gt; \n data_color(\n    columns = where(~ is.numeric(.x)),\n    method = \"numeric\",\n    palette = \"RdYlGn\", \n    reverse = FALSE)\n\n\n\n\n\nTable 11.4:  test \n  \n    \n      Evaluación final con datos de prueba\n    \n    \n      Top 3 Modelos\n    \n    \n      modelo\n      f1_tr\n      f1_test\n      rec_tr\n      rec_test\n      prec_tr\n      prec_test\n      roc_auc\n    \n  \n  \n    boruta_regular_rand_forest_ranger\n0.9760\n0.9763\n0.9742\n0.9774\n0.9784\n0.9755\n0.9991\n    boruta_regular_bt_lightgbm\n0.9669\n0.9734\n0.9639\n0.9712\n0.9716\n0.9767\n0.9962\n    infgain_regular_7_bt_lightgbm\n0.9648\n0.9722\n0.9622\n0.9701\n0.9693\n0.9757\n0.9935\n  \n  \n  \n\n\n\n\n\nLos resultados de la tabla Table 11.4 indican que el primer candidato presentó un ligero sobreajuste en la precisión. Los dos modelos restantes no presentan sobreajuste. Seleccionaremos el modelo con el mejor f1-score en los datos de prueba que no presente sobreajuste.\n\nbest_model &lt;- \"boruta_regular_bt_lightgbm\"\nlg &lt;- final_test |&gt;\n set_names(mejorcitos) |&gt;\n keep_at(best_model) |&gt;\n pluck(1)\n\n\npredicciones &lt;- lg |&gt;\n collect_predictions(summarize = T) |&gt; \n select(diag, .pred_class)\n\nAhora nos disponemos a calcular las métricas con base a las predicciones de clase. Para esto crearemos un dataframe que solo contenga la ground-truth label que en este caso es la columna diag y la estimación o predicción que es la columna .pred_class.\n\npredicciones |&gt;\n head() |&gt;\n gt() |&gt;\n gt_theme_538() |&gt; \n cols_align(align = \"left\") |&gt; \n tab_header(\n    title = md(\"**Muestra de dataset con predicciones**\"),\n    subtitle = md(\"Calculo de métricas de clase\")\n  )\n\n\n\n\n  \n    \n      Muestra de dataset con predicciones\n    \n    \n      Calculo de métricas de clase\n    \n    \n      diag\n      .pred_class\n    \n  \n  \n    DISPONIBILIDAD\nDISPONIBILIDAD\n    OPTIMIZACION\nOPTIMIZACION\n    COBERTURA\nCOBERTURA\n    CAPACIDAD\nCAPACIDAD\n    COBERTURA\nCOBERTURA\n    DISPONIBILIDAD\nDISPONIBILIDAD\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nDurante el proceso de evaluación de los modelos en nuestro proyecto, nos enfrentamos a un desafío significativo debido al desbalance de clases en nuestro conjunto de datos. Al utilizar yardstick (Kuhn, Vaughan, and Hvitfeldt 2023) para calcular la precisión multiclase, nos encontramos con advertencias indicando que algunos niveles no tenían eventos predichos, lo que resultaba en una precisión indefinida para esas clases y, por ende, afectaba el promedio de precisión general del modelo.\nEste comportamiento de yardstick puede ser problemático en nuestro contexto, ya que las clases con cero predicciones son excluidas del cálculo del promedio, lo que podría dar una impresión engañosa de un rendimiento más bajo de lo real. En contraste, classification_report de scikit-learn maneja estas situaciones asignando un valor de cero a las métricas de precisión, recall y f1-score para las clases sin eventos predichos. Esto refleja de manera más precisa la incapacidad del modelo para identificar esas clases y proporciona una imagen más realista del rendimiento del modelo.\nLa decisión de utilizar classification_report se basó en la necesidad de tener una representación más fiel del desempeño de los modelos, especialmente en lo que respecta a la precisión. Queríamos asegurarnos de que todas las clases, independientemente de su frecuencia, fueran consideradas en la evaluación del modelo. Esto es crucial en nuestro proyecto, donde cada clase tiene importancia y el objetivo es lograr un modelo que sea capaz de identificar todas las categorías de diagnóstico de manera efectiva.\nEn resumen, classification_report nos proporcionó una visión más integral y menos sesgada del rendimiento de los modelos en presencia de clases desbalanceadas, lo que nos permitió tomar decisiones más informadas durante la selección y ajuste del modelo final.\n\n\n\nsource(\"_python.R\")\n\n\nfrom sklearn.metrics import classification_report\n\n\nimport pandas as pd\nimport numpy as np\n\n\ny_true = r.predicciones['diag']\ny_pred = r.predicciones['.pred_class']\nreporte_dict = classification_report(y_true, y_pred, output_dict=True)\nreporte_df = pd.DataFrame(reporte_dict).transpose()\n\n\nres_test &lt;- py$reporte_df |&gt;\n rownames_to_column(var = \"categoria\") |&gt; \n as_tibble(.name_repair = make_clean_names) |&gt; \n slice(1:5) |&gt; \n select(-support) |&gt; \n pivot_longer(\n cols = where(is.numeric), names_to = \"metric\", values_to = \"value\")\n\n\nporcentaje &lt;- label_percent(decimal.mark = \".\", suffix = \"%\", accuracy = 0.1)\n\n\nres_test |&gt; \n ggplot(aes(x = value, y = categoria)) +\n geom_col(aes(fill = metric)) +\n scale_fill_paletteer_d(`\"ggsci::nrc_npg\"`) +\n geom_text(aes(label = porcentaje(value)),\n vjust = 0.5,\n hjust = 1.1,\n color = \"white\",\n size = 10) +\n facet_grid(~ metric, scales = \"free\") +\n theme(\n plot.margin     = unit(c(1, 1, 1, 1), \"mm\"),\n legend.position = \"none\",\n axis.title  = element_blank(),\n axis.text   = element_text(size = 30),\n strip.text  = element_text(size = 35),\n axis.text.x = element_text(size = 25))\n\n\n\n\nFigure 11.2: res-metrics\n\n\n\n\nSe observa en la figura Figure 11.2 el desempeño del modelo con el conjunto de prueba y cada una de las métricas para cada una de las clases. Al parecer el modelo captura de manera muy eficiente los patrones relacionados a la disponibilidad, sin embargo, la precisión de los promotores y el recall de la categoría de capacidad son particularmente bajos en relación al resto de métricas.\n\nlg |&gt;\n collect_predictions() |&gt;\n roc_curve(diag, .pred_CAPACIDAD:.pred_PROMOTOR) |&gt;\n ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(alpha = 0.8, linewidth = 1.2) +\n  coord_equal() +\n  scale_color_paletteer_d(`\"ggsci::nrc_npg\"`) +\n  labs(title = \"LightGBM Gradient Boosting Decision Tree\") +\n  theme(\n   plot.title = element_text(size = 30),\n   legend.position = \"bottom\",\n   legend.text     = element_text(size = 25),\n   axis.title      = element_text(size = 25),\n   axis.text       = element_text(size = 20))\n\n\n\n\nFigure 11.3: ROC-AUC\n\n\n\n\n\n11.2.1 Importancia de los predictores\n\nlgbm |&gt; lgb.plot.importance(top_n = 10, cex = 2)\n\n\n\n\nFigure 11.4: Feature Importance\n\n\n\n\nLa gráfica Figure 11.4 se realizó después de ajustar el modelo seleccionado boruta_regular_bt_lightgbm con todos los datos, es decir, con los datos de entrenamiento, validación y prueba. Contrario a lo que esperábamos, el prb no se encuentra en este top. Es posible que Boruta lo haya eliminado y dejado únicamente thp.\nEn entrevistas finales, como parte del Human-in-the-Loop, los SMEs nos indicaron que cada una de las categorías tiene al menos un predictor en este top 10.\n\n\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2023. Yardstick: Tidy Characterizations of Model Performance. https://github.com/tidymodels/yardstick."
  },
  {
    "objectID": "discusion.html",
    "href": "discusion.html",
    "title": "12  Discusión",
    "section": "",
    "text": "En esta sección, evaluamos el rendimiento de los modelos propuestos. El algoritmo que mejor se desempeñó en los datos de prueba obtuvo un f1-score de 97.35%, un recall de 97.12%, una precisión de 97.67% y un AUC de 99.61%. Este alto rendimiento se debe en gran parte a una cuidadosa ingeniería de atributos y al ajuste de hiperparámetros. Por ejemplo, ajustamos manualmente la tasa de aprendizaje de XGBoost a 0.001 y limitamos la profundidad del árbol para evitar el sobreajuste. Estos resultados superan al estudio de (Ahmad, Jafar, and Aljoumaa 2019), que logró un 93% de AUC utilizando algoritmos similares.\nLa interpretabilidad del modelo es crucial para entender qué factores contribuyen más al fenómeno que estamos estudiando, lo cual es invaluable para tomar decisiones informadas. Además, un modelo que generaliza bien es esencial para que las predicciones sean útiles en diferentes escenarios y poblaciones. Al comparar el rendimiento de los algoritmos, encontramos que LightGBM ofrecía el mejor equilibrio entre rendimiento y complejidad, aunque Random Forest lideraba en métricas. Sin embargo, Random Forest mostró signos de sobreajuste en el recall, lo cual podría mejorarse evitando la multicolinealidad en las variables.\nEn nuestras primeras iteraciones, variables geográficas como “departamento” y “ciudad” mostraron un alto poder predictivo. Sin embargo, optamos por excluir estas variables para evitar posibles problemas de fuga de datos. Los atributos más importantes, como dis_mim y cqi_mim, están asociados a categorías específicas. Por ejemplo, dis_mim se asocia a la etiqueta de disponibilidad y representa los casos donde hay más segundos fuera de servicio en una celda. tad_max se relaciona con la etiqueta de cobertura, y sus valores máximos representan coberturas lejanas. Otros atributos, como drp_max ayudan a identificar problemas de optimización.\nFinalmente, seleccionamos GLMNET como nuestro algoritmo óptimo. A pesar de ser un modelo lineal, GLMNET es un 93% más rápido y eficiente en términos computacionales que LightGBM, sin sacrificar significativamente el rendimiento. Esta eficiencia es crucial, ya que planeamos reentrenar el modelo regularmente en la nube de AWS.\nLos resultados obtenidos son directamente aplicables y se alinean con el plan de acción previamente establecido. El despliegue del modelo en modo batch agilizará el proceso de identificación de potenciales detractores y, por ende, identificará las celdas que requieren atención para mejorar los indicadores de red. Los costos se reducirán, ya que este sistema será el encargado de evaluar a toda la base de usuarios y, de forma indirecta, prevenir el churn.\nUna de las limitaciones clave que enfrentamos fue la inconsistencia en la calidad de las etiquetas, fenómeno conocido como label multiplicity. Este surgió porque diferentes personas se encargaban de la clasificación de los diagnósticos sin seguir un conjunto de reglas uniforme. Para resolver este problema, implementamos una heurística que nos ayudó a identificar las celdas afectadas, un enfoque que podría considerarse como “supervisión débil”. Este método resultó efectivo para mejorar la calidad de nuestro conjunto de datos, especialmente porque los atributos que capturamos son información que el equipo CTL normalmente incluiría en el proceso de etiquetado.\nEn resumen, aunque obtuvimos resultados prometedores, es fundamental ser cautelosos al interpretar estos resultados debido a la naturaleza de las etiquetas generadas y la supervisión débil que se aplicó. Futuras iteraciones del proyecto podrían beneficiarse de un enfoque más formalizado para manejar la “multiplicidad de etiquetas” y mejorar la robustez del modelo.\n\n\n\n\nAhmad, Abdelrahim Kasem, Assef Jafar, and Kadan Aljoumaa. 2019. “Customer Churn Prediction in Telecom Using Machine Learning in Big Data Platform.” Journal of Big Data 6 (1): 28. https://doi.org/10.1186/s40537-019-0191-6."
  },
  {
    "objectID": "conclusiones.html",
    "href": "conclusiones.html",
    "title": "13  Conclusiones",
    "section": "",
    "text": "El principal objetivo de este estudio fue desarrollar un modelo de clasificación multinomial para automatizar la clasificación de detractores de red móvil en Tigo Guatemala. Este objetivo se logró con éxito, alcanzando un F1-Score de 97.33%%, lo cual supera el umbral del 90% establecido inicialmente. De esta manera, se confirma la hipótesis de que la clasificación de detractores de red móvil puede ser automatizada de manera efectiva en nuestra empresa.\nLos resultados secundarios, como un recall de 97.11% y una precisión del 97.67%, junto con un AUC del 99.61%, demuestran la alta eficacia del modelo en la clasificación de detractores y promotores de red. Estos indicadores son un testimonio de la robustez del modelo y su capacidad para distinguir correctamente entre diferentes clases.\nLa ingeniería de atributos, el preprocesamiento de datos y el ajuste de hiperparámetros jugaron un papel crucial en el rendimiento del modelo. El uso de técnicas de selección de atributos y el conocimiento de dominio adquirido a través de entrevistas con expertos en la materia (SMEs) fueron fundamentales para este éxito.\nEn cuanto a la selección del algoritmo, se evaluaron varias opciones. Aunque Random Forest mostró un rendimiento excepcional, su alta demanda de recursos computacionales podría ser un desafío para implementaciones a gran escala. LightGBM y Glmnet emergieron como alternativas viables, siendo este último especialmente atractivo para la producción debido a su menor costo computacional y complejidad.\nLa implementación del modelo desarrollado ha demostrado ser una herramienta valiosa para los equipos técnicos de nuestra empresa. Su uso en modo batch acelerará la identificación de posibles detractores y las celdas que requieren atención, lo que a su vez contribuirá a la mejora de los indicadores clave de rendimiento (KPIs) de la red. Este modelo actuará como una brújula para guiar las inversiones en infraestructura, con el potencial de mejorar el NPS transaccional y, por ende, la experiencia del cliente (CX), un factor crucial para la retención de usuarios en un mercado competitivo.\nFinalmente, este estudio abre la puerta a futuras investigaciones, como la posibilidad de predecir qué celdas o radio bases podrían enfrentar problemas de red en el futuro cercano, un desafío que podría abordarse desde una perspectiva de detección de anomalías multivariadas."
  },
  {
    "objectID": "recomendaciones.html",
    "href": "recomendaciones.html",
    "title": "14  Recomendaciones",
    "section": "",
    "text": "Etiquetado de celdas de diagnóstico: El uso de machine learning es crucial para automatizar la clasificación de detractores, especialmente porque el etiquetado manual es inviable a largo plazo. Por ello, se recomienda que durante el etiquetado manual se incluyan atributos específicos que indiquen las celdas, rangos de fechas y horas que fundamentan el diagnóstico. Esta información será invaluable para futuras iteraciones del modelo.\nSupervisión débil: Dado el alto costo en tiempo y recursos del etiquetado manual, se sugiere la implementación de supervisión débil. Este enfoque utiliza heurísticas simples para generar etiquetas preliminares, lo que puede ser especialmente útil para diagnósticos que no requieren una comprensión profunda de las interacciones entre variables. Herramientas como Snorkel pueden ser útiles en este contexto.\nUtilizar modelos de baja varianza: En el contexto del equilibrio entre sesgo y varianza, se recomienda la utilización de modelos de baja varianza como GLMNET para la puesta en producción. Estos modelos, que han demostrado ser efectivos en pruebas preliminares, son robustos ante variaciones en los datos y ofrecen una buena generalización.\nMonitoreo y reentrenamiento: Es imperativo que cualquier modelo en producción sea objeto de un monitoreo y reentrenamiento continuos para evitar la drift del modelo. Aunque MLOps es un marco de trabajo en desarrollo, un monitoreo periódico del rendimiento del modelo es esencial.\nAplicar Human-in-the-Loop (HITL): A pesar de la eficacia de los modelos de machine learning, siempre existirá un margen de error. Por ello, se recomienda la implementación de un enfoque de Human-in-the-Loop, especialmente en contextos donde la toma de decisiones es crítica. Este enfoque permite que expertos humanos validen y, si es necesario, corrijan las decisiones del modelo.\nDetección de anomalías: Finalmente, se sugiere la exploración de modelos especializados en la detección de anomalías para series temporales. Este enfoque podría permitir intervenciones más tempranas en la red, mejorando así la experiencia del cliente y potencialmente reduciendo la tasa de abandono."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Ahmad, Abdelrahim Kasem, Assef Jafar, and Kadan Aljoumaa. 2019.\n“Customer Churn Prediction in Telecom Using Machine Learning in\nBig Data Platform.” Journal of Big Data 6 (1): 28. https://doi.org/10.1186/s40537-019-0191-6.\n\n\nAhmed, Waqar, Osman Hasan, Usman Pervez, and Junaid Qadir. 2017.\n“Reliability Modeling and Analysis of\nCommunication Networks.” Journal of\nNetwork and Computer Applications 78 (January): 191–215. https://doi.org/10.1016/j.jnca.2016.11.008.\n\n\nChawla, N. V., K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. 2002.\n“SMOTE: Synthetic Minority\nOver-Sampling Technique.” Journal\nof Artificial Intelligence Research 16 (June): 321–57. https://doi.org/10.1613/jair.953.\n\n\nDuboue, Pablo. 2020. The Art of Feature Engineering: Essentials for\nMachine Learning. First edition. Cambridge ; New York, NY:\nCambridge University Press.\n\n\nFernandez, Alberto, and Salvador García. 2018. Learning from\nImbalanced Data Sets. New York, NY: Springer Science+Business\nMedia.\n\n\nFox, John. 2003. “Effect Displays in R for\nGeneralised Linear Models.” Journal of Statistical\nSoftware 8 (15): 1–27. https://doi.org/10.18637/jss.v008.i15.\n\n\nHastie, Trevor, Junyang Quian, and Kennet Tay. 2023. “An\nIntroduction to GLMNET.” https://stanford.io/3QGGqcD.\n\n\nIzogo, Ernest Emeka. 2017. “Customer Loyalty in Telecom Service\nSector: The Role of Service Quality and Customer Commitment.”\nThe TQM Journal 29 (1): 19–36. https://doi.org/10.1108/TQM-10-2014-0089.\n\n\nJain, Hemlata, Ajay Khunteta, and Sumit Srivastava. 2021. “Telecom\nChurn Prediction and Used Techniques, Datasets and Performance Measures:\nA Review.” Telecommunication Systems 76 (4): 613–30. https://doi.org/10.1007/s11235-020-00727-0.\n\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. “LightGBM:\nA Highly Efficient\nGradient Boosting Decision\nTree,” January.\n\n\nKuhn, Max, and Kjell Johnson. 2013. Applied Predictive\nModeling. New York: Springer.\n\n\n———. 2020. Feature Engineering and Selection: A Practical Approach\nfor Predictive Models. Chapman &\nHall/CRC Data Science Series. Boca Raton\nLondon New York: CRC Press, Taylor & Francis Group.\n\n\nKuhn, Max, Davis Vaughan, and Emil Hvitfeldt. 2023. Yardstick: Tidy\nCharacterizations of Model Performance. https://github.com/tidymodels/yardstick.\n\n\nLevin, Richard I., and David S. Rubin. 1998. Statistics for\nManagement. 7th ed. Upper Saddle River, N.J: Prentice Hall.\n\n\nMitchell, Tom M. 2013. Machine Learning. Nachdr.\nMcGraw-Hill Series in Computer\nScience. New York: McGraw-Hill.\n\n\nMuhammad, Irfan, Mohammad Farid Shamsudin, and Noor Ul Hadi. 2016.\n“How Important Is Customer\nSatisfaction? Quantitative\nEvidence from Mobile\nTelecommunication Market.”\nInternational Journal of Business and Management 11 (6): 57. https://doi.org/10.5539/ijbm.v11n6p57.\n\n\nMustafa, Nurulhuda, Lew Sook Ling, and Siti Fatimah Abdul Razak. 2021.\n“Customer Churn Prediction for Telecommunication Industry:\nA Malaysian Case\nStudy.” F1000Research 10 (December): 1274.\nhttps://doi.org/10.12688/f1000research.73597.1.\n\n\nOliva Navarro, Diego Alberto, Essam H. Houssein, and Salvador Hinojosa,\neds. 2021. Metaheuristics in Machine Learning: Theory and\nApplications. Studies in Computational Intelligence, volume 967.\nCham: Springer.\n\n\nOpitz, Juri, and Sebastian Burst. 2021. “Macro F1 and\nMacro F1,” February. http://arxiv.org/abs/1911.03347.\n\n\nPawley, Steven. 2022. “GitHub - Stevenpawley/Colino:\nRecipes Steps for Supervised\nFilter-Based Feature\nSelection.” GitHub. https://github.com/stevenpawley/colino.\n\n\nPeterson, Ryan A. 2021. “Finding Optimal\nNormalizing Transformations via bestNormalize.”\nThe R Journal 13 (1): 310–29. https://doi.org/10.32614/RJ-2021-041.\n\n\nPeterson, Ryan Andrew. 2023. bestNormalize: Normalizing\nTransformation Functions. https://petersonr.github.io/bestNormalize/.\n\n\nSeymen, Omer Faruk, Emre Ölmez, Onur Doğan, Orhan Er, and Kadir\nHiziroğlu. 2023. “Customer Churn\nPrediction Using Ordinary\nArtificial Neural Network and\nConvolutional Neural Network\nAlgorithms: A Comparative\nPerformance Assessment.” Gazi\nUniversity Journal of Science 36 (2): 720–33. https://doi.org/10.35378/gujs.992738.\n\n\nStanczyk, Urszula, Beata Zielosko, and Lakhmi C. Jain. 2017.\nAdvances in Feature Selection for Data and Pattern Recognition.\nNew York, NY: Springer Berlin Heidelberg.\n\n\nUllah, Irfan, Basit Raza, Ahmad Kamran Malik, Muhammad Imran, Saif Ul\nIslam, and Sung Won Kim. 2019. “A Churn\nPrediction Model Using\nRandom Forest: Analysis of\nMachine Learning Techniques for\nChurn Prediction and Factor\nIdentification in Telecom\nSector.” IEEE Access 7: 60134–49. https://doi.org/10.1109/ACCESS.2019.2914999.\n\n\nZhou, Ronggang, Xiaorui Wang, Yuhan Shi, Renqian Zhang, Leyuan Zhang,\nand Haiyan Guo. 2019. “Measuring e-Service Quality and Its\nImportance to Customer Satisfaction and Loyalty: An Empirical Study in a\nTelecom Setting.” Electronic Commerce Research 19 (3):\n477–99. https://doi.org/10.1007/s10660-018-9301-3."
  },
  {
    "objectID": "glosario.html",
    "href": "glosario.html",
    "title": "Appendix A — Glosario",
    "section": "",
    "text": "dic_f &lt;- dir_ls(\"diccionarios\", glob = \"*.csv\")\n\n\ndic &lt;- dic_f |&gt; map(~ read_csv(file = .x)) |&gt; \n list_rbind()\n\n\n\nMostrar Código\ndic |&gt; \n gt() |&gt; \n gt_theme_538() |&gt; \n tab_header(\n  title = html(\"&lt;p style='color:#25a7f0;'&gt;&lt;strong&gt;Diccionario&lt;/strong&gt;&lt;/p&gt;\"),\n    subtitle = md(\"Tablas principales\")\n  )\n\n\n\n\n\nTable A.1:  dic \n  \n    \n      Diccionario\n    \n    \n      Tablas principales\n    \n    \n      feature\n      tipo_dato\n      descripcion\n      tabla\n    \n  \n  \n    cll_prctg\ndouble\nPorcentaje de tiempo que estuvo conectado a la celda.\nMétricas\n    fct_srvy_dt\ndate\nDía en que fue llenada la encuesta por el usuario\nDiagnóstico\n    bts_sh_nm\ncharacter\nIdentificador de celda\nMétricas\n    msisdn_dd\ninteger\nNúmero de teléfono\nDiagnóstico\n    srvy_id\ninteger\nIdentificador de encuesta\nDiagnóstico\n    cty_nm\ncharacter\nCiudad\nDiagnóstico\n    trrtry_cmrcl_nm\ncharacter\nRegión comercial\nDiagnóstico\n    trrtry_tf_nm\ncharacter\nRegión de RF planning\nDiagnóstico\n    stt_nm\ncharacter\nDepartamento al que más se conecta\nDiagnóstico\n    diag\ncharacter\nVariable respuesta con las categorías a predecir\nDiagnóstico\n    rate_prb_dl\ndouble\nPorcentaje de uso de bloques de recursos en el enlace descendente.\nMétricas\n    throughput_dl\ndouble\nVelocidad de descarga por dispositivo en Mbps.\nMétricas\n    throughput_ul\ndouble\nVelocidad de subida por dispositivo en Mbps.\nMétricas\n    time_cl\ndouble\nPorcentaje de tiempo en buen estado de carga de celda.\nMétricas\n    thp_required_lte\ndouble\nPorcentaje de tiempo con throughput requerido en LTE.\nMétricas\n    ra_ta_ue_index1\ndouble\nDistancia y conexiones a la estacion base (indice 1).\nMétricas\n    ra_ta_ue_index2\ndouble\nConexiones generadas a cierta distancia (Indice 2).\nMétricas\n    ra_ta_ue_index3\ndouble\nConexiones generadas a cierta distancia (Indice 3).\nMétricas\n    ra_ta_ue_index4\ndouble\nConexiones generadas a cierta distancia (Indice 4).\nMétricas\n    ra_ta_ue_index5\ndouble\nConexiones generadas a cierta distancia (Indice 5).\nMétricas\n    ra_ta_ue_index6\ndouble\nConexiones generadas a cierta distancia (Indice 6).\nMétricas\n    ra_ta_ue_index7\ndouble\nConexiones generadas a cierta distancia (Indice 7).\nMétricas\n    ra_ta_ue_total\ndouble\nTotal de conexiones de datos o llamadas.\nMétricas\n    time_lte\ndouble\nTiempo conectado a LTE.\nMétricas\n    avlblty\ndouble\nTiempo de navegacion sin fallas.\nMétricas\n    avlblty_tckt_2hrs\ndouble\nTickets de mas de 2 horas sobre total de tickets.\nMétricas\n    unvlbty_ttl_hrs_prop\ndouble\nHoras de indisponibilidad del usuario.\nMétricas\n    erab_success_rate\ndouble\nTasa de exito en la configuracion de E-RAB en LTE.\nMétricas\n    rrc_success_rate\ndouble\nTasa de exito en la conexion RRC en LTE.\nMétricas\n    corrected_cqi\ndouble\nIndice de calidad de celda.\nMétricas\n    l_ul_interference_avg\ndouble\nInterferencia promedio en el enlace ascendente.\nMétricas\n    volte_erlang\ndouble\nVolumen de trafico VoLTE en erlangs.\nMétricas\n    modulation_16qam_ratio\ndouble\nUso de modulacion 16QAM en la transferencia de datos.\nMétricas\n    modulation_64qam_ratio\ndouble\nUso de modulacion 64QAM en la transferencia de datos.\nMétricas\n    modulation_qpsk_ratio\ndouble\nUso de modulacion QPSK en la transferencia de datos.\nMétricas\n    fct_srvy_dt\ndate\nDía en que fue llenada la encuesta por el usuario\nDiagnóstico\n    bts_sh_nm\ncharacter\nIdentificador de celda\nMétricas\n    msisdn_dd\ninteger\nNúmero de teléfono\nDiagnóstico\n    srvy_id\ninteger\nIdentificador de encuesta\nDiagnóstico\n    rain\ndouble\nCantidad de lluvia en mm cúbicos\nDiagnóstico"
  }
]