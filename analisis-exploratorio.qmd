# Análisis Exploratorio {#sec-analisis-exploratorio}

```{r}
#| echo: false
source("_paquetesx.R")
source("_configura.R")
source("_variables.R")
source("_funciones.R")
```

```{r}
#| echo: false
ctl_split <- pin_read(board = tablero_ctl, name = "ctl_split")
ctl_train <- training(ctl_split)
ctl_validation_set <- validation_set(ctl_split)
```

## Estructura

Los datos de entrenamiento, los cuales fueron preparados anteriormente constan
de `r nrow(ctl_train)` filas y `r ncol(ctl_train)` columnas.

```{r}
ctl_train |> glimpse(width = 77)
```

```{r}
#| code-fold: true
#| code-summary: "Mostrar Código"
#| label: tbl-ctl
#| tbl-cap: "Solo 8 columnas y 5 filas"
ctl_train |> 
 select(1:8) |> 
 slice_head(n = 5) |> 
 gt() |> 
 tab_header(
    title = md("**Muestra Aleatoria - Datos de Entrenamiento**"),
    subtitle = md("Tabla principal")
  ) |> 
 gt_theme_538() |> 
 fmt_integer(columns = user, use_seps = FALSE) |> 
 fmt_number(columns = 2:8, decimals = 2) |> 
 cols_align_decimal() |> 
 cols_align(align = "center", columns = where(~ is.numeric(.x)))
```

```{r}
#| code-fold: true
#| fig-align: center
#| code-summary: "Código"
#| label: fig-resumen
#| fig-width: 9
#| fig-cap: Se puede apreciar que la gran mayoría de las columnas están completas. Todas las columnas son de tipo continuo.
plot_intro(
 ctl_train,
 ggtheme = yunkel,
 title = "Resumen Descriptivo",
 geom_label_args = list(label.size = 0.8, size = 7),
 theme_config = list(
 axis.text = element_text(size = 40)))
```

## Valores faltantes

Debido a que en la fase de preparación determinamos que la cantidad de valores
faltantes era de dos o tres filas, optamos por eliminarlas sin riesgo de algún
detrimento por pérdida de información. 

Utilizaremos un procedimiento que nos permita evaluar rápidamente si hay valores
faltantes en algunos de los predictores

```{r}
ctl_train |> 
 inspect_na() |> 
 with(all(cnt != 0))
```

## Resumen estadístico {#sec-estadisticas}

</br>

```{r}
#| code-fold: true
#| code-summary: "Mostrar Código"
#| label: tbl-resumen
#| cap-location: bottom
#| tbl-cap: Resumen estadístico
ctl_train |> 
 select(-user, -diag) |> 
 resumir() |> 
 arrange(variable) |> 
 gt() |> 
 tab_header(
  title = md("**Resumen Estadístico**"),
    subtitle = md("para *features* Numéricos")
 ) |>
 gt_theme_538() |> 
 cols_align(align = "center", columns = where(~ is.numeric(.x))) |> 
 fmt_number(columns = where(is.numeric), decimals = 2)
```

En la tabla @tbl-resumen hemos extraído las estadísticas de resumen. Lo
interesante es que estas son estadísticas descriptivas aplicadas a estadísticas.
En el contexto de este proyecto tiene sentido realizar este análisis ya que
estos *features* servirán de Input para el modelo, por lo que su distribución
y forma son relevantes.

## Análisis univariado

### Variable dependiente

Uno de los primeros pasos del proceso de análisis exploratorios cuando el
propósito final es predecir una respuesta es crear visualizaciones que
ayuden a dilucidar el conocimiento de la respuesta y luego descubrir
relaciones entre los predictores y la respuesta [@kuhn_feature_2020].


```{r}
#| fig-width: 8
#| code-fold: true
#| code-summary: "Código"
#| label: fig-depe
#| fig-cap: Distribución de la variable dependiente
ctl_train |> 
 barra(diag) +
 theme(legend.position = "none") +
 labs(title = "Clasificación variable politómica")
```

Tal como se muestra en la @fig-depe se observa un desbalance en la variable
respuesta.  Este desequilibrio puede ocasionar que el modelo se sesgue hacia
las clases más frecuentes, como "CAPACIDAD" y "PROMOTOR", lo que podría llevar
a un rendimiento deficiente en la clasificación de otras clases.  La aplicación
de técnicas de remuestreo para balancear las clases será clave en la fase de
preprocesamiento.

### Mutual information

La Información Mutua es una métrica de utilidad de características que va más
allá de las correlaciones convencionales de Pearson y Spearman. Mientras que
estas últimas son excelentes para detectar relaciones lineales o monótonas,
tienen sus limitaciones. Por ejemplo, no pueden capturar relaciones más
complejas o no lineales entre variables. Además, las correlaciones tradicionales
suelen centrarse en variables numéricas, dejando de lado las categóricas.

En contraste, la Información Mutua es una herramienta poderosa que puede
capturar cualquier tipo de relación entre variables, ya sean numéricas o
categóricas. Funciona evaluando cómo el conocimiento de una característica
reduce la incertidumbre sobre la variable objetivo. En otras palabras, si
conoces el valor de una característica, ¿cuánto más seguro te sentirías sobre el
valor de la variable objetivo? Esta capacidad la convierte en una métrica
invaluable, especialmente en las etapas iniciales del desarrollo de
características, donde aún no sabemos qué modelo vamos a utilizar.

Es importante tener en cuenta que la Información Mutua es una métrica
univariada, lo que significa que evalúa cada característica de forma individual
en relación con la variable objetivo. No puede detectar interacciones entre
características, pero sigue siendo una herramienta extremadamente útil para una
primera ronda de selección de características.

Para poder tener un dataset más amplio en cuanto a *features*, realizaremos
un procedimiento con la fecha llamado *feature template*
[@duboue_art_2020, pag. 30]. En este caso, lo que se realizará es la
descomposición de la fecha en sus parte constituyentes.

```{r}
mi <- information_gain(
 formula      = diag ~ ., 
 data         = ctl_train |> select(-user), 
 type         = "infogain", # Symmetrical Uncertainty (SU)
 discIntegers = FALSE) |> 
 as_tibble() |> 
 arrange(-importance)
```

```{r}
#| fig-width: 10
#| fig-asp: 0.65
#| label: fig-mutual-information
#| fig-cap: Information Gain
#| warning: false
mi |> 
 slice_head(n = 20) |> 
 mutate(feature = fct_reorder(attributes, importance, .desc = FALSE)) |> 
 ggplot(aes(x = importance, y = feature)) +
 geom_point() +
 labs(title = "Information Gain") +
 drako +
 theme(axis.text.y = element_text(size = 30),
       axis.text.x = element_text(size = 30))
```

A partir de lo identificado en MI, podemos explorar algunas de las variables
con mayor poder predictivo.

### Variables numéricas {#sec-distribuiciones}

Analizar la distribución de cada predictor nos puede orientar sobre si
necesitamos hacer ingeniería de características mediante transformaciones antes
del análisis. [@kuhn_feature_2020]

```{r}
predictores <- c("dis_mim", "cqi_mim", "tad_max", "drp_max", "dis_mean",
                 "tad_p95", "lod_p5", "thp_p5", "thp_max", "lod_p99")
```


```{r}
dob <- ctl_train |> 
 select(all_of(predictores))
```

```{r}
# parámetros para graficar distribuciones
pal <- allcolors[1:ncol(dob)]
ndv <- names(dob)
nar <- str_to_title(ndv)
```

```{r}
# crear distribuciones
dist_predictores <- list(ndv, pal, nar) |>
  pmap(~ estimar_densidad(df = dob, d = ..1, color = ..2, titulo = ..1))
```

```{r}
#| code-fold: true
#| code-summary: "Mostrar Código"
#| fig-height: 6
#| fig-width: 8
dist_predictores[1:2] |> 
 reduce(.f = `+`) +
 plot_layout(ncol = 2) +
 plot_annotation(title    = "Distribución predictores numéricos")
```

- **dis_mim:** Representa la distribución de los valores mínimos de disponibilidad.  Esta variable extrae por cada usuario el valor mínimo de
disponibilidad. Podría ser relevante para saber cuando no es un problema de
este tipo.  La distribución muestra un sesgo positivo con valores atípicos
que llegan hasta los 530 segundos (ver tabla @tbl-resumen).

- **cqi_mim:** El `corrected_cqi` Evalúa la calidad de una celda en una red
móvil. Este índice toma en cuenta varios factores como la potencia de la señal,
el nivel de interferencia y otros parámetros clave para dar un indicador
comprensivo de cómo está funcionando una celda en particular. Aunque los
detalles pueden variar según el fabricante y la implementación, en esencia,
mide qué tan bien una celda puede manejar el tráfico y proporcionar un servicio
de alta calidad a los usuarios. En este caso, la distribución de los valores
mínimos evidencia una media de 7.2 y mediana de 8, lo cual indica que la gran
mayoría de usuarios se encuentran en los rangos correctos.  Se observa, sin
embargo, que hay picos entre  0 y 1, lo que podría indicar valores donde se
existan problemas de optimización.


```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
dist_predictores[3:4] |>
 reduce(.f = `+`)
```

- **tad_max:** El *feature* `tad` se construyó en la etapa de preparación como
el resultado de la suma de los índices 6 y 7.  Los SMEs indicaron que podría
ser útil para determinar problemas de cobertura o capacidad en vista de que
valores muy altos podrían asociarse casi siempre a problemas con la cobertura y
valores bajos interactuando con otras métricas podrían relacionarse con
temas de capacidad. Aquí vemos que la distribución de los valores máximo tiene
un pico muy acentuado en 14%, lo cual indica que la gran mayoría de los usuarios
se encuentran, según la gráfica, en promedio en este punto o por debajo.

- **drp_max:** El `service_drop_rate` representa perdida en los servicios de
llamadas o internet, es una unidad de optimización que causa inestabilidad en
los servicios de llamadas y datos. Se mide en porcentaje y la perdida de
servicios deberia estar en 0% o en valores muy cercanos a 0%, si se ve aun
aumento súbito que sea constante es un problema, no sé tiene un porcentaje de
umbral mano, sino se basa en la tendencia del KPI.  Los valores máximos pueden
ayudar a determinar problemas de optimización en la red móvil. La distribución
muestra un fuerte sesgo positivo con valores máximos de 100% para algunos
usuarios. Los SMEs indican que valores por encima de 1.5% ya son motivo de
preocupación.

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
dist_predictores[5:6] |>
 reduce(.f = `+`)
```

- **dis_mean:** Al igual que con la distribución de `dis_max`, la distribución
del los valores promedios podrían ser relevantes para determinar problemas de
disponibilidad. La distribución es plausible y el sesgo indica un rango de
valores amplios debido a la presencia de atípicos en algunos usuarios.

- **tad_p95:** La distribución del *timing advanced* para el percentil 95 tiene
un sesgo positivo con algunos valores atípicos mayores al promedio.  

### Variables categóricas

Las variables categóricas fueron removidas por los SMEs y por el product owner
en vista de que serían variables que probablemente no vendrían en el dataset
en producción.  En su lugar se tomó en cuenta la latitud y longitud del
sitio.

## Análisis bivariado

Aunque los boxplots ofrecen una visión más completa de la distribución de los
datos, no proporcionan el mismo nivel de detalle sobre la incertidumbre en torno
a la media que los gráficos de confianza o el MI. Usar tanto MI, como gráficos
de confianza y boxplot, proporcionará una visión más completa de los datos. Los
gráficos de confianza dan detalles sobre la media y su incertidumbre, mientras
que los boxplots muestran la distribución completa.


### Dependiente vs numéricas

#### Intervalos

Debido al desbalance de clases presente en la variable respuesta, será
necesario reflejar claramente el grado de incertidumbre que esto ocasiona.  Si
una clase tiene más observaciones que otra entonces la comparación no sería
justa y la variabilidad podría tener que ver con esto más que con la
variabilidad inherente que intentamos capturar. Para hacer esto, primero
recordemos que una estimación de intervalo describe un rango de valores dentro
del cual es posible que esté un parámetro de la población y un intervalo de
confianza es la probabilidad que asociamos con una estimación de intervalo
[@levin_statistics_1998].

En este sentido, los gráficos de confianza son esenciales para entender no solo
la relación de la variable categórica `diag` con las métricas numéricas, sino
también para capturar el grado de incertidumbre asociado con estas
métricas. Estos gráficos nos permiten ir más allá de las simples medias y
observar cómo la variabilidad en los datos afecta nuestras conclusiones.

Los gráficos de confianza nos permiten ver cómo cada métrica, se relaciona
directamente con las categorías de `diag`. Por ejemplo, si observamos que los
intervalos de confianza para `thp_required_lte` son significativamente bajos en
la categoría "CAPACIDAD", podemos inferir con cierto grado de confianza que un
`thp_required_lte` bajo generalmente indica un problema de capacidad.

El intervalo da una idea de dónde podrían caer las verdaderas medias de
`thp_required_lte` para cada categoría si tuvieras acceso a toda la población,
en lugar de solo una muestra.

En el análisis univariado de la sección @sec-distribuiciones, así como en el
resumen estadístico @sec-estadisticas observamos que las distribuciones se
encuentran con mucho sesgo.

En el análisis exploratorio de datos (EDA), la presencia de valores atípicos
puede distorsionar significativamente las interpretaciones y conclusiones. Los
valores atípicos pueden tener un impacto desproporcionado en la media y la
varianza, lo que a su vez afectará las visualizaciones y las medidas de
tendencia central y dispersión. En este contexto, la transformación de variables
se convierte en una herramienta invaluable para mitigar estos efectos y permitir
un análisis más robusto.

El paquete bestNormalize en R [@R-bestNormalize] es especialmente útil para este
propósito. Según su documentación [@bestNormalize2021], el paquete intenta
encontrar la mejor transformación para normalizar una variable numérica. Utiliza
una variedad de transformaciones, como transformación de Box-Cox, transformación
de Yeo-Johnson, transformación de logaritmo, entre otras, y selecciona la que
minimiza algún criterio de ajuste, como el error cuadrático medio (MSE) o el
estadístico de Shapiro-Wilk para la normalidad.

Una de las ventajas de utilizar bestNormalize es que realiza una búsqueda
exhaustiva a través de múltiples métodos y selecciona la transformación más
óptima para cada característica. Esto es especialmente útil cuando se trabaja
con conjuntos de datos con múltiples características que pueden requerir
diferentes tipos de transformaciones.

Al aplicar la transformación más adecuada, las visualizaciones de los datos
transformados ofrecen una representación más precisa de las relaciones
subyacentes entre las variables. Esto es especialmente útil para técnicas que
asumen la normalidad de los datos. Además, al reducir el impacto de los valores
atípicos, las transformaciones permiten que las métricas como la media y la
varianza sean más representativas del conjunto de datos, lo que facilita la
interpretación y el análisis posterior.

En resumen, la transformación de variables utilizando bestNormalize mejora la
calidad del análisis exploratorio al hacer que las visualizaciones y las
métricas sean más robustas y representativas, permitiendo así una mejor
comprensión de la estructura subyacente de los datos.

```{r}
# Función para calcular la media y el intervalo de confianza
mean_ci <- function(df, metrica, conf.level = 0.95) {
 x  <- df[[metrica]][!is.na(df[[metrica]])]
 m  <- mean(x)
 ci <- t.test(x)$conf.int
 data.frame(media = m, lower = ci[1], upper = ci[2])
}
```

```{r}
#| echo: false
ctl_normalizados <- read_fst("data/ctl_normalizados.fst") |> 
 as_tibble()
```

```{r}
# Seleccionar algunos predictores con un information gain alto
best_mi <- c("lod_mim", "tad_max", "prb_mim", "thp_mim")
```

```{r}
#| eval: false
ctl_normalizados <- ctl_train |>
 select(diag, where(is.numeric), -user) |> 
 relocate(diag) |> 
 recipe(diag ~ ., data = _) |> 
 step_select(all_of(best_mi), diag) |>
 step_zv(all_predictors()) |> 
 step_nzv(all_predictors(), unique_cut = 2) |> 
 step_corr(threshold = 0.77) |> 
 step_orderNorm(all_predictors()) |> 
 ver()
```

```{r}
ctl_norm <- ctl_normalizados |> 
 select(lod_mim, tad_max, prb_mim, thp_mim, diag)
```

```{r}
#| eval: false
#| echo: false
write_fst(ctl_normalizados, path = "data/ctl_normalizados.fst", compress = 0)
```

```{r}
# Función para crear un dataframe con los intervalos por cada métrica
calcular_ci <- function(kpi) {
 ctl_normalizados |> 
 group_by(diag) %>%
 do(mean_ci(., kpi)) %>%
 ungroup() %>%
 arrange(media)
}
```

```{r}
# Guardar las métricas que vamos a comparar
nm <- names(ctl_normalizados)[-length(ctl_normalizados)]
```

```{r}
# Crear lista de dataframes con intervalos de confianza
lista_ci <- nm |> 
 map(~ calcular_ci(kpi = .x)) |> 
 set_names(nm)
```

```{r}
# Crear una lista con las gráficas de cada métrica
analisis_bivariado <- lista_ci |> 
 map2(.y = nm, ~ ggplot(.x, aes(x = diag, y = media)) +
 geom_point() +
 geom_errorbar(aes(ymin = lower, ymax = upper), width = .1) +
 theme(axis.text = element_text(size = 8)) +
 xlab("Diagnóstico") +
 labs(title = .y) +
 drako)
```

Aplicar la transformación antes de realizar los gráficos de confianza puede
ser beneficioso por las siguientes razones:

1. **Robustez frente a Atípicos:** Como se mencionó anteriormente, los valores
atípicos pueden tener un impacto significativo en la media. Al transformar los
datos, se reduce la influencia de estos puntos extremos, lo que resulta en
intervalos de confianza más robustos.

2. **Mejora de la Interpretación:** Los datos transformados pueden hacer que las
diferencias o similitudes entre grupos sean más claras y fáciles de interpretar
en los gráficos de confianza.

3. **Normalidad Aproximada:** Muchas técnicas estadísticas, incluida la
estimación de intervalos de confianza, asumen que los datos son aproximadamente
normales. Las transformaciones pueden ayudar a cumplir con este supuesto.

4. **Consistencia en el Análisis:** Si ya se han transformado las variables para
otros aspectos del análisis exploratorio de datos, mantener esa transformación
para los gráficos de confianza asegura que se están evaluando las mismas
distribuciones a lo largo de todo el análisis.

5. **Claridad en la Presentación:** Los gráficos de datos transformados pueden
ser más fáciles de entender y comunicar, especialmente si reducen la asimetría o
la escala de los datos originales.

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
#| label: fig-thp-required-lte
#| fig-cap: Porcentaje de tiempo que mantiene el throughput requerido en LTE
walk(analisis_bivariado[1], ~ print(.x))
```

Tal como se ilustra en la figura @fig-thp-required-lte el intervalo de confianza
para la categoría "CAPACIDAD" es relativamente pequeño, lo que sugiere que hay
menos incertidumbre en la estimación de la media para esta categoría.  Esto
podría significar que `lod_mim` es una predictor fuerte para la categoría de
capacidad. Es decir, este predictor es importante para ayudar a distinguir la
categoría "capacidad" de las demás.

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
#| label: fig-cll-prctg
#| fig-cap: Porcentaje de tiempo que el usuario estuvo conectado a la celda.
walk(analisis_bivariado[2], ~ print(.x))
```

Los resultados observados en la @fig-cll-prctg son consistentes con las
expectativas.  Vemos que la categoría "COBERTURA" se encuentra bien explicada
por valores altos en la variable `tad_max`, lo cual es congruente con la
naturaleza de esta categoría.  

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
#| label: fig-rate-prb-dl
#| fig-cap: Porcentaje de bloques físicos utilizados para transmitir datos desde la RBS hacia los terminales. Una alta tasa de utilización de PRB podría generar problemas de capacidad.
walk(analisis_bivariado[3], ~ print(.x))
```

Esta métrica es utilizada principalmente por el equipo CTL para determinar
problemas de capacidad en la red LTE.  Vemos en la @fig-rate-prb-dl que cuando
hay valores intermedios de `rate-prb-dl` podrían presentarse categoría de
disponibilidad. No tenemos una explicación sobre esta relación, por lo que
se investigará con los SMEs. 

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
#| label: fig-rrc-success-rate
#| fig-cap: Eficiencia en el establecimiento de una conexión RRC
walk(analisis_bivariado[4], ~ print(.x))
```

Dado que `thp_mim` es un indicador de rendimiento que mide la velocidad de
descarga por Unidad de Equipo de Usuario en una red celular, generalmente en
Mbps, un valor muy alto (en este sentido sería thp bajo en downlink ya que es
thp mínimo) podría apuntar a problemas en la disponibilidad.  Los microcortes
podrían ocasionar un thp bajo y por lo tanto indisponibilidad.

#### Densidad

Es importante realizar una comparación bivariada de las distribuciones y
comparar su forma (simetría y sesgo) entre cada una de las categorías.

```{r}
ctl_pivoted <- ctl_normalizados |>
 pivot_longer(
  cols      = lod_mim:thp_mim,
  names_to  = "metric", 
  values_to = "value") |> 
 filter(!is.na(value))
```

```{r}
lista_densidad <- nm |> 
 map(~ ctl_normalizados |> 
  ggplot(aes(x  = .data[[.x]], y = diag, fill = after_stat(x))) +
  geom_density_ridges_gradient(
   scale = 5, 
   rel_min_height = 0.01,
   bandwidth = 0.248
  ) +
  scale_fill_viridis_c(name = "Score", option = "C") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  coord_cartesian(clip = "off") +
  labs(title = .x) +
  theme_ridges(font_size = 18, font_family = "yano", grid = TRUE) +
  theme(
   axis.text.y  = element_text(size = 24),
   axis.text.x  = element_text(size = 24),
   axis.title.y = element_blank(),
   axis.title.x = element_blank(),
   plot.title   = element_text(size = 50))
 )
```


```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_densidad[1], ~ print(.x))
```

En este caso es evidente que la distribución de tipo "leptocúrtica" para el
caso de optimización de `lod_mim`, así como la distancia de su media con el del
resto, sugiere que hay muchos ejemplos con valores bajos de cell_load que
ayudan a determinar problemas de este tipo.

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_densidad[2], ~ print(.x))
```

Cobertura y optimización parecen tener valores más grandes de `tad_max`

```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_densidad[3], ~ print(.x))
```

Los valores de `rate_prb` muy bajos podrían implicar problemas de optimización
en una buena cantidad de casos.


```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_densidad[4], ~ print(.x))
```

Valores bajos de `throupt_dl` (thp_mim) pareciera que apuntan a situaciones
donde hay problemas de optimización.

#### Boxplots

Los boxplots pueden agregar información adicional que no es posible ver solo
con los histogramas, las densidades o incluso los gráficos de confianza. En
este caso nos interesa analizar el ancho de la distribución, así como la
cantidad de puntos que están generando la distribución.

```{r}
lista_boxplots <- nm |> 
 map(~ ctl_normalizados |> 
  drop_na() |> 
  ggplot(aes(x  = .data[[.x]], y = diag)) +
  geom_boxplot(aes(fill = diag),
               outlier.colour = "red",
               outlier.shape = 16,
               outlier.size = 2,
               notch = F)  +
   stat_summary(fun = mean, 
               colour = "darkred",
               geom = "point", 
               shape = 4, 
               size = 3) +
  # geom_jitter(width = 0.2, alpha = .1) +
  labs(title = .x) +
  drako +
  theme(legend.position = "none",
        axis.text.y = element_text(size = 25),
        axis.title.y = element_blank()) 
 )
```


```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_boxplots[1], ~ print(.x))
```



```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_boxplots[2], ~ print(.x))
```


```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_boxplots[3], ~ print(.x))
```


```{r}
#| echo: false
#| fig-height: 6
#| fig-width: 8
walk(lista_boxplots[4], ~ print(.x))
```

De manera similar a como se ha analizado anteriormente, los predictores más
informativos escogidos a través de information gain parecieran tener algún
tipo de efecto principal sobre cada una de las categorías.  La presencia de
valores atípicos es relevante y debe tomarse en cuenta en las transformaciones
en la fase de preprocesamiento.

Para tratar los valores atípicos podemos considerar el aplicar una
transformación llamada *spatial sign* [ver @kuhn_feature_2020] y
[@kuhn_applied_2013] o bien usar modelos robustos a valores atípicos.

### Interacciones

Según los SMEs entrevistados, la categoría **optimización** por lo general
depende de un par de variables, es decir, que esta categoría podría ser el
resultado de interacciones entre variables.  Las métricas que principalmente se
utilizan para determinar esto son:

```{r}
ctl_interact <- ctl_train |>  
  select(all_of(best_mi), diag) |> 
  filter(diag %in% c("OPTIMIZACION", "CAPACIDAD", "COBERTURA")) |> 
  mutate(across(diag, as_factor),
         across(diag, fct_drop))
```

```{r}
#| message: false
modelo_multinom <- multinom(
 diag ~ prb_mim * thp_mim, data = ctl_interact) |> 
 suppressMessages()
```


```{r}
n1 <-  seq(min(ctl_interact$prb_mim), 
           max(ctl_interact$prb_mim), 
           length.out = 4) |> round(2)

n2 <- seq(min(ctl_interact$thp_mim), 
                          max(ctl_interact$thp_mim), 
                          length.out = 4) |> round(2)

```


```{r}
# Visualizar las interacciones
efectos_prediccion <- predictorEffects(
 modelo_multinom, ~ prb_mim + thp_mim,
 xlevels = list(
   prb_mim = n1,
   thp_mim =  n2))
```

```{r}
#| fig-width: 10
#| fig-asp: 0.6
#| label: fig-interacciones
#| fig-cap: Interacciones
#| warning: false
plot(
 efectos_prediccion,
 axes = list(grid = TRUE, x = list(rug = FALSE,
 prb_mim = list(ticks = list(at = n1)),
 thp_mim  = list(ticks = list(at = n2))),
                 y = list(style = "stacked")),
 lines = list(col = c("blue", "red", "orange", "brown", "black")),
 lattice = list(key.args = list(columns = 1),
                strip = list(factor.names = FALSE)))
```

### Análisis de correlación {#sec-correlacion}

Se debe realizar la inspección de las correlaciones que se encuentren arriba
de 0.75 en valor absoluto.  Este umbral es sugerido por algunos autores
[@kuhn_applied_2013, pag 87].

Debido a la alta dimensionalidad del dataset, seleccionaremos de manera
aleatoria un conjunto de *features* para evaluar el nivel de correlación
encontrado.

```{r}
#| echo: false
analize_corr <- read_fst("./data/correlacion.fst") |> as_tibble()
```

```{r}
#| eval: false
analize_corr <- ctl_train |>
 select(diag, where(is.numeric), -user) |> 
 relocate(diag) |> 
 recipe(diag ~ ., data = _) |> 
 step_zv(all_predictors()) |> 
 step_nzv(all_predictors(), unique_cut = 2) |> 
 ver() |> 
 select(sample.int(n = ncol(analize_corr), size = 11, replace = FALSE)) 
```

```{r}
#| eval: false
#| echo: false
write_fst(analize_corr, path = "./data/correlacion.fst", compress = 0)
```


```{r}
corm <- analize_corr |>
 correlate(
  use    = "pairwise.complete.obs", 
  method = "pearson", 
  quiet  = TRUE)
```

```{r}
#| fig-width: 10
#| fig-asp: 0.6
#| label: fig-correlacion
#| fig-cap: Análisis de Correlación entre predictores
#| warning: false
corm |> 
 shave() |> 
 rplot(print_cor = TRUE) +
 theme(
  axis.text.y = element_text(size = 8),
  axis.text.x = element_text(size = 8, angle = 90, hjust = -0.3, vjust = 0.4))
```

### K-means

El análisis de clústeres, y en particular el método k-means, es una técnica
poderosa para evaluar la relevancia de las variables seleccionadas mediante el
cálculo de la información mutua o ganancia de información. Este método nos
ayuda a comprender si las variables seleccionadas tienen un fuerte efecto
principal, es decir, si cada variable por sí sola tiene un impacto
significativo en la formación de los grupos, sin necesariamente depender de
interacciones complejas con otras variables.

Si al aplicar k-means encontramos que los clústeres se forman de manera
coherente y significativa alrededor de estas variables, podemos inferir que son
predictores fuertes y que su inclusión en un modelo predictivo podría mejorar
su rendimiento. Esto es especialmente útil cuando queremos incorporar variables
de manera gradual en un enfoque de selección de características paso a paso
(stepwise feature selection). En este proceso, añadimos o eliminamos
predictores basándonos en criterios estadísticos, buscando optimizar algún
criterio de selección como el AIC (Criterio de Información de Akaike) o el BIC
(Criterio de Información Bayesiano).

Por lo tanto, si las variables seleccionadas demuestran ser relevantes en el
análisis de clústeres con k-means, esto nos proporciona una validación
adicional de su importancia y nos da la confianza para incluirlas en el modelo.
Esto nos permite realizar una selección de características más informada y
metódica, asegurándonos de que cada variable que incluimos contribuye
significativamente a la capacidad predictiva del modelo.

En resumen, el uso de k-means en conjunto con la ganancia de información nos
ofrece una estrategia robusta para identificar las variables más prometedoras
para nuestros modelos predictivos, permitiéndonos construir modelos más
precisos y eficientes.

El primer paso es cargar algunas librerías de apoyo para realizar el análisis.

```{r}
import::from(clustertend, hopkins)
import::from(factoextra, fviz_nbclust, fviz_cluster)
import::from(NbClust, NbClust)
import::from(clValid, clValid, optimalScores)
```

Elegiremos únicamente aquellas variables que tienen más información predictiva
y que podrían ayudar a realizar de manera correcta la separación de clases.

```{r}
ctl_scaled <- ctl_train |> 
 select(diag, all_of(predictores)) |> 
 recipe(diag ~ ., data = _) |> 
 step_zv(all_predictors()) |> 
 # step_nzv(all_predictors(), unique_cut = 2) |> 
 step_center(all_numeric()) |> 
 step_scale(all_numeric()) |> 
 step_smote(diag) |> 
 ver()
```

Realizaremos una prueba con las las categorías que tienden a tener mayor
separación en sus métricas.

```{r}
ctl_sd <- ctl_scaled |> 
 filter(diag %in% c("COBERTURA", "CAPACIDAD", "DISPONIBILIDAD")) |> 
 select(-diag)
```


```{r}
#| echo: false
ctl_dis <- pin_read(board = tablero_ctl, name = "ctl_dis")
```

```{r}
#| eval: false
ctl_dis <- dist(ctl_scaled)
```

```{r, echo=FALSE}
#| message: false
# Guardar split
pin_write(
  board       = tablero_ctl,
  x           = ctl_dis,
  name        = "ctl_dis",
  type        = "qs") |> suppressMessages()
```

Exploremos cual es el efecto de diferentes valores de `k` probando del 1 al 9.
Primero agruparemos los datos 9 veces, en cada una utilizando diferentes
tamaños de `k`, luego agregaremos las columnas que contienen los datos
ordenados.

```{r}
kclusts <- tibble(k = 1:9) |> 
 mutate(
  kclust    = map(k, ~kmeans(ctl_sd, .x)),
  tidied    = map(kclust, tidy),
  glanced   = map(kclust, glance),
  augmented = map(kclust, augment, ctl_sd)
 )
```

```{r}
kclusts
```

Podemos convertir esto en tres distintos datasets cada uno representando
distintos tipos de datos obtenidos con las funciones `tidy()`, `augment()` y
`glance()`.

```{r}
clusters <- kclusts |>
  unnest(cols = c(tidied))

assignments <- kclusts |> 
  unnest(cols = c(augmented))

clusterings <- kclusts |>
  unnest(cols = c(glanced))
```

Ahora podemos graficar los puntos originales utilizando la data de `augment()`
con cada punto coloreado de acuerdo al cluster predicho

```{r}
#| fig-width: 10
#| fig-asp: 0.6
#| label: fig-cluster
#| fig-cap: K-Means Clustering
#| warning: false
assignments |> 
 ggplot(aes(x = tad_max, y = thp_max)) +
 geom_point(aes(color = .cluster), alpha = 0.8) + 
 geom_point(data = clusters, size = 10, shape = "x") +
 facet_wrap(~ k)
```

Visualmente observamos en la figura @fig-cluster que la mejor separación de
clases se logra con tres clusters.

A continuación podemos utilizar la función `fviz_nbclust()` del paquete
factoextra para evaluar por medio del método "silhouette" cuál es la cantidad
óptima de clusters.

```{r}
#| fig-width: 10
#| fig-asp: 0.6
#| label: fig-optim-cluster
#| fig-cap: Cantidad óptimda de clusters
#| warning: false
fviz_nbclust(ctl_sd, kmeans, method = "silhouette")
```

En la figura @fig-optim-cluster se comprueba que la cantidad óptima son tres
grupos.

```{r}
#| fig-width: 10
#| fig-asp: 0.6
#| label: fig-elbow
#| fig-cap: Cantidad óptimda de clusters
#| warning: false
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```

Nuevamente en la figura @fig-elbow comprobamos por el método elbow que tres
es el número adecuado de grupos.

Una vez que hemos validado los grupos, podemos realizar el ajuste nuevamente
con la cantidad de clusters para poder graficar adecuadamente.

```{r}
kclust <- kmeans(ctl_sd, centers = 3)
```

```{r}
#| fig-width: 10
#| fig-asp: 0.6
#| label: fig-kmeans
#| fig-cap: Separación óptima de clases
#| warning: false
fviz_cluster(kclust, ctl_sd, ellipse.type = "norm") +
 yunkel
```

Vemos que, dejando de lado algunas excepciones, las categorías presentan una
separación apreciable, sin embargo, hay un cierto solapamiento en las métricas
de algunas clases.

### PCA

En la sección de análisis de correlación (@sec-correlacion), notamos una alta
correlación entre muchas de las estadísticas descriptivas utilizadas para
colapsar el dataset. Esto podría deberse a varias razones. Por ejemplo, las
métricas de rendimiento en telecomunicaciones a menudo están interconectadas;
un cambio en una métrica puede influir directamente en otra, como la relación
entre la calidad de la señal y la tasa de error de bits. Además, la naturaleza
de los datos recopilados en intervalos regulares puede llevar a redundancias,
donde las medidas repetidas reflejan la misma variabilidad subyacente.

El PCA, como técnica de extracción de características, es particularmente útil
en este contexto de alta multicolinealidad. Al transformar las variables
originales en un nuevo conjunto de variables ortogonales (las componentes
principales), PCA nos permite capturar la mayor parte de la variabilidad en los
datos con menos predictores. Esto no solo ayuda a aliviar los problemas
asociados con predictores correlacionados sino que también simplifica el modelo
al reducir la cantidad de variables, lo cual es crucial para evitar el
sobreajuste y mejorar la interpretación del modelo.

Además, al aplicar PCA en la fase de preprocesamiento, podemos evaluar la
idoneidad de esta técnica para reducir la dimensionalidad antes de proceder al
modelado predictivo. Si las primeras componentes principales capturan una
cantidad significativa de la variabilidad total, esto indica que PCA podría ser
una estrategia efectiva para condensar la información sin perder insights
críticos. Esto es especialmente relevante en nuestro proyecto CTL, donde la
eficiencia y la claridad en la interpretación de los modelos son de suma
importancia para la toma de decisiones basada en datos.

```{r}
ctl_validacion  <- validation(ctl_split)
```

```{r}
ctl_num <- ctl_train |> select(diag, where(is.numeric), -user)
ctl_rec <- recipe(diag ~ ., data = ctl_num) |>
 step_zv(all_numeric_predictors()) |>
 step_orderNorm(all_numeric_predictors()) |>
 step_normalize(all_numeric_predictors())
```

```{r}
ctl_trained <- prep(ctl_rec)
```


```{r}
plot_validation_results <- function(recipe, dat = ctl_validacion) {
 recipe |> 
 prep() |> 
 bake(new_data = dat) |> 
 ggplot(aes(x = .panel_x, y = .panel_y, color = diag, fill = diag)) +
 geom_point(alpha = 0.8, size = 1) +
 geom_autodensity(alpha = .3) +
 facet_matrix(vars(-diag), layer.diag = 2) +
 scale_color_brewer(palette = "Dark2") +
 scale_fill_brewer(palette = "Dark2") +
 drako
}
```


```{r}
#| fig-width: 11
#| fig-asp: 0.8
#| label: fig-pca
#| fig-cap: Análisis de Componentes Principales
#| warning: false
ctl_trained %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>%
  plot_validation_results() +
  ggtitle("Principal Component Analysis") +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 25),
        strip.text = element_text(size = 25))
```

La visualización resultante del PCA, presentada en la Figura @fig-pca, no
revela una distinción nítida entre las diferentes clases de diagnóstico. Los
dos primeros componentes principales, PC1 y PC2, no logran capturar una
separación definida entre las categorías. Esto puede ser indicativo de que las
variables seleccionadas están influidas por una variabilidad común que no
distingue adecuadamente entre las clases, o que las interacciones entre
variables son complejas y no lineales, lo que el PCA, siendo un método lineal,
no puede determinar.

Es interesante notar que la clase 'disponibilidad' se distingue ligeramente de
las demás, lo cual es coherente con la naturaleza de sus métricas, que tienden
a tener un rango de valores más restringido y específico. Esto sugiere que las
métricas de 'disponibilidad' podrían estar operando en un espacio de
características distintas o estar menos correlacionadas con las métricas de
otras clases.

La superposición observada en las clases podría también reflejar la presencia
de patrones subyacentes complejos que requieren métodos más sofisticados para
su identificación y separación. Alternativamente, podría ser un indicativo de
que se necesitan más componentes para capturar la variabilidad relevante para
la diferenciación de clases, o que otras técnicas de reducción de
dimensionalidad, como el t-SNE o el UMAP, que son capaces de capturar
estructuras no lineales, podrían ser más adecuadas para este conjunto de
datos.

En resumen, los resultados del PCA nos instan a considerar la posibilidad de
explorar otras técnicas de preprocesamiento y modelado que puedan manejar la
complejidad inherente a nuestros datos y, por ende, mejorar la separación de
clases para fines de diagnóstico en el proyecto CTL

