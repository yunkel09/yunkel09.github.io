# Modelado {#sec-modelado}


```{r}
#| echo: false
source("_paquetesx.R")
source("_configura.R")
source("_variables.R")
source("_funciones.R")
```

```{r}
#| echo: false
ctl_split <- pin_read(board = tablero_ctl, name = "ctl_split")
ctl_train <- training(ctl_split)
ctl_validation_set <- validation_set(ctl_split)
```

## Métricas

Primeramente definiremos las métricas a utilizar. Estas se determinaron en la
fase de EDA con base a la distribución de la variable respuesta y en especial
del hecho de que no está balanceada.

- **Precisión:** Esta métrica es importante cuando los costos de los falsos
positivos son altos. En el contexto de nuestro proyecto, queremos asegurarnos
de que las predicciones de las categorías de diagnóstico sean correctas y no
queremos alarmar innecesariamente sobre posibles problemas que no existen.

- **Recall (Sensibilidad):** El recall es crucial cuando es esencial detectar
todos los casos positivos. Dado que nuestro proyecto podría estar relacionado
con el mantenimiento predictivo, no queremos pasar por alto ninguna instancia
que realmente pertenezca a una categoría de diagnóstico crítica.

- **F1-Score Macro:** Esta métrica combina la precisión y el recall en un solo
número, proporcionando un equilibrio entre ambos. Al usar el promedio macro,
tratamos todas las clases por igual, dando el mismo peso a cada una, lo cual es
importante en nuestro conjunto de datos desbalanceados. Esto asegura que no
ignoramos el rendimiento en las clases minoritarias, que a menudo son las más
importantes en aplicaciones de diagnóstico.

- **ROC_AUC:** La curva ROC y el área bajo la curva (AUC) proporcionan una
medida agregada del rendimiento en todos los umbrales de clasificación. Esto es
particularmente útil cuando las clases son desbalanceadas y los costos de los
falsos positivos y falsos negativos varían. El ROC_AUC es una medida de la
capacidad del modelo para discriminar entre las clases positivas y negativas, y
un valor alto indica que el modelo tiene una buena medida de separabilidad.

```{r}
mset <- metric_set(precision, recall, f_meas, roc_auc)
```

## Control de Hiperparámetros

Debemos establecer la configuración de la búsqueda de hiperparámtros. Para este
fin utilizaremos un método especial, el cual es más eficiente que el método
tradicional. 

```{r}
race_ctrl <- control_race(
 save_pred     = TRUE,
 parallel_over = "everything",
 verbose       = TRUE,
 verbose_elim  = TRUE,
 save_workflow = FALSE)
```

## Algoritmos

Una de las fortalezas del framework Tidymodels es su capacidad para facilitar
la evaluación simultánea de múltiples algoritmos y diversas estrategias de
preprocesamiento. Esta flexibilidad es sumamente valiosa, ya que nos permite
explorar una extensa gama de combinaciones en un único proceso de modelado.
Idealmente, las recetas de preprocesamiento deberían ser personalizadas para
cada algoritmo específico, optimizando así su rendimiento. No obstante, en la
práctica, y siguiendo el principio del 'no free lunch theorem', es recomendable
realizar pruebas exhaustivas y ajustes iterativos. Esto se debe a que no existe
un único modelo o método que sea el mejor para todos los problemas y conjuntos
de datos; la efectividad puede variar según la naturaleza específica de cada
tarea de modelado.

```{r}
rand_forest_ranger <- rand_forest(
 mtry  = tune(),
 min_n = tune()) |>
set_engine('ranger', importance = "permutation") |>
set_mode('classification')

bt_lightgbm <- boost_tree(
 tree_depth = 2,
 learn_rate = 0.001,
 trees = tune(),
 min_n = 20) |>
set_engine(engine = "lightgbm") |>
set_mode(mode = "classification")

svm_linear_kernlab <- svm_linear(
 cost = tune(),
 margin = tune()) |>
set_engine('kernlab') |>
set_mode('classification')

glmnet_spec <- multinom_reg(
 penalty = tune(),
 mixture = tune()) |>
set_engine('glmnet') |>
set_mode('classification')

xgboost_spec <- boost_tree(
 tree_depth     = 3,           
 trees          = tune(),
 learn_rate     = tune(),
 min_n          = tune(),
 loss_reduction = tune(),
 sample_size    = tune(),
 stop_iter      = tune()) |>
set_engine('xgboost') |>
set_mode('classification')

decision_tree_partykit <- decision_tree() |>
set_engine(engine = "partykit") |>
set_mode(mode = "classification")

nearest_neighbor_kknn_spec <- nearest_neighbor(
 neighbors = tune(), 
 weight_func = tune(), 
 dist_power = tune()) |>
set_engine('kknn') |>
set_mode('classification')
```

Es importante notar que el *placeholder* `tune()` sirve para que ahí se
evalué una cuadrícula de hiperparámetros.

```{r}
# lista de modelos
modelos <- list(
 rand_forest_ranger     = rand_forest_ranger,
 bt_lightgbm            = bt_lightgbm,
 svm_linear_kernlab     = svm_linear_kernlab,
 glmnet                 = glmnet_spec,
 xgboost                = xgboost_spec,
 decision_tree_partykit = decision_tree_partykit,
 knn                    = nearest_neighbor_kknn_spec)
```

## Preprocesamiento

Crearemos 6 recetas de preprocesamiento, cada una con distintos pasos y filtros.
En esta parte se aplican distintas técnicas de *feature selection* y de
*feature scaling*.

```{r}
boruta_regular <-  recipe(diag ~ ., data = ctl_train) |>
 update_role(user, new_role = "id") |> 
 step_select_boruta(all_predictors(), outcome = "diag") |> 
 step_smote(diag, skip = TRUE)
 
infgain_regular_7 <- recipe(diag ~ ., data = ctl_train) |>
 update_role(user, new_role = "id") |>
 step_select_infgain(
  all_predictors(),
   outcome = "diag",
   threshold = 0.7) |>
 step_smote(diag, skip = TRUE)

infgain_regular_9 <- recipe(diag ~ ., data = ctl_train) |>
 update_role(user, new_role = "id") |>
 step_select_infgain(
  all_predictors(),
   outcome = "diag",
   threshold = 0.7) |>
 step_smote(diag, skip = TRUE)

infgain_regular_7_nzv <- recipe(diag ~ ., data = ctl_train) |>
 update_role(user, new_role = "id") |>
 step_zv(all_predictors()) |> 
 step_nzv(all_predictors()) |> 
 step_select_infgain(
  all_predictors(),
   outcome = "diag",
   threshold = 0.7) |>
 step_smote(diag, skip = TRUE)

mrmr_regular_top20 <- recipe(diag ~ ., data = ctl_train) |>
 update_role(user, new_role = "id") |>
 step_select_mrmr(
  all_predictors(),
   outcome = "class",
   top_p = 20) |> 
 step_smote(diag, skip = TRUE)

infgain_norm_7 <- recipe(diag ~ ., data = ctl_train) |>
 update_role(user, new_role = "id") |>
 step_orderNorm(all_numeric_predictors()) |>
 step_normalize(all_numeric_predictors()) |>
 step_select_infgain(
  all_predictors(),
   outcome = "diag",
   threshold = 0.7) |>
 step_smote(diag, skip = TRUE)
```

```{r}
recetas <- list(
 boruta_regular        = boruta_regular,
 infgain_regular_7     = infgain_regular_7,
 infgain_regular_9     = infgain_regular_9,
 infgain_regular_7_nzv = infgain_regular_7_nzv,
 mrmr_regular_top20    = mrmr_regular_top20,
 infgain_norm_7        = infgain_norm_7)
```

## Workflow Set

Una vez que hemos definido los motores (algoritmos) a probar junto con las
distintas recetas de preprocesamiento, procederemos a juntar todo dentro de
un `workflow_set`. 

```{r}
ctl_set <- workflow_set(preproc = recetas, models  = modelos)
ctl_set |> print(n = Inf)
```

Podemos observar que todos los algoritmos se combinaron con todas las recetas
de preprocesamiento. En total hay `r nrow(ctl_set)` combinaciones.

## Ajustar

```{r}
#| eval: false
cl <- makePSOCKcluster(10)    # <1>
registerDoParallel(cl)        # <1>

final_tune <- ctl_set |>
 workflow_map(
 fn        = "tune_race_anova",     # <2> 
 verbose   = TRUE,           # <3>
 resamples = ctl_folds,      # <4>
 control   = race_ctrl,      # <5>
 seed      = 2023,           # <6>
 metrics   = mset,           # <7>
 grid      = 20)             # <8>

stopCluster(cl)              # <9>
unregister()                 # <9>
```

1. Habilitar el backend para procesamiento paralelo haciendo uso de 10 nucleos
físicos del CPU.
2. Utilizar el método de tipo "race" el cual realiza un modelo ANOVA para
probar la significancia estadística de las diferentes configuraciones del
modelo (ver [Racing Methods](https://bit.ly/465ErTQ)).
3. Establecer que se muestre el avance en el ajuste.
4. Utilizar los pliegos definidos con validación cruzada.
5. Utilizar la configuración de búsqueda de hiperparámetros del método "race".
6. Definir semilla.
7. Establecer métricas a calcular.
8. Tamaño de la cuadricula de búsqueda de hiperparámetros.
9. Detener el backend de parelelización.


































